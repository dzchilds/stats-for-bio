[
["index.html", "APS 240: Data Analysis and Statistics with R Chapter 1 Course information and overview 1.1 Why do a data analysis course? 1.2 Course overview 1.3 How to use the teaching material 1.4 Health and safety using display screen equipment", " APS 240: Data Analysis and Statistics with R Dylan Z. Childs 2018-09-21 Chapter 1 Course information and overview This is the online course book for the Data Analysis and Statistics with R (APS 240) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained—it contains all the material you are expected to learn this year. Bethan Hindle is running the course. Please email her if you have any general queries about the course. 1.1 Why do a data analysis course? To do science yourself, or to understand the science other people do, you need some understanding of the principles of experimental design, data collection, data presentation and data analysis. That doesn’t mean becoming a maths wizard, or a computer genius. It means knowing how to take sensible decisions about designing studies and collecting data, and then being able to interpret those data correctly. Sometimes the methods required are extremely simple, sometimes more complex. You aren’t expected to get to grips with all of them, but what we hope to do in the course is to give you a good practical grasp of the core techniques that are widely used in biology and environmental sciences. You should then be equipped to use these techniques intelligently and, equally importantly, know when they are not appropriate, and when you need to seek help to find the correct way to design or analyse your study. You should, with some work on your part, acquire a set of skills which you will use at various stages throughout the remainder of your course, in practicals, field courses and in your project work. These same skills will almost certainly also be useful after your degree, whether doing biology, or something completely different. We live in a world that is increasingly flooded with data, and people who know how to make sense of this are in high demand. The R statistical programming environment underpins much of this endeavour, in both academic and commercial settings. Learning the basic principles of data analysis with R will only improve your employment prospects. 1.2 Course overview 1.2.1 Aims This course has two main, and equal, aims. The first is to provide a basic training in the use of statistical methods and software (R and RStudio) to analyse biological data. The second is to introduce some of the principles of experimental design, sampling, data interpretation, graphical presentation and scientific writing relevant to the biological and environmental sciences. 1.2.2 Objectives By the end of the course you should be familiar with the principles and use of a range of basic statistical techniques, be able to use the R programming language to carry out appropriate analyses of biological data, evaluate your statistical models, and make sensible interpretation of the results. You should be able to relate the ways in which data are collected (by different designs of sampling or experiment) to the types of statistical methods that can be used to analyse those data. In combination with the skills you developed in APS 135, you should be able to decide on appropriate ways of investigate data graphically, be able to produce good quality scientific figures, and incorporate these, along with statistical results, into a formal report. 1.2.3 Assumed background You are assumed to be familiar with the use of personal computers on the University network, and with the use of R for data input, manipulation and plotting introduced in APS 135. If you are unsure about these basic methods, then you will need to revise the material covered in the Level 1 IT practicals. The key skills you need are covered in the Programming prerequisites chapter. We will revise the most important topics in the first practical session. 1.2.4 Methods The course runs over semester 1, in weeks 1-12. The first 10 weeks consists of a 2-hour IT practical each week, along with some additional preparatory practical work and reading. All components of the course are compulsory. The remaining 2 weeks are devoted to revision and an assessed data analysis project. Students come to APS 240 with very different experiences and competancies. Because of this unavoidable situation, the course is designed as a ‘self-teaching’ module to you flexibility in the rate at which you work through the material. This doesn’t mean no one is going to help you, but you are expected to take responsibility of your own learning. 1.2.4.1 Preparatory reading and practical work Each week starts with some preparatory reading and ‘walk-through’ practical work from the course book. We’ll let you know what you need to do each week. The book chapters generally come in one of two forms: Practical walk-through chapters. These are designed to introduce the practical aspects of different analysis techniques. These generally focus on the ‘when’ and ‘why’ we use a particular technique, as well how to actually do it in R. You are free (encouraged even) to work through these in groups. Concept chapters. These focus on ideas and concepts, rather then using R per se (though they may use R from time to time). They provide background information or more detailed discussion relating to the topics you are covering. These are an integral part of the course. Please don’t skip them! It is up to you to complete the preparatory work in your own time. However, we are not expecting you to understand everything the first time around. This point is so important it’s worth saying again—you are not expected to understand everything in the course book the first time you read it. Just do your best to understand it, taking careful notes of anything you’re struggling with. The TAs and staff will happily answer any questions you have during the timetabled sessions. Indeed, that is what they’re there to do. 1.2.4.2 Timetabled practicals The timetabled practicals take place in the APS IT rooms, either Perak IT labs or B56. In each of these sessions, you will work through a number of small exercises to help you consolidate what you’re been working on. You’ll be asked to note down your answers to some of the exercises. The correct answers will be available. TAs and staff are there to help if you get stuck, so don’t suffer in silence. You can (should!) also use this time to ask questions about concepts that are confusing you. You are welcome to use your own computer to complete your work. Keep in mind that the university computers are the only ‘officially supported’ platform. If you run into a problem using your own computer, the TAs and staff will try to help resolve these. Unfortunately, if these prove to be intractable, you will have to use the university computing facilities. It just isn’t fair on other students for teaching staff to spend valuable contact time trying to solve installation / setup problems. 1.2.5 Non-assessed material Although every topic is important, in the sense that it contains material that will help you become a better data analyst, we want to avoid creating too much of an assessment burden in this course. To this end, the material in a few of the chapters is be formally assessed. The Expected learning outcomes chapter tells you what you need to be able to know by the end of the course. If you’re the kind of person who wants to focus on the assessment, you should use that as your guide. As always, feel free ask an instructor (not a TA) for clarification if you’re not sure of what you need to be able to do. 1.2.6 What is required of you? A willingness to learn and to take responsibility for your learning! Data analysis is not the easiest subject in the world, but neither is it the most difficult. It’s worth making the effort. What you learn in this course will form the basis for much of what you do in field course, practical and project work that follows in later semesters. The minimum requirement for the course is that you: attend your designated practical session each week (please ensure that you arrive on time and register, or you will be recorded as absent), complete the preparatory practical work and reading, taking careful notes of anything that you don’t understand, be proactive about your lerning and ask the TAs and staff questions about things you’re struggling with, complete the exercises for each week before the next practical class, and check through your answers to questions using MOLE. How you work through the book is fairly flexible, but remember, the self-study preparatory work lays the foundatation for the material covered in the timetabled practicals. Take our word for it, you won’t get much out of the timetabled sessions if you skip the preparatory work. In each practical session you should aim to complete most, if not all, of the exercises for that practical. If you don’t manage to do this you should try to finish off the work in your own time before the next practical. However, if you have problems with any of the work, staff will help you during the practical sessions, even if it is not the topic designated for that session. So if you need to catch up, there will be opportunities to do so. A word of advice: Don’t let the flexibility of the course tempt you into letting a backlog of work build up. This will compromise your ability to do the assessed work when it is set and will make it difficult to revise for the exam. One last point: the university guidelines assume the total study time associated with a 10 credit module to be about 100 hours. There is an expectation that you will spend significant time outside the timetabled practical classes working on this module. You should aim to spend about 5 hours each week working through the chapters, completing the practical exercises, and finally, working on the assessed project. This leaves you about 40 hours to revise for the formal exam in the new year. 1.2.7 Assessment Assessment of the course will have two components. The first is a short data analysis project in weeks 11-12 of the course, the second is an open book exam in the winter exam period. ‘Open book’ means you will have access to this book during the exam, i.e. you don’t have to memorise everything in here, just understand it! Further details will be given as the course progresses. 1.3 How to use the teaching material 1.3.1 The online course book All of the teaching material will be made available through a single online course book (this website). The book is organised such that it forms a complete, stand-alone introduction to data analysis. You should bookmark this now if you haven’t already done so. There are a couple of very good reasons for delivering the course material this way: Practicality: Most exercises can be completed by building on the examples in the course material. Copying the relevant R code from the course website and pasting it into your script is much more efficient, and less error-prone, than copying by eye from a printed page. A website also allows us to cross-reference topics and link to the odd bit of outside reading. Permanence: Experience suggests that many of you will want to refer to the material in this course after you graduate. However, bits of paper are easy to lose, and because the R landscape is always changing, some elements of the course may be less relevant in a few years time. By putting everything on a website, we can ensure that you will always be able to access a familiar, but up-to-date data analysis course. 1.3.2 Printed material There is a small amount of printed material in this course: Cheat sheets: We will supply you with copies of the dplyr and ggplot2 cheat sheets produced by the people who build RStudio . It may help you to refer to these when you need use either the dplyr or ggplot2 packages in a practical. Assessment information: Although much of the assessment will be done on the computer, any information relating to the assessments will be produced in printed form. This will be handed out in week 10. 1.3.3 How to make best use of the teaching material DO: When working through an exercise, follow the instructions carefully, but also think about what you are doing. Work at your own pace; you are not being assessed on whether you can do an exercise in a particular time. Ask teaching staff for help in the practicals if there are things that you don’t follow, or when things don’t seem to come out the way they should—that’s what they’re there for! Collaborate! If you are not sure you understand something feel free to discuss it with a friend—more often than not this is exactly how scientists resolve and clarify problems in their experimental design and analysis. Be prepared to experiment with R to solve problems that you encounter. You can’t break your R or RStudio by generating errors. When you run into a problem, go back to the line of code that generated the first error and try making a change. Complete each week’s work before the next week’s session. You may be able to complete some sessions quite quickly, others may take more time and require more work on your own outside the timetabled periods. DON’T: Just copy what someone else tells you to do without understanding why you are doing it. You need to understand it for yourself (and you’ll be on your own in the exam). Skip practicals or preparatory work and get behind schedule — there is too much material to assimilate all at once when you get to the assessments. Like all skills data analysis is something you have practice. 1.3.4 Conventions used in the course material The teaching material, as far as possible, uses a standard set of conventions to differentiate between various sorts of information, action and instruction: 1.3.4.1 Text, instructions, and explanations Normal text, instructions, explanations etc. are written in the same type as this paragarph (obvious really), we will tend to use bold to highlight specific technical terms when they are first introduced. Italics are generally used for emphasis and with Latin names. When we want you to do something important or pay particular attentions—e.g., waksing you to write down an answer or giving you a set of instructions—we place the text inside a box like this one: Brown boxes Here is some important text telling you to do something or remember something important. Sometimes it just contains a warning that the next bit will be hard… Please don’t ignore these. At various points in the text you will also come across different coloured boxes that contain additional information: Blue boxes These aim to offer a not-too-technical discussion of how or why something works the way it does. These are things that it may be useful to know, or at least know about, but aren’t necessarily part of the main thread of a section. Red boxes These contain a warning or flag a common gotcha that may trip you up. They highlight potential pitfalls and show you how to avoid them. You will avoid a lot of future mistakes if pay close attention to these. We use block quotations to indicate an example of how a particular statistical result should be presented when you write it in a report: e.g. The mean lengths of male and female locusts differed significantly (t=4.04, df=15, p=0.001), with males being significantly larger. 1.3.4.2 R code, files and RStudio This typeface is used to distinguish R code within a sentence of text: e.g. “We use the summary function to obtain information about an object produced by the lm function.” A sequence of selections from an RStudio menu is indicated as follows: e.g. File ▶ New File ▶ R Script File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV. 1.3.5 Feedback There are a number of ways in which you can obtain feedback on how well you understand the course material 1.3.5.1 Self-assessment questions: At various points in the course material there are questions for you to answer. When you reach one of these, you should be in a position to answer the question —- so make a note of the answer! When you’ve completed the session, you can check your answers using the ‘self-test’ for that particular session on MOLE. You will see if you have the correct answer and in some cases you will also get some additional explanation as to why that answer is right (or wrong!). 1.3.5.2 Each other: Discussing what you are doing with someone as you go along, or working through a problem with someone else, can help clarify your understanding. Please bear in mind, however, that you learn little or nothing by simply copying information from someone else, and when it comes to the assessed project, it must be your own work. 1.3.5.3 Staff: In the practicals you will have opportunities to ask questions and discuss what you are doing with staff and teaching assistants. They are not just there to help you with the practical. You should use them to help you work through any problems you have with the course material, both conceptual and practical. There will also be an opportunity to have topics you raise discussed in later practicals. 1.3.6 Help sessions We will run an open ‘help’ session every Friday from 12-2.00pm, in the B56 IT Room in APS. An instructor will be on hand during this period to answer specific questions about the course material. This room holds about 40 students, so please only attend if you require one-to-one assistance, i.e, don’t just use this session to complete unfinished practicals (unless you are stuck of course). 1.3.7 Overall… We hope that the material is clear and easy to use, and that you find the course useful, or even enjoy it! In a text of this size, which is continually being improved and updated, errors do creep in; if you find something you think is wrong please tell us. If it’s not wrong we will be happy to explain why, and if it is then you will save yourself and others a lot of confusion. Similarly, if you have any comments or suggestions for improving the teaching materials please let us know. 1.4 Health and safety using display screen equipment Although using a computer may not seem like a particularly risky activity you should be aware that you can suffer ill effects if you work at a computer for long periods without observing a few sensible precautions. The standard guidelines are as follows: Make sure that your equipment is properly adjusted: ensure that your lower back is well supported by adjusting the seat back height adjust your chair seat height so that your forearms are level when using the keyboard make sure that the front edge of the keyboard is at least 8-10 cm away from the edge of the desk if you are using a mouse, have it far enough away from the edge of the desk so that your wrist is supported whilst you use it. If you can learn to use the mouse with either hand then this can help avoid strains Do not have your screen positioned in such a way that there is glare or reflections from the windows or room lights on the screen. Maintain good posture. Take regular breaks away from the computer. It is recommended that you take about 10 minutes break every hour. Most Departments will have a Display Screen Trainer or Advisor, who can offer specific advice if you are using a display screen for a substantial amount of time, or if you experience, or anticipate, specific problems. "],
["expected-learning-outcomes.html", "Chapter 2 Expected learning outcomes 2.1 Collecting and using data 2.2 Statistical Concepts 2.3 Simple parametric statistics 2.4 Regression and ANOVA 2.5 Doing more with models 2.6 Experimental design 2.7 Beyond simple models 2.8 Frequency data and non-parameteric tests", " Chapter 2 Expected learning outcomes 2.1 Collecting and using data Given a description of some data, classify variables as numeric vs categorical, ratio vs. interval, and ordinal vs. nominal. Construct simple summaries of numeric variables in R using the mean, sd, var functions. 2.2 Statistical Concepts Explain what sampling error is (in non-technical terms) and understand why it is necessary to quantify sampling error alongside point estimates. Recognise the difference between the distribution of a sample, and the sampling distribution of an estimate derived from that sample. Recognise the difference between the standard deviation (a property of a sample) and the standard error (a property of a sampling distribution). Given a description of an experimental setting, recognise… …the difference between a statistical population and a sample from the population. …the difference between a population parameter and a point estimate of the population parameter. Understand what is meant by the term ‘null hypothesis’ and, given a scenario, state the appropriate null hypothesis for the associated statistical test. Identify whether or not a result is ‘statistically significance’ by examining the p-value it produces. Calculate the standard error of a sample mean when the population distribution of a variable follows a normal distribution. You are not expected to be able to explain or use the the bootstrap or permutation test—this were introduced to help you learn the principles listed above. 2.3 Simple parametric statistics Understand that a one-sample t-test is used to assess whether a population mean is different from a particular reference value (often 0). Understand that a two-sample t-test is used to assess whether two population means are different from one another. Given an experimental scenario and question, choose the correct t-test to use to answer the question. State the assumptions of the one-sample, and two-sample t-tests, and explain how you might check them for a given problem using R. Carry out a one-sample or two-sample t-test using the t.test function, and be able to interpret the output produced by t.test. Write an informative and concise summary of the results from a one-sample or two-sample t-test. State the assumptions of Pearson’s correlation and determine which when it is appropriate to use. Carry out Pearson’s correlation using the cor.test function and interpret the results. Write an informative and concise summary of the results from a correlation analysis. 2.4 Regression and ANOVA Understand what simple linear regression and one-way ANOVA do, and when to use them. Distinguish between situations in which correlation or regression is the most appropriate technique to use. Fit a simple linear regression using the lm function and interpret the output from… …anova to determine the significance of fit …summary to extract the variance explained (\\(R^{2}\\)) and the fitted coefficients (intercept and slope) Calculate predicted \\(y\\)-values from a fitted regression. Fit a one-way ANOVA using the lm function and interpret the global test of significance produced by the anova function. Write an informative and concise summary of the results from a simple linear regression and one-way ANOVA analyses. Your ability to summarise a one-way ANOVA or regression analysis graphically—showing the means and standard errors (ANOVA) or the data and fitted line (regression)—will not be assessed. 2.5 Doing more with models State the assumptions of the simple linear regression and one-way ANOVA models. Use regression diagnostic plots to identify potential problems with the assumptions of a model. Use the plot function with a fitted model object to… …construct a residuals vs. fitted values plot and evaluate the linearity assumption (regression only). …construct a normal probability plot and evaluate the normality assumption. …construct a scale-location plot and evaluate the constant variance assumption. (You are not expected to be able to produce these plots manually using functions like resid and fitted) When a problem with the assumptions of a model has been identified, choose an appropriate mathematical transformation to remedy the problem from… …logarithm (including the “add 1” case when zeros are present) …square root …arcsine square root Identify situations where transforming a variable fails, so that a non-parametric test is required. Determine whether it is appropriate to carry out a multiple comparison test. Where appropriate, carry out a Tukey multiple comparison test using the TukeyHSD or HSD.test functions and interpret the results. 2.6 Experimental design Understand that a paired-sample t-test is used to assess whether the mean difference among paired cases is different from reference value (usually 0). Given an experimental scenario and question, choose the correct t-test to use to answer the question. State the assumptions of the paired-sample t-tests, and explain how you might check them for a given problem using R. Carry out a paired-sample t-test using the t.test function, and be able to interpret the output produced by t.test. Write an informative and concise summary of the results from a paired-sample t-test. 2.7 Beyond simple models Understand what two-way ANOVA and ANCOVA do and when to use them. Understand what is meant by the terms ‘main effect’ and ‘interaction’ and evaluate the likley presence or absence of these effects using an interaction plot. State the assumptions of the two-way ANOVA and ANCOVA models. Fit a two-way ANOVA and ANCOVA using the lm function and interpret the three global tests of significance produced by the anova function. Use the plot function with a fitted model object to… …construct a residuals vs. fitted values plot and evaluate the linearity assumption (ANCOVA only). …construct a normal probability plot and evaluate the normality assumption. …construct a scale-location plot and evaluate the constant variance assumption. Identify the main effect and interaction terms for which it is appropriate to carry out a multiple comparisons test. Where appropriate, carry out Tukey multiple comparison test(s) using the TukeyHSD or HSD.test functions, and interpret the results. Write an informative and concise summary of the results from a two-way ANOVA and ANCOVA analyses. Your ability to summarise a two-way ANOVA or ANCOVA graphically will not be assessed. 2.8 Frequency data and non-parameteric tests Recognise situations where you are studying one or more categorical variables and you need to compare the frequencies of each category (or combinations of categories) in some way. Given an experimental scenario and question… …decide on the appropriate type of analysis to use: goodness-of-fit, or contingency table. …state the null hypothesis of the required test. State the assumptions of the goodness-of-fit and contingency table tests. Carry out a goodness-of-fit test using the chisq.test function. Use the xtabs function to convert a set of counts of different combinations of categories to a table of cross-tabulated counts (when necessary). Carry out a contingency table test of association using the chisq.test function. Write an informative and concise summary of the results from goodness-of-fit and contingency table tests. Recognise that the Wilcoxon signed-rank test …is an alternative to one-sample and paired-sample t-test …is carried out by the wilcox.test function in R Recognise that the Mann-Whitney U-test …is an alternative to two-sample t-test …is carried out by the wilcox.test (note the name!) function in R Recognise that the Kruskal-Wallis test …is an alternative to one-way ANOVA …is carried out by the kruskal.test function in R Recognise that the Spearman’s rank test …is an alternative to Pearson’s correlation and when it is appropriate to use each method …is carried out by the cor.test function in R "],
["programming-prerequisites.html", "Chapter 3 Programming prerequisites 3.1 Starting an R session in RStudio 3.2 Using packages 3.3 Reading data into R", " Chapter 3 Programming prerequisites This chapter gives a quick overview of the prerequisite R skills needed to use this book (we studied these last year). We will use these skills this year, so you may need to spend revising them if you feel that you’re a little rusty. The key R skills you need to have in place to use this book will be revised in the first two practical sessions. The lecture slides will be placed on MOLE after the practical. Transfer students The material in this chapter won’t make any sense at the moment if you are an Environmental Sciences student joining us from Geography, or a student transferring into APS from a different department. Don’t panic! You will have the opportunity to catch up in the first few weeks. 3.1 Starting an R session in RStudio Here is the process you should go through every time you start a new R session: Open up RStudio and set your working directory. Do this via the RStudio menu system: Session ▶ Set Working Directory ▶ Choose Working Directory…. Be sure to choose a sensible location. The working directory is where R will look for data files and R scripts. It’s simplest to use the same working directory in every practical, but it isn’t necessary to do this. Open up a new R script using the RStudio menu system: File ▶ New File ▶ R Script. This will create (but not save) an empty, unnamed R script. Don’t create any other kind of file unless you already know how to use R Notebooks or R Markdown files. There are a few of things that regularly appear at the start of a script, e.g. we often start by clearing the workspace with rm and loading packages with library. Add these preamble chunk of code (and comments!) before doing anything else: # clear the workspace so that we have a &#39;clean sheet&#39; rm(list = ls()) # load and attach the packages we want to use... # 1. &#39;dplyr&#39; for data manipulation library(dplyr) # 2. &#39;ggplot2&#39; for plotting library(ggplot2) Now run the preamble section of the script, i.e. highlight everything and hit Ctrl+Enter. If the library commands didn’t work the relevant package probably isn’t installed yet. Install the package (see below) and try rerunning the script. Look at the label of the tab the script lives in. This will probably be called something like Untitled1 and the label text will be red. This is RStudio signalling that the file has not yet been saved. So after the preamble part of the script is working, save the script in the working directory! Now we’re ready to start developing the script. 3.2 Using packages R packages extend the basic functionality of R so that we can do more with it. In a nutshell, an R package bundles together R code, data, and documentation in a standardised way that is easy to use and share with other users. This book uses a subset of the tidyverse ecosystem of packages: the dplyr package for data manipulation, and the ggplot2 package for making plots. We need to understand how R’s package system works to use these. Here’s the key point: Installing a package, and then loading and attaching the package, are different operations. We only have to install a package once onto our computer, but we have to load and attach the package every time we want to use it in a new R session (i.e. every time we start RStudio). If that doesn’t make any sense, revise the package system chapter in the APS 135 course book. Installing a package can be done via the install.packages function, e.g. install.packages(&quot;dplyr&quot;) There’s no need to leave install.packages statements in an R script. Loading and attaching a package so that it can actually be used happens via the library function, e.g. library(&quot;dplyr&quot;) We do often leave library statements in scripts. 3.3 Reading data into R Last year we made extensive use of data sets that reside inside various R packages. This meant we could use the data without getting bogged down trying to read it into R. We don’t have the luxury of this short cut when we work with our own data, so we’ll adopt more realistic practices in this book. Whenever we need to work with a data set, we will have to first save a copy of it onto our computer, and then read it into R. All the data sets we use are stored as a Comma Separated Value (‘CSV’) text file. The read.csv function can be used to read in such files. The resulting data object in R is a data frame. A data frame is a table-like object that collects together different variables, storing each of them as a named column. We can access the data inside the data frame by referring to particular columns and rows, or manipulate the whole data frame with a package like dplyr. If that last paragraph was confusing, it would be a good idea to work through the data frames chapter in the APS 135 course book. "],
["the-scientific-process.html", "Chapter 4 The scientific process 4.1 Stages in the scientific process 4.2 Hypothesis testing 4.3 Don’t we ever know anything for sure?", " Chapter 4 The scientific process There is something fascinating about science. One gets such a wholesale return of conjecture out of a trifling investment of fact. Mark Twain To do science is to search for patterns, not simply to accumulate facts. Robert MacArthur 4.1 Stages in the scientific process Science is about asking, and answering, the right questions. A number of distinct stages usually occur within this process: making observations, asking questions, formulating hypotheses, and testing predictions. Collectively these are the building blocks of what is known as the scientific method. Exactly how they fit together, and what the philosophical and practical limitations of different approaches are, have been the subject of much debate by philosophers of science. We’re not going to tackle those issues here, but instead try to extract a general working framework for the process of a typical scientific investigation. 4.1.1 Observations Observation — information, or impression, about events or objects. In general the questions we ask are not generated by pure abstract thought, but are a result of observations about the natural world. These may take the form of direct observations we make ourselves, patterns that crop up in data collected for other purposes, in non-specific surveys, and the accumulated works of other people. Imagine, while observing a stream one day, we notice that the freshwater ‘shrimps’ (Gammarus) seem to occur almost entirely under stones; we rarely seem to see them in a patch of open stream bed. Having made observations, it may be necessary to collect some more data to check that this phenomenon is not just a one-off event, or a false impression. Look under a few more stones, watch the same species another day, or in another place, check the literature for similar observations by others. Such observations of biological systems will lead almost automatically into asking questions. 4.1.2 Questions Question — what it is that you want to know; the scope of your investigation. e.g., Why does Gammarus spend most of its time under stones? It is important to keep the question sufficiently focussed. The overall aim of a particular study should be to answer one or (perhaps) two questions. The question of why the tropics are more diverse than the temperate regions is a valid and fascinating question, but it’s probably not be a good choice for a final year project or even a PhD. The next stage is to formulate an hypothesis. 4.1.3 Hypotheses Hypothesis — an explanation proposed to account for observed facts. In general, in biology, the important distinguishing feature of an hypothesis is that it specifies some biological process, or processes, which might account for the observations made. One question will often generate more than one hypothesis: Gammarus occur under stones because: they need to shelter from the current their food (leaf litter) gets trapped and accumulates under stones they are subject to predation by visually hunting fish and need to remain out of sight Formulating hypotheses requires more than just a restatement of the question—it usually embodies some mechanism (though this may not be fully understood) and it will often draw on additional information (e.g., the fact that Gammarus feed on dead leaves). Once we have formulated a set of hypotheses, we need to derive the predictions that follow. 4.1.4 Predictions Prediction — what you would expect to see if the hypothesis was true. Hypotheses are about proposing explanations, but they might not be directly testable. That is, they may not tell us what data to collect, or what pattern to expect in the data. To be able to test an hypothesis we need to make some predictions from that hypothesis. These will be determined both by what we expect to see and what it is possible, or practical, to measure. A prediction is not simply a rephrasing of the hypothesis - it should more or less lead to a statement of the experiment to conduct or observation to make, and type of data to collect: Shelter hypothesis: a greater proportion of Gammarus should be found in the open in streams with slow flow, or in slower flowing areas of a stream. Food hypothesis: Gammarus should not aggregate under stones from which all leaf litter has been removed; Gammarus should aggregate on patches of leaf litter tethered in the open part of the stream bed. Predation hypothesis: Gammarus should aggregate under stones more in streams where fish are present than where they are not; Gammarus may spend less time under stones at night. The ideal prediction is one that is unique to the hypothesis it is based on, so if the prediction is true only one of the hypotheses could have been responsible. It may not always be possible to generate such ‘clean’ predictions, in which case, some combination of predictions may need to be addressed. Additionally, several processes may be operating at the same time. This makes hypothesis testing harder still, because it may be necessary to consider two or more hypotheses, and their corresponding predictions, in combination. For example, Gammarus may be under stones because it prefers the sheltered environment, but also because food accumulates there. In this case we might expect that Gammarus will show a weak aggregative response to shelter alone, or food alone, and a stronger one to them both together. 4.2 Hypothesis testing Once we have firmed up our hypotheses and predictions we are in the position to collect the data to evaluate our ideas. On the basis of these data we will either accept or reject the various hypotheses. The important thing to realise about the process of hypothesis testing is that, in science especially, hypotheses are either rejected, or not rejected, but an hypothesis can rarely, except in trivial cases, be proved. Why is this? Since we cannot be sure that we’ve thought of all the possible hypotheses to explain an observation, finding evidence that supports a prediction does not guarantee that the underlying hypothesis is the only one which could have produced the effect we find. On the other hand, if we find evidence that directly contradicts the prediction(s) from an hypothesis, we can be certain (assuming the prediction and data are not flawed) that the hypothesis cannot be true. An hypothesis which predicted that all conifers should be evergreen could be supported by numerous observations of different conifer species in forests around the world, but is conclusively refuted by the first larch tree we encounter. Having tested our hypothesis, by examining the evidence that its predictions are true, we may accept it as the best current explanation of the observations, or we may reject it as an explanation, and turn to other hypotheses. The same procedure must then be repeated for these hypotheses. This basic cycle of proposing hypotheses and then seeking evidence potentially capable of falsifying them, is, in essence, the idealized model of the scientific process famously proposed by the philosopher of science Karl Popper (1902-1994). It is often termed falsificationism. 4.3 Don’t we ever know anything for sure? The method presented here provides a view of science as one in which we suggest hypotheses, then test them, trying to reject them by finding conclusive counter-evidence, then replacing them with new hypotheses. It all sounds a bit frustrating. In fact of course we do ‘accept’ hypotheses all the time—that is we fail to reject them over and over again. These hypotheses become more accepted and in some sense become regarded as ‘true’ if repeated attempts to test them all fail to provide good counter evidence. In other words, we have some ideas that are doing pretty well in terms of resisting falsification, and we use these as our best estimates of the truth, with the proviso that it is still possible a better idea will come along in due course. The simple process of falsification described above also presents a picture of scientists as neutral, objective creatures, rationally proceeding through cycles of setting up hypotheses, testing them, rejecting them, setting them aside and starting over again. Of course this is not a true reflection of the complex, often messy, business involved in trying to figure out how the world works. Philosophers of science have argued long and hard about how far from this idealized process real science actually is. Various alternative philosophies suggest more ‘realistic’ processes. For example… Thomas Kuhn’s view of science as periods of relative stasis, where people work within an accepted paradigm (a set of views about how things work) despite accumulating evidence that doesn’t always support the paradigm, until finally it is upset by a ‘revolution’ which rejects the entire paradigm, and proposes a new view. This is where the phrase ‘paradigm shift’ originated. Imre Lakatos proposed some resolution of Kuhn’s views, suggesting that scientific ideas were grouped together in ‘research programmes’ concerned with particular endeavours, and that within these there may be core ideas that are not challenged, but other related ideas that are being challenged and adjusted by falsification, and that together these make each research programme progress. This is a very over-simplified sketch of some important ideas. These are well worth reading about, but in practice most philosophical arguments are more focused on how whole areas of science develop. When thinking about how to construct a study of one problem, the basic falsification cycle is a pretty good approach to have in mind. Keep in mind that the process laid out here is not a strict set of rules, but outlines an approach to scientific investigation which is widely considered to provide a rigorous and productive system. As with all such systems understanding the ‘normal’ process is a prerequisite for constructively breaking the rules. "],
["data-and-variables.html", "Chapter 5 Data and variables 5.1 “Observations on material and obvious things” 5.2 Revision: Types of variable 5.3 Accuracy and precision", " Chapter 5 Data and variables The truth is the science of nature has been already too long made only a work of the Brain and the Fancy. It is now high time that it should return to the plainness and soundness of Observations on material and obvious things. Robert Hooke (1665) The plural of anecdote is not data. Roger Brinner 5.1 “Observations on material and obvious things” As Hooke’s observation suggests, science cannot proceed on theory alone. The information we gather about a system stimulates questions and ideas about it and, in turn, can also allow us to test these ideas. In fact the idea of measuring and counting things is so familiar that it is easy to start a project without giving much thought to something as apparently mundane as the nature, quantity and resolution of the data we intend to collect. Nonetheless, the features of the data we collect determine both the types of analyses that can be carried out, and the confidence we can have in the conclusions drawn. We will spend a lot of time considering the statistical tools that can help us extract information from data, but no level of statistical sophistication will enable us to extract information that isn’t there to begin with. So what is there to say about data? The first point to note is that, properly, the word data is the plural of datum (a single piece of information), so we should say “the data are…” not “the data is…”. That said, the use of the word in the singular is becoming more widespread, despite the objections of Grammar Nazis. The second, and much more important, point is that there are many different sorts of data. Examples include spatial maps of species’ occurrence and/or environmental factors, the DNA sequences of individuals, and networks of feeding relationships among species (i.e. food webs). The data we collect in a study can be organised one or more statistical variables. In its loosest sense, statisticians use the word ‘variable’ to mean any characteristic that can be measured or experimentally controlled on different items or objects. People tend to think of variables as numeric quantities, such as height, but there is nothing to stop us working with non-numeric variables, such as colour. Collectively, a set of related variables are referred to as a data set (or just ‘the data’). Confused? Consider a concrete example: the spatial map example above. A minimal data set might comprise two variables containing the x and y position of sample locations, a third variable denoting the presence / absence of a species, and one or more additional variables containing information about the environmental factors we measured. Data and variables in R When using R, we often store a data set in a data frame. Each column in the data frame is one of R’s vectors — numeric, character, etc. The data are said to be tidy if the columns of the data frame correspond to the statistical variables in our data, and each row corresponds to a single observation. This simple connection between abstract statistical concepts and the concrete objects in R is not coincidence. R was designed first and foremost to analyse data. 5.2 Revision: Types of variable When it comes to designing an experimental study or carrying out data analysis, it is very important to understand what sort of data we need, or have, as this affects what we can do with it. The variables in a data set can often be classified as being either numeric or categorical: numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’; categorical variables have values that describe a characteristic of an observation, like ‘what type’ or ‘which category’. Numeric variables can be further characterised according to the type of scale they are measured on (interval vs. ratio scales). Categorical variables can be further characterised according to whether or not they have a natural order (nominal vs. ordinal variables). 5.2.1 Nominal (categorical) variables Nominal variables arise when observations are recorded as categories that have no natural ordering relative to one other. For example: Marital status Sex Colour morph Single Male Red Married Female Yellow Widowed Black Divorced Variables of this type are common in surveys where, for example, a record is made of the ‘type of thing’ (e.g. the species) observed. 5.2.2 Ordinal (categorical) data Ordinal variables occur where observations can be assigned some meaningful order, but where the exact ‘distance’ between items is not fixed, or even known. For example, if we are studying the behaviour of an animal when it meets another individual, it may not be possible to obtain quantitative data about the interaction, but we might be able to score the behaviours in order of aggressiveness: Behaviour Score initiates attack 3 aggressive display 2 ignores 1 retreats 0 Rank orderings are a type of ordinal data. For example, the order in which runners finish a race (1st, 2nd, 3rd, etc.) is a rank ordering. It doesn’t tell us whether it was a close finish or not, but still conveys important information about the result. In both situations we can say something about the relationships between categories: in the first example, the larger the score the more aggressive the response; in the second example the greater the rank the slower the runner. However, we can’t say that the gap between the first runner and the second was the same as between the second and third and we can’t say that a score of 2 is twice as aggressive as a score of 1 (though people sometimes make this kind of mistake). How should you encode different categories in R? We have to define some kind of coding scheme to represent the different categories of a nominal/ordinal variables in computerised form. It was once common to assign numbers to different categories (e.g. Female=1, Male=2). This method was sensible in the early days of computer-based data analysis because it allowed data to be stored efficiently. This efficiency argument is not really relevant on a modern computer. What’s more, there are good reasons to avoid coding categorical variables as numbers: Numeric coding makes it harder to understand the raw data and to interpret the output of a statistical analysis of those data. This is because we have to remember which number is associated with each category. This is particularly problematic when a variable has many categories. Numeric codes are arbitrary and should not be used as numbers in mathematical operations. In the above example, it is meaningless to say 2 [“male”] is twice the size of 1 [“female”]. R usually assumes that a variable containing numeric values is meant to be treated as a number. If we use numeric coding for a categorical variable, R will try to treat the offending variable as a number, which can lead to confusing errors. The take home message: Always use words (e.g., ‘female’ vs. ‘male’), not numbers, to describe categories in R. 5.2.3 Interval scale (numeric) variables Interval scale variables take values on a consistent numeric scale, but where that scale starts at an arbitrary point. Temperature on the Celsius scale is a good example of interval data. We can say that 60\\(^{\\circ}\\)C is hotter than 50\\(^{\\circ}\\)C, and we can say that the difference in temperature between 60\\(^{\\circ}\\)C and 70\\(^{\\circ}\\)C is the same as that between -20\\(^{\\circ}\\)C and -10\\(^{\\circ}\\)C. We cannot say that 60\\(^{\\circ}\\)C is twice as hot as 30\\(^{\\circ}\\)C. This is because temperature on the Celsius scale has an artificial zero value—the freezing point of water. The idea becomes obvious when we consider that temperature can equally well be measured on the Fahrenheit scale (where the freezing point of water is 32\\(^{\\circ}\\)F). 5.2.4 Ratio scale (numeric) variables Ratio scale variables have a true zero and a known and consistent mathematical relationship between any points on the measurement scale. For example, there is a temperature scale that has a true zero: the Kelvin scale. Zero K is absolute zero, where matter actually has no thermal energy whatsoever. Temperature measurements in degrees K are on a ratio scale, i.e. it does make sense to say that 60 K is twice as hot as 30 K. Interval scale numeric variables are quite familiar, because the physical quantities that crop up in science are usually measured on a ratio scale: length, weight, and numbers of organisms are good examples. Continuous or discontinuous? A common confusion with numeric data concerns whether the data are on continuous or discontinuous scales. Ratio data can be either. Many biological ratio data are discrete (i.e. only certain discrete values are possible in the original data), and therefore discontinuous. Count data are an obvious example, e.g. the number of eggs found in a nest, the number of plants recorded in a quadrat, or number of heartbeats counted in a minute. These can only comprise whole numbers, ‘in between’ values are not possible. However, the distinction between continuous and discontinuous data is often not clear cut – even ‘continuous’ variables such as weight are made discontinuous in reality by the fact that our measuring apparatus is of limited resolution (i.e. a balance may weigh to the nearest 0.01 g). So… just keep in mind that the fact that data look (or really are) discontinuous does not mean they are necessarily ordinal data. 5.2.5 Why does the distinction matter? The measurement scale affects what we can do (mathematically) to a numeric variable. We can add and subtract interval scale variables, but we can not divide or multiply them and arrive a meaningful result. In contrast, we can add, subtract, multiply or divide ratio scale variables. Put another way, differences and distances are meaningful for both interval and ratio scales, but ratios are only meaningful for ratio variables (the clue is in the name). 5.2.6 Which is best? All types of data can be useful, but it is important to be aware that not all types of data can be used with all statistical models. This is one very good reason for why it is worth having a clear idea of the statistical tools we intend to use when designing a study. In general, ratio variables are best suited to statistical analysis. However, biological systems often cannot be readily represented as ratio data, or the work involved in collecting good ratio data may be vastly greater than the resources allow, or the question we are interested in may not demand ratio data to achieve a perfectly satisfactory answer. It is this last question that should come first when thinking about a study. What sort of data do we need to answer the question we are interested in? If it is clear at the outset that data on a rank scale will not be sufficiently detailed to enable us to answer the question, then we must either develop a better way of collecting the data, or abandon that approach altogether. If we know the data we’re able to collect cannot address the question, then we should do something else. An obvious, but important point: we can always convert measurements taken on a ratio scale to an interval scale, but we cannot do the reverse. Similarly, we can convert interval scale data to ordinal data, but we cannot do the reverse. That said, it’s good practise to avoid such conversions as they inevitably result in a loss of information. 5.3 Accuracy and precision 5.3.1 What do they mean? The two terms accuracy and precision are used more or less synonymously in everyday speech, but in scientific investigation they have quite distinct meanings. Accuracy – how close a measurement is to the true value of whatever it is you are trying to measure. Precision – how repeatable a measure is, irrespective of whether it is close to the actual value. If we are measuring an insect’s weight on an old and poorly maintained balance, which measures to the nearest 0.1 g, we might weigh the same insect several times and each time get a different weight — the balance is not very precise, though some of the measurements might happen be quite close to the real weight. By contrast you could be using a new electronic balance, weighing to the nearest 0.01g, but which has been incorrectly zeroed so that it is 0.2 g out from the true weight. Repeated weighing here might yield results that are identical, but all incorrect (i.e. not the true value) — the balance is precise, but the results are inaccurate. The analogy often used is with shooting at a target: Figure 5.1: Accuracy and precision It’s obviously important to know how accurate and how precise our data are. The ideal is the situation in the top left target in the diagram, but in many circumstances high precision is not possible and it is usually preferable to make measurements of whose accuracy we can be reasonably confident (bottom left), than more precise measurements, whose accuracy may be suspect (top right). Taking an average of the values for the bottom left target would produce a value pretty close to the centre; taking an average for the top right target wouldn’t help the accuracy at all. 5.3.2 Implied precision – significant figures It’s worth being aware that when we state results, we’re making implicit statements about the precision of the measurement. The number of significant figures we use suggests something about the precision of the result. A result quoted as 12.375 mm implies the measurement is more precise than one quoted as 12.4 mm. A value of 12.4 actually measured with the same precision as 12.735 should properly be written 12.400. When quoting results, look at the original data to decide how many significant figures to use—generally the same number of significant figures will be appropriate. These considerations do not apply in quite the same way when working with discrete data, e.g. precision of measurement is not an issue in recording the number of eggs in a nest. We use 4 not 4.0, but since 4 eggs implies 4.0 eggs it’s fine to quote average clutch size from several nests as 4.3 eggs. However, even with discrete data, if numbers are large then obviously precision is an issue again … a figure of 300 000 ants in a nest is likely to imply a precision of plus or minus 50 000. A figure of 320987 ants implies a rather improbably precise measurement. 5.3.3 How precise should measurements be? The appropriate precision to use when making measurements is largely common sense. It will depend on practicality and the use to which we wish to put the data. It may not be possible to weigh an elephant to the nearest 0.001g, but if we want to know whether the elephant will cause a 10 tonne bridge to collapse, then the nearest tonne will be good enough. On the other hand, if we want to compare the mean sizes of male and female elephants then the nearest 100 kg may be sufficient, but if we plan to monitor the progress of a pregnant female elephant then the nearest 10 kg or less might be desirable. As a rough guide aim, where possible, for a scale where the number of measurement steps is between 30 and 300 over the expected range of the data. So for example, in a study of the variation in shell thickness of dogwhelks on a 300 m transect up a shore, it would be adequate to measure the position of each sampling point on the transect to the nearest metre, but shell thickness will almost certainly need to be measured to the nearest 0.1 mm. 5.3.4 Error, bias and prejudice Error is present in almost all biological data, but not all error is equally problematic. Usually the worst form of error is bias. Bias is a systematic lack of accuracy, i.e. the data are not just inaccurate, but all tend to deviate from the true measurements in the same direction (situations B and D in the ‘target’ analogy above). Thus there is an important distinction in statistics between the situation where the measurements differ from the true value at random and those where they differ systematically. Measurements lacking some precision, such as the situation illustrated in C, may still yield a reasonable estimate of the true value if the mean of a number of values is taken. Avoiding bias in the collection of data is one of the most important skills in designing scientific investigations. Some forms of bias are obvious, others more subtle and hard to spot: Non-random sampling. Many sampling techniques are selective, and may result in biased information. For example, pitfall trapping of arthropods will favour collection of the very active species that encounter traps most frequently. Studying escape responses of an organism in the lab may be biased if the process of catching organisms to use in the study selected for those whose escape response is poorest. Conditioning of biological material. Organisms kept under particular conditions for long periods of time may become acclimatised to those conditions. Similarly, if stocks are kept in a laboratory for many generations their characteristics may change through natural selection. Such organisms may give a biased impression of the behaviour of the organism in natural conditions. Interference by the process of investigation. Often the process of making a measurement itself distorts the characteristic being measured. For example, it may be hard to measure the level of adrenalin in the blood of a small mammal without affecting the adrenalin level in the process. Pitfall traps are often filled with a preservative, such as ethanol, but the ethanol attracts species of insect that normally feed on decaying fruit and use the fermentation products as a cue to find resources. Investigator bias. Measurements can be strongly influenced by conscious or unconscious prejudice on the part of the investigator. We rarely undertake studies without some initial idea of what we are expecting, or we form ideas about the patterns we think we are seeing as the study progresses. For example, rounding up ‘in between’ values in the samples you are expecting to have large values and rounding down where a smaller value is expected, or having another ‘random’ throw of a quadrat when it doesn’t land in a ‘typical’ bit of habitat. The ways in which biases, conscious and unconscious, can affect our investigations are many, often subtle, and sometimes serious. Sutherland (1994) gives an illuminating and sometimes frightening catalogue of the ways in which biases affect our perception of the world and the judgements we make about it. The message is that the results we get from an investigation must always be judged and interpreted with respect to the nature of the data used to derive them—if the data are suspect, the results will be suspect too. "],
["learning-from-data.html", "Chapter 6 Learning from data 6.1 Populations 6.2 Learning about populations 6.3 A simple example 6.4 Now what?", " Chapter 6 Learning from data Statistics is the science of learning from data, and of measuring, controlling, and communicating uncertainty; and it thereby provides the navigation essential for controlling the course of scientific and societal advances Davidian and Louis (2012) The particular flavour of statistics covered in this book is called ‘frequentist statistics’. In one sense, it isn’t very important that we realise this. From a practical perspective, it’s possible to apply frequentist tools by just learning a few basic ‘rules’. Nonetheless, it is helpful to acquire a rough sense of how frequentist ideas work so that these rules can be applied correctly. Ultimately, it’s much easier to use statistical tools if we understand (at least roughly) how they work. The goal of the next few chapters is to provide an overview of the important ideas. We’re not going to cover the challenging mathematical details in any great detail. Instead, we just want to get a conceptual feel for the most important ideas. These can be difficult to understand at first. This is fine. We’ll return to them repeatedly as we introduce different statistical models and tests. We’re going to start, in this chapter, by laying out a somewhat simplified overview of the steps involved in ‘doing frequentist statistics’. We’ll also introduce a few key ideas and definitions along the way. Later chapters will drill down into the really important ideas—things like sampling variation, standard errors, null hypotheses and p-values. These are the concepts we really need to understand. 6.1 Populations When a biologist talks about a population they mean a group of individuals of the same species who interbreed. This definition should be familiar to you. But what does a statistician mean when they talk about populations? The word has a different meaning in statistics. It is a much more abstract concept: a statistical population is any group of items that share certain attributes or properties. This is best understood by example… The readers of this book could be viewed as a statistical population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, and they tend to have similar educational backgrounds and career aspirations. As a consequence of these similarities, APS students tend to be more similar to one another than they would be to a randomly chosen inhabitant of the UK. The different areas of peatland in the UK comprise a statistical population. There are many peatland sites in the UK, and although their ecology varies somewhat from one location to the next they are also very similar in many respects. Peatland habitat is generally characterised by low-growing vegetation and acidic soils. If you visit two different peatland sites in the UK, they will seem quite similar compared to, for example, a neighbouring calcareous grassland (think of the Peak District). A population of plants or animals (as understood by biologists) can obviously be thought of as a statistical population. Indeed, this is often the kind of population a biologist is interested in. The individuals that comprise a biological population share common behaviours, physiology and life history characteristics. Much of organismal biology is concerned with learning about these properties of organisms. Populations are conceptualised as fixed but unknown entities within the framework of frequentist statistics. The goal of statistics is to learn something about populations by analysing data collected from them. It is important to realise that ‘the population’ is defined by the investigator, and the ‘something we want to learn about’ can be anything we know how to measure. Consider the examples again. A social scientist might be interested in understanding the political attitudes of undergraduates, so they might choose to survey a group of students in their university. A climate change scientist might measure the mass of carbon that is stored in peatland areas at sites across Scotland and northern England. A behavioural ecologist might want to understand how much time beavers spend foraging for food, so they might study one of the two Scottish populations. Let’s review the steps involved in these kinds of studies. 6.2 Learning about populations The examples discussed above involve very different kinds of populations and questions. Nonetheless, there are fundamental commonalities in how these questions are addressed. The process can be broken down into a number of distinct steps: Step 1: Refine your questions, hypotheses and predictions This step was discussed in The scientific process chapter. The key point is that we should never begin collecting data until we’ve set out the relevant questions, hypotheses and predictions. This might seem very obvious, but it is surprising how often people don’t get these things straight before diving into data collection. Collecting data without a clear scientific objective and rationale is guaranteed to waste time and energy. Step 2: Decide which population(s) is (are) important The second step is to decide which population (or populations) we should study. This problem is more subtle than it first appears. What constitutes ‘the population’ might be fairly obvious in some kinds of study. In each of the three cases considered above, the corresponding populations we choose to study could be undergraduate students in APS, peatland habitats from across the UK, and beavers in Scotland, respectively. Each of these is an example of what is called an observational study. It is often fairly easy to define what we mean by a population in this kind of study (though that doesn’t mean the population is easy to study—we’ll return to this idea later). What happens if we’re planning an experiment? Imagine that we want to test a prediction that nutrient addition reduces biodiversity in chalk grasslands. We might set up an experiment with two kinds of plots: 1) manipulated plots where we add fertiliser, and 2) control plots where we do nothing. Comparing these would allow us to assess the impact of adding nutrients. There are two statistical populations in this setting: control and manipulated communities. These two populations are completely defined by the experimental design. The nutrient addition plots don’t exist until we do the experiment. The weird mental contortion that a frequentist does is to imagine that the experimental plots are part of some larger, unobserved population of nutrient addition plots. Again, the important point is that statistical populations are something the investigator defines. They might be ‘real’, like the undergraduates in APS, or they might be something that doesn’t really exist in a meaningful way, like a population of not-yet-realised experimentally manipulated plots. In either case, we can use the same statistical techniques to learn about ‘the populations’. Step 3: Decide which variables to study The next step is to decide which features of the population we need to measure to address our question. This comes down to deciding which variable (or variables) we need to measure. In the examples above, the appropriate variables might be things like a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in the biological population. This step is often reasonably straightforward, though some effort may be required to pick among different options. There isn’t a whole lot of ambiguity associated with a physical variable like body mass, but something like ‘political attitude’ needs careful thought. Can we quantify this by studying just one thing, like voting patterns? Probably not. Part of the art of designing a good data collection protocol is deciding what to measure. We discussed some of the considerations in the Data and variables chapter. What matters most is that we choose the right kind of variables to address the substantive research question. Step 4: Decide which population parameters are relevant Once we have decided which variables to study, we have to decide which population parameter is relevant. A population parameter is a numeric quantity that describes a particular aspect of the variables in the populations (to be more precise, it describes a feature of the distribution of the variables in the population). A simple population parameter that many people are familiar with is the population mean. We often study means because they allows us to answer questions like, “how much, on average, of something is present?”. Much of this book is about asking questions of population means, though other population parameters may also be important, e.g. The goal of statistical genetics is to partition variability among individuals—we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case it is population variances we want to learn about. Sometimes we want to understand how two or more aspects of the population are related to one another. In this situation a correlation coefficient might be the right population parameter to focus on. Step 5: Gather a representative sample If we could measure every object in a population we wouldn’t need to use statistics. We’d simply calculate the quantity of interest using an exhaustive sample. Of course, in the real world we have limited time and money to invest in a problem, no matter how important it is. This means we have to work with a sample of a population. A sample is just a subset of the wider population that has been chosen so that it is representative of that population. The word ‘representative’ in that statement is very important. If we can’t collect a representative sample it is very difficult to infer anything useful about the population it came from. For example, if we aim to understand the reproductive characteristics of our favourite study organism, but we only sample older individuals, it will be impossible to generalise our findings if reproductive performance changes with age. The study of how to generate useful samples from a population is an important part of statistics. It falls under the banners of experimental design and sampling theory. These are large, technical topics, and it is well beyond the scope of this introductory book to study them in any great deal. We will touch on the more important practical aspects, particularly in the Principles of experimental design chapter. Step 6: Estimate the population parameter(s) Once we have a representative sample from a population we can calculate something called a point estimate of the population parameter. Remember, the population parameter is always unknown—that’s why we collect samples. A point estimate is simply a number that represents our best guess at the true value of the parameter. For example, if we are interested in a population mean, then the obvious point estimate to use is the mean of the sample (i.e. ‘the average’ we learn how to calculate in school). People often say ‘estimate’ instead of ‘point estimate’, for the simple reason that saying or writing ‘point estimate’ all the time soon becomes tedious. We use both terms in this book. Step 7: Quantify the uncertainty of estimate(s) A point estimate is useless on its own. Why? Because estimates are always derived from a limited sample of the wider population. Even if we’re very careful about how we sample a population, and collect a really big sample, there is no way to guarantee that the composition of the sample exactly matches the population. This means any point estimate that we derive from a sample will always be imperfect, in that it won’t exactly match the true population value. There is always uncertainty associated with an estimate of a population parameter. What can we do about this? We have to find a way to quantify that uncertainty. This bit of the process can be tricky to understand, so we’ll spend a fair bit of time thinking about it in the Sampling error chapter. Step 8: Answer the question! Once we have point estimates and measures of uncertainty we’re finally in the position to start answering questions. We have to go about this carefully. Imagine that we want to answer a seemingly simple question, e.g. “Are there more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?” We might sample a number of sites, measure the stored carbon at each of these, and then calculate the mean of these measurements. What should we conclude if the sample mean is 210 t h-1? We can’t say much until we have a sense of how reliable that mean is likely to be. To answer our question, we have to know how to assess whether or not the difference we observe (210 - 200 = 10) was just a fluke. The tools we’ll learn about in this book are designed to answer a range of different kinds of scientific question. Nonetheless, they all boil down to the same basic question: Is the pattern I see ‘real’, or is it instead likely to be a result of chance variation? To tackle this, we combine point estimates and measures of uncertainty in various ways. Statistical software like R will do the hard work for us, but we still just have to learn how to interpret the results it gives us. 6.3 A simple example The best way to get a sense of how all this fits together is by working through an example. We’ll finish this chapter by introducing a very simple example which we’ll use again in later chapters. We’re going to work through steps 1-6 here. The final two steps are sufficiently challenging that they need their own chapters. Imagine that we are working on a plant species that is phenotypically polymorphic. There are two different ‘morphs’, a purple morph and a green morph. We can depict this situation visually with a map showing where purple and green plants are located on a hypothetical landscape: Figure 6.1: Stylised landscape showing a population of purple and green plants These idealised data were generated using a simulation in R. Let’s proceed as though this were a real situation… Step 1: Refine your questions, hypotheses and predictions Imagine we had previously studied a different population that exhibits the same polymorphism. We’re fairly sure both populations were once connected, but habitat loss over the last few decades has significantly reduced the connectance between them. Our studies with the neighbouring population have shown: The colour polymorphism is controlled by a single gene with two alleles: a recessive mutant allele (‘P’) confers the purple colour, and the dominant wild-type allele (‘G’) makes plants green. Genetic studies have shown that the two alleles are present in a ratio of about 1:1 in the neighbouring population. There seems to be no observable fitness difference between the two morphs in the neighbouring population. What’s more, about 25% of plants are purple. This suggests that the two alleles are in Hardy-Weinberg equilibrium. These two observations indicate that there is no selection operating on the polymorphism. Things are different in the new study population. The purple morph seems to be about as common as the green morph, and some preliminary research indicates that purple plants seem to produce more seeds than green plants. Our hypothesis is, therefore, that purple plants have a selective advantage in the new study population. The corresponding prediction is that the frequency of the purple morph will be greater than 25% in the new study population, as selection is driving the ‘P’ allele to fixation. (This isn’t the strongest test of our hypothesis, by the way. Really, we need to study allele and genotype frequencies, not just phenotypes. Sadly, since the Brexit vote, the government has pulled the research funding for genetic research on plant polymorphism, so this is the best we can do.) Step 2: Decide which population is important Our situation is made up, so consideration of the statistical population is a little artificial. In the real world, we would consider various factors, such as whether or not we can study the whole population or must restrict ourselves to one sub-population. Working at a large scale should produce a more general result, but it could also present a significant logistical challenge. Step 3: Decide which variables to study This step is easy in this example. We could measure all kinds of different attributes of our plants—biomass, height, seed production, etc—but to study the polymorphism, we only need to collect information about the colour of different individuals. This means we are going to be working with a nominal variable that takes two values: ‘purple’ or ‘green’. Step 4: Decide which population parameters are relevant The prediction we want to test is about the purple morph frequency, or equivalently, the percentage, or proportion, of purple plants. Therefore, the relevant population parameter is the frequency of purple morphs in the wider population. We need to collect data that would allow us learn about this unknown quantity. Step 5: Gather a representative sample A representative sample here is one in which every individual on the landscape has the same probability of being sampled. This is what people really mean when they refer to a ‘random sample’. Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality. It is at least easy to do in a simulation. Let’s seen what happens if we sample 20 plants at random… Figure 6.2: Sampling plants. Sampled plants are circled in red The new plot shows the original population of plants, only this time we’ve circled the sampled individuals in red. Step 6: Estimate the population parameter Estimating a frequency from a sample is simple enough. We can express a frequency in different ways. We’ll use a percentage. We found 13 green plants and 7 purple plants in our sample, which means our point estimate of the purple morph frequency is 35%. This is certainly greater than 25%—the value of observed in the original population—but it isn’t that far off. 6.4 Now what? Maybe the purple plants aren’t at a selective advantage after all? Or perhaps they are? We’ll need to use a statistical test of some kind to rigorously evaluate our prediction, but before we can do this, we need to learn a few more concepts. The first of these, sampling error, is the topic of the next chapter. "],
["sampling-error.html", "Chapter 7 Sampling error 7.1 Sampling error 7.2 Sampling distributions 7.3 The effect of sample size 7.4 The standard error 7.5 What is the point of all this?", " Chapter 7 Sampling error In the previous chapter, we introduced the idea that a point estimate of a population parameter will always be imperfect, in that it won’t exactly reflect the true value of that parameter. This uncertainty is always present, so it’s not enough to have just estimated something. We have to know about the uncertainty of the estimate. We can use the machinery of statistics to quantify this uncertainty. Once we have pinned down the uncertainty we can start to provide meaningful answers to our scientific questions. We will arrive at this ‘getting to the answer step’ in the next chapter. First we have to develop the uncertainty idea a bit more by considering things such as sampling error, sampling distributions and standard errors. 7.1 Sampling error Let’s continue with the plant polymorphism example from the previous chapter. So far, we had taken one sample of 20 plants from our hypothetical population and found that the frequency of purple plants in that sample was 35%. This is a point estimate of purple plant frequency based on a random sample of 20 plants. What happens if we repeat the same process, leading to a new, completely independent sample? Here’s a reminder of what the population looked like, along with a new sample highlighted with red circles: Figure 7.1: Plants sampled on the second occasion This time we ended up sampling 16 green plants and 4 purple plants, so our second estimate of the purple morph frequency is 20%. This is quite different from the first estimate. Notice that it is actually lower than that seen in the original study population. The hypothesis that the purple morph is more prevalent in the new study population is beginning to look a little shaky. Nothing about the study population changed between the first and second sample. What’s more, we used a completely reliable sampling scheme to generate these samples (trust us on that one). There was nothing biased or ‘incorrect’ about the way individuals were sampled—every individual had the same chance of being selected. The two different estimates of the purple morph frequency simply arise from chance variation in the selection process. This variation, which arises whenever we observe a sample instead of the whole population, has a special name. It is called the sampling error. (Another name for sampling error is ‘sampling variation’. We’ll use both terms—‘sampling error’ and ‘sampling variation’—in this book because they are both widely used.) Sampling error is the reason why we have to use statistics. Any estimate derived from a sample is affected by it. Sampling error is not really a property of any particular sample. The form of sampling error in any given problem is a consequence of the population distribution of the variables we’re studying, and the sampling method used to investigate the population. Let’s try to understand what all this means. 7.2 Sampling distributions We can develop our simple simulation example to explore the consequences of sampling error. Rather than taking one sample at a time, we’ll simulate thousands of independent samples. The number of plants sampled (‘n’) will always be 20. Every sample will be drawn from the same population, i.e. the population parameter (purple morph frequency) will never change across samples. This means any variation we observe is due to nothing more than sampling error. Here is a summary of one such repeated simulation exercise: Figure 7.2: Distribution of number of purple morphs sampled (n = 20) This bar plot summarises the result from 100000 samples. In each sample, we took 20 individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, … purple individuals, all the way up to the maximum possible (20). We could have converted these numbers to frequencies, but here we’re just summarising the raw distribution of purple morph counts. This kind of distribution has a special name. It is called a sampling distribution. The sampling distribution is the distribution we expect a particular estimate to follow. In order to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. This can often be done using mathematical reasoning. Here, we used brute-force simulation to approximate the sampling distribution of purple morph counts that arises when we sample 20 individuals from our hypothetical population. What does this particular sampling distribution show? It shows us the range of outcomes we can expect to observe when we repeat the same sampling process over and over again. The most common outcome is 8 purple morphs, which would yield an estimate of 8/20 = 40% for the purple morph frequency. Although there is no way to know this without being told, a 40% purple morph frequency is actually the number used to simulate the original, hypothetical data set. So it turns out that the true value of the parameter we’re trying to learn about also happens to be the most common estimate we expect to see under repeated sampling. So now we know the answer to our question—the purple morph frequency is 40%. Of course, we have ‘cheated’ because we used information from 1000s of samples. In the real world we have to work with one sample. The sampling distribution is the key to ‘doing statistics’. Look at the spread (dispersion) of the sampling distribution. The range of outcomes is roughly 2 to 15, which corresponds to estimated frequencies of the purple morph in the range of 10-75% (we sampled 20 individuals). This tells us that when we sample only 20 individuals, the sampling error for a frequency estimate can be quite large. The sampling distribution we summarised above is only relevant for the case where 20 individuals are sampled, and the frequency of purple plants in the population is 40%. If we change either of those two things we end up with a different sampling distribution. That’s what was meant by, “The form of sampling error in any given problem is a consequence of the population distribution of the variable(s) we’re studying, and the sampling method used to investigate this.” Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error, to quantify uncertainty, and we can begin to address scientific questions. We don’t have to work any of this out for ourselves—statisticians have done the hard work for us. 7.3 The effect of sample size One of the most important aspects of a sampling scheme is the sample size (often denoted ‘n’). This is the number of observations—individuals, objects, items, etc—in a sample. What happens when we change the sample size in our example? We’ll repeat the multiple sampling exercise, but this time using two different sample sizes. First we’ll use a sample size of 40 individuals, and then we’ll take a sample of 80 individuals. As before, we’ll take a total 100000 repeated samples from the population: Figure 7.3: Distribution of number of purple morphs sampled (n = 40) Figure 7.4: Distribution of number of purple morphs sampled (n = 80) What do these plots tell us about the effect of changing sample size? Notice that we plotted each of them over the full range of possible outcomes, i.e. the x axis runs from 0-40 and 0-80, respectively, in the first and second plot. This allows us to compare the spread of each sampling distribution relative to the range of possible outcomes. The range of outcomes in the first plot (n = 40) is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65%. The range of outcomes in the second plot (n = 80) is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. The implications of this not-so-rigorous assessment are clear. We reduce sampling error as we increase sample size. This makes intuitive sense: the composition of a large sample should more closely approximate that of the true population than a small sample. How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample 500 individuals: Figure 7.5: Distribution of number of purple morphs sampled (n = 500) Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples we just considered, but still, even with 500 individuals in a sample, we still expect quite a lot of uncertainty in our estimate. The take home message is that we need a lot of data to reduce sampling error. 7.4 The standard error So far, we’ve not been very careful about how we quantify the spread of a sampling distribution—we’ve estimated the approximate range of purple morph counts by looking at histograms. This is fine for an investigation of general patterns, but to make rigorous comparisons, we really need a quantitative measure of sampling error variation. One such quantity is called the standard error. The standard error is actually quite a simple idea, though its definition is a bit of mouthful: the standard error of an estimate is the standard deviation of its sampling distribution. Don’t worry, most people find the definition of the standard error confusing when they first encounter it. The key point is that it is a standard deviation, so it a measure of the spread, or dispersion, of a distribution. The distribution in the case of a standard error is the sampling distribution associated with some kind of estimate. (It is common to use a shorthand abbreviations such “SE”, “S.E.”, “se” or “s.e.” in place of ‘standard error’ when referring to the standard error in text.) We can use R to calculate the expected standard error of an estimate of purple morph frequency. In order to do this we have to specify the value of the population frequency, and we have to decide what sample size to use. Let’s find the expected standard error when the purple morph frequency is 40% and the sample size is 80. First, we set up the simulation by assigning values to different variables to control what the simulation does: purple_prob &lt;- 0.4 sample_size &lt;- 80 n_samples &lt;- 100000 The value of purple_prob is the probability a plant will be purple (0.4: R doesn’t like percentages), the value of sample_size is the sample size for each sample, and the value of n_samples is the number of independent samples we’ll take. That’s simple enough. The next bit requires a bit more knowledge of R and probability theory: raw_samples &lt;- rbinom(n = n_samples, size = sample_size, prob = purple_prob) percent_samples &lt;- 100 * raw_samples / sample_size The details of the R code are not important here (a minimum of A-level statistics is needed to understand what the rbinom function is doing). We’re only showing the code to demonstrate that seemingly complex simulations are often easy to do in R. The result is what matters. We simulated the percentage of purple morph individuals found in 100000 samples of 80 individuals, assuming the purple morph frequency is always 40%. The results are now stored the result in a vector called percent_samples. Here are its first 50 values: ## [1] 42.50 37.50 36.25 30.00 35.00 35.00 51.25 35.00 41.25 36.25 32.50 ## [12] 37.50 32.50 38.75 37.50 36.25 33.75 35.00 42.50 41.25 46.25 35.00 ## [23] 42.50 45.00 43.75 40.00 41.25 52.50 43.75 38.75 38.75 47.50 42.50 ## [34] 40.00 43.75 42.50 36.25 42.50 43.75 43.75 35.00 42.50 35.00 37.50 ## [45] 41.25 40.00 45.00 40.00 41.25 38.75 These numbers are all part of the sampling distribution of morph frequency estimates. How to calculate the standard error? This is defined as the standard deviation of these numbers, which we can get with the sd function: sd(percent_samples) ## [1] 5.494687 Why is this useful? The standard error gives us a means to compare the variability we expect to see in different sampling distributions. When a sampling distribution is ‘well-behaved’, then roughly speaking, about 95% of estimates are expected to lie within a range of about four standard errors. Look at the second bar plot we produced, where the sample size was 80 and the purple morph frequency was 40%. What is the approximate range of simulated values? How close is this to \\(4 \\times 5.5\\)? Those numbers are quite close. In summary, the standard error gives us a way to quantify how much variability we expect to see in a sampling distribution. We said in the previous chapter (Learning from data) that a point estimate is useless without some kind of associated measure of uncertainty. A standard error is one such measure. 7.5 What is the point of all this? Why have we just spent so much time looking at properties of repeated samples from a population? After all, when we collect data in the real world we’ll only have a single sample to work with. We can’t just keep collecting more and more data. We also don’t know anything about the population parameter of interest. This lack of knowledge is the reason for collecting the data in the first place! The short answer to this question is that before we can use frequentist statistics—our ultimate goal—we need to have a sense of… how point estimates behave under repeated sampling (i.e. sampling distributions), and how ‘sampling error’ and ‘standard error’ relate to sampling distributions. Once we understand these links, we’re in a position to make sense of the techniques that underlie frequentist statistics. That’s what we’ll do in the next few chapters. "],
["statistical-significance-and-p-values.html", "Chapter 8 Statistical significance and p-values 8.1 Estimating a sampling distribution 8.2 Statistical significance 8.3 Concluding remarks", " Chapter 8 Statistical significance and p-values This introductory book uses frequentist statistics. Though it isn’t possible to give a detailed description of how this methodology works, we can at least provide a rough indication. In a nutshell, frequentist statistics works by asking what would have happened if we were to repeat an experiment or data collection exercise many times, assuming that the population remains the same each time. This is the basic idea we used to generate sampling distributions of plant colour morph frequency in the last chapter. The details of this procedure depend on what kind of question we are asking, which This obviously varies from one situation to another. What is common to every frequentist technique is that we ultimately have to work out what a sampling distribution of some kind looks like. If we can do that, then we can evaluate how likely a particular result is. This naturally leads onto two of the most important ideas in frequentist statistics: p-values and statistical significance. The goal of this chapter is to introduce these ideas. 8.1 Estimating a sampling distribution Let’s work with the plant polymorphism example again. Our goal is to evaluate whether the purple morph frequency is greater than 25% in the new study population. The suggestion in the preamble of this chapter is that, to get to this point, we need to work out what the sampling distribution of the purple morph frequency estimate looks like. At first glance this seems like an impossible task. If this was a real problem, we can’t use simulations because we don’t know the true frequency of purple morphs in the population. We only have access to a single sample. The solution to this problem is surprisingly simple: since we don’t know much about the population, we use the sample to approximate some aspect(s) of it, and then work out what the sampling distribution of our estimate should look like using this approximation. We’ll unpack this idea a bit more, and then try it out for real. 8.1.1 Overview of bootstrapping There are many different ways to approximate a population from a sample. One of the simplest methods is to pretend the sample is the true population. All we then have to do to get at a sampling distribution is draw new samples from this pretend population. This may sound like ‘cheating’, but it turns out that this is a perfectly valid way to construct approximate sampling distributions. We’ll try to get a sense of how this works using a physical analogy. Imagine that we have written down the colour of every sampled plant on a different piece of paper, and then placed all of these bits of paper into in a hat. We then do the following: Pick a piece of paper at random, record its value (purple or green), put the paper back into the hat, and shake the hat about to mix up the bits of paper. (The shaking here is meant to ensure that each piece of paper has an equal chance of being picked, i.e. we’re taking a random sample.) Now pick another piece of paper (we might get the same one), record its value, and put that one back into the hat, remembering to shake everything up again. Repeat this process until we have a recorded new sample of colours that is the same size as the real sample. We have now have generated a ‘new sample’ from our original one. (This process is called ‘sampling with replacement’. Each artificial sample is called a ‘bootstrapped sample’.) For each bootstrapped sample, calculate whatever quantity is of interest. In our example, this is the proportion of purple plants sampled. Repeat steps 1-4 until we have generated a large number of bootstrapped samples. About 10000 is sufficient for most problems. Although it seems like cheating, this procedure really does produce an approximation of the sampling distribution of the purple plant frequency. It is called bootstrapping (or ‘the bootstrap’). The bootstrap is a sophisticated technique developed by the eminent statistician Bradley Efron. We’re not going to use it to solve real data analysis problems. We’re introducing it because offers an intuitive way to get a sense how frequentist methodology works without having to get stuck into any difficult mathematics. 8.1.2 Doing it for real No one carries out bootstrapping using bits of paper and hat. It could be done, but generating 10000 bootstrapped samples via such a method would obviously take a very long time. On the other hand, computers are very good at carrying out repetitive tasks quickly. Let’s take a look at how to implement the bootstrap for our hypothetical example using R. The best way to understand what follows is to actually work through the example. You’re strongly encouraged to do this… Assume that we had sampled 250 individuals from our new plant population. A data set representing this situation is stored in the Comma Separated Value (CSV) file called ‘MORPH_DATA.CSV’. Download the file from MOLE and place it in the working directory. Next, run through the following steps: Read the data into an R data frame using read.csv, assigning the data frame the name morph.data. Use a function like glimpse (from dplyr) or str to inspect morph.data. Use the View function to inspect the data. Review the following questions: How many observations are in the data set? How many variables are in the data set? What are their names? What kind of variables are they? What values do the different variables take? The point of all this is to ‘sanity check’ the data, i.e. to make sure that we understand the data. Always check our data after reading it into R. There is no point messing about with the likes of dplyr and ggplot2, or carrying out a statistical analysis, until we have done this. If we don’t understand how our data is organised, and what variables we are working with, we’re bound to make a lot of avoidable mistakes. What you should have found is that morph.data contains 250 rows and two columns/variables: Colour and Weight. Colour is a categorical variable (a ‘factor’) and Weight is a numeric variable. The Colour variable contains the colour of each plant in the sample. What about Weight? Actually, we don’t need this now—we’ll use it in the next chapter. Now that we understand the data, we’re ready to implement bootstrapping in R. We’ll use a few new R tricks here. We’ll explain these as we go, but there’s really no need to remember them. Focus on the ‘why’ (the logic of what we’re doing) not the ‘how’. Our goal to construct a sampling distribution for the frequency of purple morphs, so the variable that matters here is Colour. Rather than work with this inside the morph.data data frame, we’re going to pull it out using the $ operator, assign it a name (plant_morphs): # pull out the &#39;Colour&#39; variable plant_morphs &lt;- morph.data$Colour # what values does &#39;plant_morphs&#39; take? levels(plant_morphs) ## [1] &quot;Green&quot; &quot;Purple&quot; # show the first 100 values head(plant_morphs, 100) ## [1] Green Green Green Purple Green Green Green Green Green Green ## [11] Green Green Green Purple Green Green Purple Purple Green Green ## [21] Green Green Green Purple Green Green Green Green Purple Purple ## [31] Green Green Green Purple Purple Green Green Green Green Purple ## [41] Green Purple Green Green Purple Purple Green Green Green Green ## [51] Green Green Purple Purple Purple Green Green Green Purple Green ## [61] Purple Green Purple Green Purple Purple Green Green Purple Green ## [71] Green Purple Purple Green Purple Green Green Green Green Purple ## [81] Purple Green Purple Green Green Green Purple Purple Green Purple ## [91] Green Green Green Green Green Green Purple Green Green Green ## Levels: Green Purple The last line printed out the first 100 values of plant_morphs. This shows that plant_morphs is a simple vector with two categories describing the plant colour information. Next, we calculate and store the sample size (samp_size) and the point estimate of purple morph frequency (mean_point_est) from the sample: # get the sample size form the length of &#39;plant_morphs&#39; samp_size &lt;- length(plant_morphs) samp_size ## [1] 250 # estimate the frequency of purple plants mean_point_est &lt;- 100 * sum(plant_morphs == &quot;Purple&quot;) / samp_size mean_point_est ## [1] 30.8 So… 30.8% of plants were purple among our sample of 250 plants. We’re ready to start bootstrapping. For convenience, we’ll store the number of bootrapped samples we plan to construct in n_samp (i.e. 10000 samples): # number of bootstrapped samples we want n_samp &lt;- 10000 Next, we need to work out how to resample the values in the plant_morphs vector. The sample function will do this for us: # resample the plant colours samp &lt;- sample(plant_morphs, replace = TRUE) # show the first 100 values of the bootstrapped sample head(samp, 100) ## [1] Purple Green Green Green Purple Green Green Green Purple Purple ## [11] Green Purple Green Green Green Green Purple Green Green Green ## [21] Green Green Green Purple Green Purple Green Green Green Green ## [31] Green Purple Purple Purple Green Green Green Green Green Green ## [41] Green Green Purple Green Green Green Purple Green Green Green ## [51] Green Green Green Green Green Green Purple Green Green Purple ## [61] Purple Green Green Green Green Purple Purple Green Green Purple ## [71] Green Green Green Green Purple Green Green Purple Green Purple ## [81] Green Purple Green Green Green Green Green Green Green Green ## [91] Green Purple Green Green Green Green Green Green Purple Green ## Levels: Green Purple The replace = TRUE ensures that we sample with replacement—this is the ‘putting the bits of paper back in the hat’ part of the process. The samp variable now contains a random sample of the values in the true sample. We only need one number from this sample—the frequency of purple morphs: # calculate the purple morph frequencyin the bootstrapped sample first_bs_freq &lt;- 100 * sum(samp == &quot;Purple&quot;) / samp_size That’s one bootstrapped value of the purple morph frequency. Simple, but we need \\(10^{4}\\) values. We don’t want to have to keep doing this over an over ‘by hand’—making second_bs_freq, third_bs_freq, and so on—as this would be slow. As we said earlier, computers are very good at carrying out repetitive tasks. Here is some R code that repeats what we just did n_samp times, storing the resulting bootstrapped samples in a vector called boot_out: boot_out &lt;- replicate(n_samp, { samp &lt;- sample(plant_morphs, replace = TRUE) 100 * sum(samp == &quot;Purple&quot;) / samp_size }) The replicate function replicates an R expression many times and returns the set of results. There’s no need to remember how this works. What have we achieved? The boot_out vector now contains a bootstrapped sample of frequency estimates. Let’s take a quick look at the first 25 values rounded to 2 decimal places: head(boot_out, 25) %&gt;% round(1) ## [1] 26.0 28.0 32.0 26.8 32.8 35.6 31.2 32.4 33.6 30.0 27.6 29.6 30.8 34.0 ## [15] 32.4 38.8 31.6 24.4 31.6 28.4 30.0 32.8 32.4 33.6 32.0 We used the pipe %&gt;% to make a code a bit more readable—this won’t work unless the dplyr package was loaded. The numbers in boot_out represent the values of purple morph frequency we can expect to generate if we repeated the data collection exercise many times, under the assumption that the purple morph frequency is equal to that of the actual sample. This is a bootstrapped sampling distribution. We can use this bootstrapped sampling distribution in a number of ways. Let’s plot it first get a sense of what it looks like. A histogram is a good choice here because we have a reasonably large number of cases: Figure 8.1: Bootstrapped sampling distribution of purple morph frequency The mean of the sampling distribution looks to be round about 31%: mean(boot_out) %&gt;% round(1) ## [1] 30.8 This is essentially the same as the point estimate of purple morph frequency from the true sample. In fact, this is guaranteed to be the case if we construct a large enough sample, because we’re just resampling the data used to estimate the purple morph frequency. A more useful quantity is the bootstrapped standard error (SE) of our estimate. This is defined as the standard deviation of the sampling distribution, so all we have to do is apply the sd function to the bootstrapped sample in boot_out: sd(boot_out) %&gt;% round(1) ## [1] 2.9 The standard error is a very useful quantity. Remember, the standard error is a measure of the precision of an estimate. For example, a large SE would imply that our sample size was too small to reliably estimate the population mean. It is standard practice to summarise the precision of a point estimate by reporting its standard error. Whenever we report a point estimate, we should also report the standard error, like this: The frequency of purple morph plants (n = 250) was 30.8% (s.e. ± 2.9). Notice that we also report the sample size. 8.2 Statistical significance Now back to the question that motivated all the work in the last few chapters. Is the purple morph frequency greater than 25% in the new study population? We can never answer a question like this definitively from a sample. Instead, we have to carry out some kind of probabilistic assessment. To make this assessment, we’re going to do something that looks rather odd. Don’t panic… The ideas in this next section are very abstract. You aren’t expected to understand them straight away, and you won’t be asked to explain them in an assessment. We’ll make two important assumptions: Assume that the true value of the purple morph frequency in our new study population is 25%, i.e. we’ll assume the population parameter of interest is the same as that of the original population that motivated this work. In effect, we’re assuming there is really no difference between the populations. Assume that the form of sampling distribution that we just generated would have been the same if the ‘equal population’ hypothesis were true. That is, the expected ‘shape’ of the sampling distribution would not change if the purple morph frequency really was 25%. That first assumption is an example of a null hypothesis. It is called this because it is an hypothesis of ‘no effect’ or ‘no difference’. The second assumption is necessary for the reasoning below to work. It can be shown to be a reasonable assumption in many situations. Now we ask, if the purple morph frequency in the population is really 25%, what would the corresponding sampling distribution look like? This is called the null distribution—the distribution expected under the null hypothesis. If the second assumption is valid, we can construct the null distribution in R as follows: null_dist &lt;- boot_out - mean(boot_out) + 25 All we did here was shift the bootstrapped sampling distribution left until the mean is at 25%. Here’s what the null distribution looks: Figure 8.2: Sampling distribution of purple morph frequency under the null hypothesis The red line shows where the point estimate from the true sample lies. It looks like the observed purple morph frequency would be quite unlikely to have arisen through sampling variation if the population frequency really was 25%. We can say this because the observed frequency (red line) lies at the end of one ‘tail’ of the sampling distribution. We need to be able to make a more precise statement than this though. We need to quantify how often the values of the bootstrapped null distribution are greater than the value we estimated from the sample. This is easy to do in R: p_value &lt;- sum(null_dist &gt; mean_point_est) / n_samp p_value ## [1] 0.0265 This number (generally denoted ‘p’) is called a p-value. What are we supposed to do with the finding p = 0.0265? This is the probability of obtaining a result equal to, or ‘more extreme’, than that which was actually observed, assuming that the hypothesis under consideration (the null hypothesis) is true. The null hypothesis is one of no effect (or no difference), and so a low p-value can be interpreted as evidence for an effect being present. It’s worth reading that a few times… In our example, it appears that the purple morph frequency we observed is fairly unlikely to occur if its frequency in the new population really was 25%. In biological terms, we take the low p-value as evidence for a difference in purple morph frequency among the populations, i.e. the data supports the prediction that the purple morph is present at a frequency greater than 25% in the new study population. One important question remains: How small does a p-value have to be before we are happy to conclude that the effect we’re interested in is probably present? In practice, we do this by applying a threshold, called a significance level. If the p-value is less than the chosen significance level we say the result is said to be statistically significant. Most often (in biology at least), we use a significance level of p &lt; 0.05 (5%). Why do we use a significance level of p &lt; 0.05? The short answer is that this is just a convention. Nothing more. There is nothing special about the 5% threshold, other than the fact that it’s the one most often used. Statistical significance has nothing to do with biological significance. Unfortunately, many people are very uncritical about the use of this arbitrary threshold, to the extent that it can be very hard to publish a scientific study if it doesn’t contain ‘statistically significant’ results. We just carried out a significance test. It took quite a lot of convoluted reasoning to get there (frequentist statistics is odd). Nonetheless, that rather non-intuitive chain of reasoning underlies all of the statistical tests we use in this course. The good news is that we don’t need to understand the low-level details to use these tools effectively. We just need to be able to identify the null hypothesis being used and understand how to interpret the associated p-values. These ideas are so important that we’ll discuss null hypotheses and p-values some more in the next two chapters. 8.3 Concluding remarks The bootstrap is a very powerful tool in the right hands, but it is an advanced technique that is hard to apply in more complicated settings, e.g. the analysis of complex experiments. In practice, we will use much simpler methods to analyse our data. We only introduced the technique to illustrate how frequentist ideas can be used to decide whether or not an effect is likely to be present. The details vary from one problem to the next, but ultimately, when using frequentist ideas we… assume that there is actually no ‘effect’ (the null hypothesis), where an effect is expressed in terms of one or more population parameters, construct the corresponding null distribution of the estimated parameter by working out what would happen if we were to take frequent samples in the ‘no effect’ situation, (This is why the word ‘frequentist’ is used to describe this flavour of statistics.) then compare the estimated population parameter to the null distribution to arrive at a p-value, which evaluates how frequently the result, or a more extreme result, would be observed under the hypothesis of no effect. "],
["comparing-populations.html", "Chapter 9 Comparing populations 9.1 Making comparisons 9.2 A new example 9.3 Evaluating differences between population means 9.4 A permutation test 9.5 What have we learned?", " Chapter 9 Comparing populations 9.1 Making comparisons Scientific inquiry requires that we evaluate our predictions about natural or experimentally-induced differences between populations. In its simplest form this involves just two populations, e.g. ‘Do male and female locusts differ in length?’ ‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’ ‘Do eagle owls feed on rats of different sizes during winter and summer?’ ‘Do purple and green plants differ in their biomass?’ In this setting, we’re evaluating whether or not two populations are different in some way. In the last chapter we sketched out an approach to evaluate whether the purple morph frequency was different from 25%. We were only considering data from one population, but that 25% number arose from observations of a neighbouring population. That 25% number was itself an estimate that carries with it some uncertainty. We should really have used a methodology that accounts for this uncertainty when making the comparison with the new population. To do this, we have to step through the same kind of process discussed in the last few chapters. This chapter demonstrates how to compare two populations by employing ideas like null hypotheses and p-values. The goal is not really to learn how to compare populations using frequentist techniques. Instead, we want to continue learning how these ideas are used to construct significance tests and evaluate predictions. On the way we’re going to introduce something called a ‘test statistic’. To begin, we’ll introduce a new example… 9.2 A new example We want to tackle the following question: “Is there a fitness difference between the purple and green morphs in the new population?” Let’s step through the process introduced in the Learning from data chapter. Based on various observations that we’ve already discussed, our hypothesis is that purple plants are generally fitter than green plants. Since fitness is strongly correlated with size in plants, we predict that purple morphs will be larger. That’s our question-hypothesis-prediction sorted out. What statistical populations are we interested in? In this new analysis, we conceive each morph to be a separate population, i.e. there are now two statistical populations in play. This change of focus is perfectly valid. Remember, statistical populations are not really concrete things. That’s what we were getting at when we said ‘the populations’ are defined by the investigator. Which variable(s) should we study? One way to address the prediction of size differences would be to measure the dry weight biomass of individuals of each morph. That’s a pretty reliable measure of how ‘big’ a plant is. Dry weight is a numeric variable, measured on a ratio scale (i.e. zero really does mean ‘nothing’). Which population parameter(s) should we work with? Our prediction is that purple morphs will be larger than green morphs, but what do we really mean by that? We probably don’t mean that every purple plant is bigger than every green plant. That’s a very strong prediction, which, in any case, is not something we could ever validate with a sample. Instead, we want to know if purple plants are generally bigger than green plants. This can be thought of as a statement about central tendency—we want to evaluate whether purple plants are larger than green plants, on average, in their respective populations. The population parameters of interest are therefore the mean dry weights of each morph. The next step is to gather appropriate samples. Since this is a made-up example, we’ll cut to the chase. We’ve already seen the samples we’re going to use. When we read in the ‘MORPH_DATA.CSV’ in the previous chapter we found it contained a numeric variable called Weight. This contains our dry weight biomass information. The categorical Colour variable analysed in the previous chapter tells us which kind of colour morph each observation corresponds to. (These data are tidy by the way—each observation is in a separate row and each variable is in only one column.) You should work through this example. Read the data into an R data frame using read.csv, assigning it the name morph.data. Check it over with str or glimpse again. Just do it. Yes, you already know about these data, but it is a good habit to get into. The next step is to calculate point estimates of the mean dry weight of each morph. These are our ‘best guess’ of the population means. It is also useful to know something about the sample size and variability of the samples. We can summarise the variability using the standard deviation of each sample (these are not the standard errors!). Here is how to do this using dplyr: # using morph data... morph.data %&gt;% # ...group the data by colour morph category group_by(Colour) %&gt;% # ... calculate the mean, sd and sample size of weight in each category summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## # A tibble: 2 x 4 ## Colour mean sd samp_size ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Green 707.8424 149.8092 173 ## 2 Purple 766.5638 156.0316 77 This shows that the mean dry weight of the purple morph is greater than that of green morph. The standard deviation estimates indicate that the dry weight of purple morphs is a little more variable than the green morphs. These numbers are just point estimates derived from limited samples of the populations. If we sampled the populations again, sampling variation would lead to a different set of estimates. We’re not yet in a position to conclude that purple morphs are bigger than green morphs. To do this, we need to employ a statistical test of some kind. We’ll evaluate the statistical significance of these differences in the next section. First, we need to visualise the data. We could do this in a variety of ways. As we only have two samples, we may as well summarise the full sample distributions of each morph’s weight. Here is some ggplot2 code to make a pair of histograms: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) + facet_wrap(~Colour, ncol = 1) Figure 9.1: Size distributions of purple and green morph samples (This plot also demonstrates how to use the facet_wrap function to make a multipanel plot based on the values of categorical variable—Colour in this instance). What does this figure tell us? We’re interested in the degree of similarity of the two samples. It looks like purple morph individuals tend to have higher dry weights than green morphs. We already knew this, but that difference could have resulted from the odd outlier (an unusually large or small value). The histograms indicate that there does seem to be a general difference in the size of the two morphs. However, there is also a lot of overlap between the two dry weight distributions, so perhaps the difference between the sample means is just a result of sampling variation? It’s time to use a statistical test… What do people mean when they ‘compare samples’? By comparing the central tendency (e.g. the mean) of different samples, we can evaluate whether or not something we have measured changes, on average, among populations. We do this using the information in the samples to learn about the populations. It’s common to use the phrase ‘comparing samples’ when discussing the statistical tests that underlie these efforts. This is a little misleading though. When someone uses a statistical test to ‘compare samples’, what they are really doing is ‘using information in the sample to compare population parameters’. This distinction may seem unnecessarily pedantic. However, it’s important to be aware of the correct description because this helps us understand what a statistical test is really doing. That said, saying or writing ‘using information in the sample to compare population parameters’ all the time is dull, so we often revert to the phrase ‘comparing samples’. We’ll do this from time to time, but try to keep in mind what we really mean by the ‘comparing samples’ phrase. 9.3 Evaluating differences between population means We’re going to use frequentist concepts to evaluate whether two population means are different. In order to assess the strength of evidence for a difference between the two population means, we have to do something that seems quite odd at first glance. We can break the process down into four steps: First, we assume there is really no difference between the population means. That is, we hypothesize that all the data are sampled from a pair of populations that are characterised by the same population mean. To put it another way, we pretend there is really only one population. We know about this trick. It’s a statement of the null hypothesis. Next, we use information in the samples to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation of ‘no difference between samples’. We summarise this by calculating the null distribution of some kind of test statistic. (We worked directly with the point estimates and their bootstrapped versions in the previous chapter. When dealing with more complicated statistical tests, we tend to work with other kinds of numeric quantities derived from the samples. The generic name for these is ‘test statistic’.) We then ask, “if there were no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we observed in the true sample?” We know about this probability too. It’s a p-value. If the observed difference is sufficiently improbable, then we conclude that we have found a statistically significant result. A statistically significant result is one that is inconsistent with the hypothesis of no difference. This is exactly the same logic applied in the last chapter. There are different ways to go about realising this process. Regardless of the details, they all work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (e.g. a difference between means) is absent. We’re going to use something called a permutation test to evaluate the statistical significance of the difference between the means of the purple and green morph dry weights. Let’s see how this might work in practice with our example. 9.4 A permutation test In our example, a hypothesis of ‘no difference’ between the mean dry weights of purple and green morphs implies the following: if the two morphs are sampled from identical populations, the labels ‘purple’ and ‘green’ are meaningless. These labels don’t carry any real information so they may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows: Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels ‘purple’ and ‘green’ to this new copy of the data. Do this in such a way that the original sample sizes are preserved. The process of assigning random labels is called permutation. (We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used, i.e. we want to hold everything constant apart from the labelling of individuals.) Repeat the permutation scheme many times until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient. For each permuted sample, calculate whatever test statistic captures the relevant information. In our example, this is the difference between the mean dry weight of purple and green morphs in each permuted sample. (It doesn’t matter which way round we calculate the difference.) Compare the observed test statistic—the difference between the mean dry weights of purple and green plants in the true sample—to the distribution of sample statistics from the randomly permuted samples. This scheme is called a permutation test, because it involves random permutation of the group labels. Why is this useful? Each unique random permutation yields an observation from the null distribution of the difference among sample means, under the assumption that this difference is really zero in the population. We can use this to assess whether or not the observed difference is consistent with the hypothesis of no difference, by looking at where it lies relative to this distribution. We can easily implement a permutation test in R. We won’t show the code because it uses quite a few tricks that won’t be needed again. It’s worth having a quick look at the permuted samples. The first 50 values from two permuted samples are: ## Purple Green Green Purple Green Green Green ## 714.3592 693.4924 556.2063 653.6619 672.5207 661.0097 445.1001 ## Green Purple Green Purple Green Purple Purple ## 481.5068 647.6679 858.4820 567.4104 597.1629 718.7132 539.8551 ## Green Green Purple Green Green Green Green ## 753.0170 807.7700 1085.7036 926.4972 617.1209 632.5897 859.7013 ## Purple Green Green Green Purple Green Green ## 815.4634 666.7693 573.5907 694.2877 836.6883 665.8489 617.7895 ## Green Green Purple Green Green Green Purple ## 590.5936 775.8980 686.6790 813.4272 506.3904 566.9971 629.5894 ## Purple Green Green Green Purple Green Purple ## 878.2477 823.1128 542.7877 507.7345 786.2809 912.5058 853.5730 ## Purple Green Green Purple Green Purple Green ## 485.2197 879.7922 852.4711 516.7459 534.4548 702.1948 977.8877 ## Green ## 653.5126 ## Purple Purple Green Green Purple Green Green ## 714.3592 693.4924 556.2063 653.6619 672.5207 661.0097 445.1001 ## Green Purple Green Green Green Green Purple ## 481.5068 647.6679 858.4820 567.4104 597.1629 718.7132 539.8551 ## Purple Purple Purple Green Green Purple Green ## 753.0170 807.7700 1085.7036 926.4972 617.1209 632.5897 859.7013 ## Purple Green Purple Green Green Green Green ## 815.4634 666.7693 573.5907 694.2877 836.6883 665.8489 617.7895 ## Green Green Green Green Green Purple Green ## 590.5936 775.8980 686.6790 813.4272 506.3904 566.9971 629.5894 ## Green Green Green Green Green Green Green ## 878.2477 823.1128 542.7877 507.7345 786.2809 912.5058 853.5730 ## Green Green Green Green Green Green Green ## 485.2197 879.7922 852.4711 516.7459 534.4548 702.1948 977.8877 ## Green ## 653.5126 The data from each permutation are stored as numeric vectors, where each element of the vector is named according to the morph type it corresponds to (these are the labels we referred to above). The set of numbers doesn’t vary among permuted samples. The only difference between them is the labelling. The difference between the mean dry weights in the first permutation is 7.6698902, while in the second sample, the difference is 23.8737542. What really matters here is the distribution of these differences over the complete set of permutations. This is our approximation to the sampling distribution of the difference between means under the null hypothesis, i.e. it’s our null distribution. Here is a histogram that summarises the 2500 mean differences from the permuted samples: Figure 9.2: Difference between means of permuted samples Notice that the distribution is centred at zero. This makes sense. If we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. The red line shows the estimated value of the difference between the mean purple and green morph dry weights in the real sample. This is our test statistic. The key thing to pay attention to here is the location of this value within the null distribution. It looks like the estimated difference would be very unlikely to have arisen through sampling variation if the population means of the two groups were identical. We can say this because the estimated difference lies at the end of one ‘tail’ of the null distribution. We can use the null distribution to quantify the probability of seeing the observed difference under the null hypothesis. Only 4 out of the 2500 permutations ended up being equal to, or ‘more extreme’ (more positive) than, the observed difference. The probability of finding a difference in the means equal to, or more positive than, the observed difference is therefore, p = 0.0016. This is the p-value associated with our significance test. Let’s run through the interpretation of that p-value. Here’s the general chain of logic again… The p-value is the probability of obtaining a test statistic (i.e. the difference between means) equal to, or ‘more extreme’ than, the estimated value, assuming the null hypothesis is true. The null hypothesis is one of no effect (i.e. the difference is 0), so a low p-value can be interpreted as evidence for the effect being present. How low does the p-value have to be before we decide we have ‘enough evidence’? A significance threshold of p &lt; 0.05 is conventionally used in biology. If we find p &lt; 0.05, then we conclude that we found a statistically significant effect. Here’s how this logic applies to our example… The permutation test assumed there was no difference between the purple and green morphs, so the low p-value indicates that the estimated difference between the mean dry weight of purple and green morphs was unlikely to have occurred by chance, if there is really no difference at the population level. This means we should interpret the low p-value as evidence for the existence of a difference in mean dry weight among the populations of purple and green morphs. Since p = 0.0016, we say we found a statistically significant difference at the 5% level. Directional tests The test we just did is a ‘one-tailed’ test. It’s called a ‘one-tailed’ because we only looked at one end of the null distribution. This kind of test is appropriate for evaluating directional predictions (e.g. purple &gt; green). If, instead of testing whether purple plants were larger than green plants, we just want to know if they were different in some way (i.e. in either direction), we should use a ‘two-tailed’ test. These work by looking at both ends of the null distribution. We won’t do this here though—the one- vs two-tailed distinction is discussed in the One-tailed vs. two-tailed tests supplementary chapter. Here’s how we might summarise our analysis in in a written report: ## Warning: package &#39;bindrcpp&#39; was built under R version 3.3.2 The mean dry weight biomass of purple plants (77) was significantly greater than that of green plants (173) (one-tailed permutation test, p&lt;0.05). We report the sample sizes used, the type of test employed, and the significance threshold we passed (not the raw p-value). 9.5 What have we learned? Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to use in a more complex setting. Our goal was not really to learn how permutation tests work. Just as with bootstrapping in the previous chapter, we used it to demonstrate the logic of how frequentist statistics work. In this instance, we wanted to see how to evaluate a difference between two groups. The basic ideas are no different from those introduced in the previous chapter… define what constitutes an ‘effect’ (e.g. a difference between means), then assume that there is ‘no effect’ (i.e. define the null hypothesis), select an appropriate test statistic that can distinguish between the presence of an ‘effect’ and ‘no effect’, (In practice, each kind of statistical test uses a standard test statistic. We don’t have to pick these ourselves.) construct the corresponding null distribution of the test statistic, by working out what would happen if we were to take frequent samples in the ‘no effect’ situation, and finally, use the null distribution and the test statistic to calculate a p-value, to evaluate how frequently the observed difference, or a more extreme difference, would be observed under the hypothesis of no effect. We only really introduced one new idea in this chapter. When evaluating differences among populations we need to work with a single number that can distinguish between ‘effect’ and ‘no effect’. This is called the test statistic. Sometimes this can be expressed in terms of familiar quantities like means (we just used a difference between means above). However, this isn’t always the case, e.g. we can use something called an F-ratio to evaluate the statistical significance of differences among more than two means. We’ll get to this later in the book… "],
["hypotheses-and-p-values.html", "Chapter 10 Hypotheses and p-values 10.1 A few words about the null hypothesis 10.2 Interpreting and reporting p-values 10.3 Biological vs. statistical significance", " Chapter 10 Hypotheses and p-values 10.1 A few words about the null hypothesis When using frequentist statistics, we are always asking what would happen if we continually sampled from a population where the effect we are interested in is not present. This idea of an hypothetical ‘no effect’ situation is so important that it has a special name; it is called the null hypothesis. Every kind of statistical test (in this book at least) works by first specifying a particular null hypothesis. It is not possible to fully understand the results of a statistical test if we don’t know the null hypothesis it relies on. 10.1.1 Hypotheses and null hypotheses When discussing the scientific process, we said that an hypothesis is a statement of a proposed process or mechanism which might be responsible for an observed pattern or effect. We have also seen that in statistics, we encounter ‘hypothesis’ used in a different, and quite specific way. In particular we frequently see the term: null hypothesis (often written in statistics books as H0). The null hypothesis is simply a statement of what we would expect to see if there is no effect of the factor we are looking at (e.g., plant morphology) on the variable that we measure (e.g., dry weight biomass). So in the second plant morph example our null hypothesis was There is no difference in mean biomass of purple and green plants. All frequentist statistical tests work by specifying a null hypothesis and then evaluating the observed data to see if they deviate from the null hypothesis in a way that is inconsistent with sampling variation. This may seem like a rather odd approach, but this really is how frequentist tests work. It is important to be aware of what a null hypothesis is, and what it is used for, so that we can interpret the results of statistical tests. However, in a general discussion of an analysis we normally refer to the effect we are actually interested in. This is called the test hypothesis, or the alternative hypothesis (often denoted H1 in statistics books). The alternative hypothesis is essentially a statement of the effect we are expecting (or hoping!) to see, e.g., purple and green plants differ in their mean size. It is a statement of whatever is implied if the null hypothesis is not true. Having got all the types of hypothesis sorted out, we can then use a particular frequentist technique to evaluate the observed result against that expected if the null hypothesis was true. The test gives us a probability (p-value) telling us how likely it is that we would have got the result we observe, or a more extreme result, if the null hypothesis was really true. If the value is sufficiently small we judge it unlikely that we would have seen this result if the null hypothesis was true. Consequently, we say we reject the null hypothesis (i.e. reject the notion that there is no difference). This is not the same as ‘proving’ the alternative hypothesis is true. We can’t prove anything by collecting data or carrying out an experiment. If the p-value is large, then it is quite likely that we could have got the observed result if the null hypothesis was true, i.e. it is due to sampling variation. In this case we cannot reject the null hypothesis. Note that in this situation we say that we “do not reject the null hypothesis”. This is not the same as accepting that the null hypothesis is true, paradoxical though this may seem. One obvious reason for this is that if we only have a small sample then there may be an effect of the factor we are looking at, but we simply can’t detect it because we don’t have enough data. 10.2 Interpreting and reporting p-values It is important to understand the meaning of the probabilities generated by frequentist tests. We have already said a p-value is the proportion of occasions on which we would expect to see a result at least as extreme as the one you actually observed if the null hypothesis (of no effect) was true. Conventionally, we accept a result as statistically significant if p &lt; 0.05 (also expressed as 5%). This threshold is called the significance level of a test. We’ve said it before but it is worth repeating: there is nothing special about the p &lt; 0.05 significance level! It is just a widely used convention. Which significance level should you use We will always use the p = 0.05 threshold in this book. You need to remember this fact, because we aren’t always going to remind you of it. A probability of 0.05 is a chance of 1 in 20. This means that if there really was no effect of the factor we are investigating, we would expect to get a result significant at p=0.05 about 5 times in 100 samples. To envisage it more easily, it is slightly less than the chance of tossing a coin 4 times and getting 4 heads in a row (p=0.0625). It’s not all that rare really. This puts a ‘significant’ result into context. Would we launch a new drug on the market or bring a prosecution for pollution on the evidence of the strength of four heads coming up in a row when a coin is tossed? Well of course such things are unlikely to hinge on a single test, but it is always worth bearing in mind what ‘significance’ actually means. In general, the smaller the p-value the more confident one can be that the effect we see is ‘real’. For a given analysis, a probability of p=0.001 provides stronger evidence for an effect being present than p=0.01. For this reason, in some critical applications such as drug testing the significance threshold may be lower than we use in biology. The costs of using a more stringent threshold is that this increases the possibility of false negatives—we are more likely to fail to detect an effect when it is present by adopting a lower significance threshold. 10.2.1 Careful with those p-values This is a good time to issue an important warning about p-values. Frequentist p-values are counter-intuitive quantities that are easily (and often) misinterpreted. Whole books have been written about the problems associated with them. We don’t have time to really cover the issues here, but here are a few key observations: Scientists tend to use p=0.05 to define ‘significance’, but p=0.055 is really no different from p=0.045. It would be irrational to reject an idea completely just on the basis of a result of p=0.055, while at the same time being prepared to invest large amounts of time and money implementing policies based on a result of p=0.045. The exact value of p is affected by the size of the true effect being studied, the amount of data being analysed, and how appropriate a statistical model is for those data. It’s very easy to arrive at a tiny p-value, when an effect is weak or even absent, by using a statistical model that is inappropriate for the data in hand. The relationship between the ‘strength of an effect’ and its associated p-value is a complicated one. It is simply not correct to equate the size of a p-value with the weight of evidence for the effect being present, nor is correct to interpret a p-value a statement about how big the effect is. Take home message: p-values are hard to interpret, and should only be used as one line of evidence when answering scientific questions. They are not the ‘final word’ on truth. 10.2.2 Presenting p-values R will typically display p-values from a statistical significance test to six decimal places (e.g. p = 0.003672). However, when we write about them, the results from tests are usually presented as one of the following four categories: p &gt; 0.05, for results which are not statistically significant (sometimes also written as ‘NS’), p &lt; 0.05, for results where 0.01 &lt; p &lt; 0.05, p &lt; 0.01, for results where 0.001 &lt; p &lt; 0.01, p &lt; 0.001 for results where p &lt; 0.001, This style of presentation stems from the fact that statistical tests often had to be calculated by hand in the days before everyone had access to a computer. The significance of the result was difficult to calculate directly, so it would have been looked up in a special table. We still use this style because the value of p does not have a simple interpretation in terms of weight of evidence or effect sizes. Knowing which category a p-value falls into provides sufficient information to roughly judge ‘how significant’ the result is. Significance thresholds vs. p-values The significance level is used to determine whether or not a result is deemed to be ‘statistically significant’. We will always adopt p &lt; 0.05 in this book, and we will use the above categories to report the results of a test. Don’t confuse the category used to report the p-value with the actual significance level an investigator is using. Just because someone writes ‘p &lt; 0.01’ when they report the results of a test, it does not mean that they were working at the 1% significance level (p &lt; 0.01). It’s usually sufficient to use the four categories above when writing about the significance of a statistical test, though occasionally, giving the actual probability can be appropriate. For example, it can be informative to know that a test yielded p = 0.06 rather than simply quoting it just as p &gt; 0.05 or NS. This is because p is so close to the significance threshold. While not wholly convincing, it is still suggestive of the possibility that an effect is present. The asterisks convention It is common to see ranges of probabilities coded with asterisks in tables and figures: * for p = 0.05…0.01, ** for p = 0.01…0.001, *** for p &lt; 0.001. This is common in tables and figures as it is a more compact and visually obvious representation than numbers. Never use the asterisks convention in the text of a report. 10.3 Biological vs. statistical significance A final, but vital, point: do not confuse statistical significance with biological significance. A result may be statistically highly significant (say p &lt; 0.001) but biologically trivial. To give a real example, in a study of the factors determining the distribution of freshwater invertebrates in a river, the pH of water was measured in the open water and in the middle of the beds of submerged vegetation. There was a statistically significant difference in pH (p &lt; 0.01) but the mean pH values were 7.1 in the open water and 6.9 in the weeds. This is a very small effect, and almost certainly of no importance at all to the invertebrates. The significance of a result depends on a combination of three things (1) the size of the true effect in the population, (2) the variability of the data, and (3) the sample size. Even a tiny effect can be significant if the sample size is very large. Do not automatically equate a significant result with a large biological effect. Plot the data, inspect the estimates, and consider the biological implications of the difference. The statistical results provide some guidance in separating genuine differences from random variation, but they can’t tell us whether the difference is biologically interesting or important—that’s the scientist’s job! "],
["parametric-statistics.html", "Chapter 11 Parametric statistics 11.1 Introduction 11.2 Mathematical models 11.3 The normal distribution", " Chapter 11 Parametric statistics 11.1 Introduction The majority of statistical tools that we use in this book share one important feature: they are underpinned by a mathematical model of some kind. Because a mathematical model is lurking in the background, this particular flavour of statistics is known as parametric statistics. In this context, the word ‘parametric’ refers to the fact that the behaviour of such a model is defined by one or more quantities known as ‘parameters’. We aren’t going to study the mathematical details of these models in any detail. This isn’t a maths book. However, it’s important to at least understand the assumptions underlying a statistical model. Mathematical assumptions are the aspects of a system that we accept as true, or at least nearly true. If these aren’t reasonable for a given situation, we can’t be sure that the results of the corresponding analysis (e.g. a statistical test) will be reliable. We always need to evaluate the assumptions of an analysis to determine whether or not we trust it. Ultimately, we want to understand, in rough terms at least, how a model and its assumptions lead to a particular statistical test. We explored a number of concepts from frequentist statistics in the last few chapters, such as sampling variation, null distributions, and p-values. These ideas will crop up time and time again throughout the book. By thinking about models and their assumptions we can begin to connect the abstract ideas in the last few chapters to the practical aspects of ‘doing statistics’. 11.2 Mathematical models A mathematical model is a description of a system using the language and concepts of mathematics. A statistical model is a particular class of mathematical model that describes how samples of data are generated from a hypothetical population. We’re going to consider only a small subset of the huge array of statistical models people routinely use. In conceptual terms, the statistical models we use describe data in terms of a systematic component and a random component: \\[\\text{Observed Data} = \\text{Systematic Component} + \\text{Random Component}\\] The systematic component of a model describes the structure, or the relationships, in the data. When people refer to ‘the model’, this is the bit they usually care about. The random component captures the left over “noise” in the data. This is essentially the part of the data that the systematic component of the model fails to describe. This is best understood by example. In what follows we’re going to label the individual values in the sample \\(y_i\\). The \\(i\\) in this label indexes the individual values; it takes values 1, 2, 3, 4, … and so on. We can think of the collection of the \\(y_i\\)’s as the variable we’re interested in. The simplest kind of model we might consider is one that describes a single variable. A model for these data can be written: \\(y_i = a + \\epsilon_i\\). With this model, the systematic part is given by \\(a\\). This is usually the population mean. The random component is given by \\(\\epsilon_i\\). The \\(\\epsilon_i\\) is a model variable that describes how the individual values deviate from the mean. A more complicated model is one that considers the relationship between the values of \\(y_i\\) and another variable. We’ll call this second variable \\(x_i\\). A model for these data could be written as: \\(y_i = a + b \\times x_i + \\epsilon_i\\). The \\(a + b \\times x_i\\) bit of this—the equation of a straight line with an intercept \\(a\\) and slope \\(b\\)—is the systematic component. The random component is given by the \\(\\epsilon_i\\). Here, this model variable describes how the individual values deviate from the line. Each of these two descriptions represents a partially specified statistical model. We need to make one more assumption to complete them. What is missing is a description of the distribution of the \\(\\epsilon_i\\). In rough terms, a statement about a variable’s distribution is a statement how likely different values are. In this book, this assumption is almost always the same: we assume that the \\(\\epsilon_i\\) are drawn from a normal distribution… 11.3 The normal distribution Most people will come across the normal distribution at one point or another, though they may not realise it at the time. Here’s a histogram of 100000 values drawn from a normal distribution (the details don’t matter here): Figure 11.1: Distribution of a large sample of normally distributed variable Does that look familiar? The normal distribution is sometimes called the ‘Gaussian distribution’, or more colloquially, the ‘bell-shaped curve’. We don’t have time in this book to really study this distribution in much detail, nor is it really necessary that we do this. We’ll just list some key facts about the normal distribution that we need to refer to from time to time: The normal distribution is appropriate for numeric variables measured on an interval or ratio scale. Strictly speaking, the variable should also be continuous, though a normal distribution can provide a decent approximation for some kinds of discrete numeric data. The normal distribution is completely described by its mean (a measure of ‘central tendency’) and its standard deviation (a measure of ‘dispersion’). If we know these two quantities for a particular normal distribution, we know everything there is to know about that distribution. If a variable is normally distributed, then about 95% of its values will fall inside an interval that is 4 standard deviations wide: the upper bound is equal to the \\(\\text{Mean} + 2 \\times \\text{S.D.}\\); the lower bound is equal to \\(\\text{Mean} - 2 \\times \\text{S.D.}\\). When we add or subtract two normally distributed variables to create a new variable, the resulting variable will also be normally distributed. Similarly, if we multiply a normally distributed by a number to create a new variable, the resulting variable will still be normally distributed. The mathematical properties of the normal distribution are very well understood and many of these properties make the distribution easy to work with. This has made it possible for mathematicians to work out how the sampling distribution of means and variances behave when the underlying variables are normally distributed. This knowledge underpins many of the statistical tests we use in this book. 11.3.1 Standard error of the mean Let’s consider a simple example. Say that we want to estimate the standard error of the sampling distribution of a mean. If we’re happy to assume that the sample was drawn from a normal distribution, then there’s no need to resort to computationally expensive techniques like bootstrapping to work this out. Instead, there is a well-known formula for calculating the standard error we need. If \\(s^2\\) is the variance of the sample, and \\(n\\) is the sample size, the standard error is given by: \\[\\text{Standard error of the mean} = \\sqrt{\\frac{s^2}{n}}\\] That’s it, if we know the variance and the size of a sample, it’s easy to estimate the standard error of its mean. In fact, as a result of rule #4 above, we can calculate the standard error of any quantity that involves adding or subtracting the means of samples drawn from normal distributions. 11.3.2 The t distribution The normal distribution is usually the first distribution people learn about. The reasons for this are: 1) it crops up a lot because of something called ‘the central limit theorem’ and 2) many other distributions are related to the normal distribution. One of the most important of these ‘other distributions’ is Student’s t-distribution1. This arises when… we take a sample from a normally distributed variable, estimate the population mean from the sample, and then divide the mean by its standard error (i.e. calculate: mean / s.e.). The sampling distribution of this new quantity has a particular form. It follows a Student’s t-distribution. Student’s t-distribution arises all the time in relation to means. For example, what happens if we take samples from a pair of normal distributions, calculate the difference between their estimated means, and then divide this difference by its standard error? The sampling distribution of the scaled difference between means also follows a Student’s t-distribution. Because it involves rescaling a mean by its standard error, the form of the resulting t-distribution only depends on one thing: the sample size. This may not sound like an important result, but it is because it allows us to construct simple statistical tests to evaluate differences between means. We’ll use this result in the next two chapters as we learn about so-called ‘t-tests’. Why is it called Student’s t? The t-distribution was discovered by W.G. Gosset, a statistician employed by the Guinness Brewery. He published his statistical work under the pseudonym of ‘Student’, because Guinness would have claimed ownership of his work if he had used his real name.↩ "],
["one-sample-t-tests.html", "Chapter 12 One sample t-tests 12.1 When do we use one-sample t-test? 12.2 How does the one-sample t-test work? 12.3 Carrying out a one-sample t-test in R", " Chapter 12 One sample t-tests 12.1 When do we use one-sample t-test? The one-sample t-test is the simplest of statistical tests. It is used in situations where we have a sample of numeric variable from a population, and we need to compare the population mean to a particular value. The one-sample t-test uses information in the sample to evaluate whether the population mean is likely to be different from this value. The expected value might be something predicted from theory, or some other prespecified value we are interested in. Here are a couple of examples: We have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If we have data on the actual time spent by 25 animals observed foraging in the patch, we could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test. We are monitoring sea pollution and have a series of water samples from a beach. We wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach can be regarded as greater than the legislated limit. A one-sample t-test will enable us to test whether the mean value for the beach as a whole exceeds this limit. 12.2 How does the one-sample t-test work? Imagine we have taken a sample of a variable (called ‘X’) and we want to evaluate whether the mean is different from some number. Here’s an example of what these data might look like, assuming a sample size of 50 was used: Figure 12.1: Example of data used in a one-sample t-test The red line shows the sample mean, and the blue line shows the expected value (this is 10, so this example could correspond to the foraging study mentioned above). The observed sample mean is about one unit larger than the expected value. The question is, how do we decide whether the population mean is really different from the expected value? Perhaps the difference between the observed and expected value is due to sampling variation. Here’s how a frequentist tackles the question: We have to first set up an appropriate null hypothesis, i.e. an hypothesis of ‘no effect’ or ‘no difference’. The null hypothesis in this instance is that the population mean is equal to the expected value. We then have to work out what the sampling distribution of the mean looks like under this null hypothesis. This is the null distribution. We use the null distribution to assess how likely the observed result is under the null hypothesis. There are some minor differences in the details, but this chain of reasoning is no different from that developed in the bootstrapping example considered in the Statistical significance and p-values chapter. The new idea is that now we will make an extra assumption. The key assumption of one-sample t-test that the variable is normally distributed in the population. The distribution above look roughly bell-shaped, so it seems plausible that it was drawn from a normal distribution. Now, because we’re prepared to make the normality assumption, the whole process of carrying out the statistical test is very simple. The consequence of the normality assumption is that the null distribution will have a known mathematical form—it’s related to the t-distribution. We can use this knowledge to construct the test of statistical significance. But instead of using the whole sample, as we did with bootstrapping, we only need three pieces of information to construct the test: the sample size, the sample variance, and the sample means. No resampling of data is involved. So how does a one-sample t-test it actually work? It is carried out as follows: Step 1. Calculate the mean. That’s simple enough. This is our ‘best guess’ of the unknown population mean. However, its role in the one-sample t-test is to allow us to construct a test statistic in the next step. Step 2. Estimate the standard error of the sample mean. This gives us an idea of how much sampling variation we expect to observe. The standard error doesn’t depend on the true value of the mean, so the standard error of the sample mean is also the standard error of any mean under any particular null hypothesis. This step boils down to applying a simple formula involving the sample size and the standard deviation of the sample: \\[\\text{Standard Error of the Mean} = \\sqrt{\\frac{s^2}{n}}\\] …where \\(s^2\\) is the square of the standard deviation (the sample variance) and \\(n\\) is for the sample size. This is the formula introduced in the Parametric statistics chapter. The standard error of the mean gets smaller as the sample sizes grows or the sample variance shrinks. Step 3. Calculate a ‘test statistic’ from the sample mean and standard error. We calculate this by dividing the sample mean (step 1) by its estimated standard error (step 2): \\[\\text{t} = \\frac{\\text{Sample Mean}}{\\text{Standard Error of the Mean}}\\] Why is this useful? If our normality assumption is reasonable this test-statistic follows a t-distribution. This is guaranteed by the normality assumption. So this particular test statistic is also a t-statistic. That’s why we label it t. This knowledge leads to the final step… Step 4. Compare the t-statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the difference between observed and expected value. We calculate the probability that we would have observed a difference with a magnitude as large as, or larger than, the observed difference, if the null hypothesis were true. That’s the p-value for the test. We could step through the actual calculations involved in these steps in detail, using R to help us, but there’s no need to do this. We can let R handle everything for us. But first, we should review the assumptions of the one-sample t-test. 12.2.1 Assumptions of the one-sample t-test There are a number of assumptions that need to be met in order for a one-sample t-test to be valid. Some of these are more important than others. We’ll start with the most important and work down the list in order of importance: Independence. People tend to forget about this one. We’ll discuss the idea of independence later when we consider principles of experimental design. For now, we just need to state why the assumption matters: if the data are not independent the p-values generated by the one-sample t-test will smaller than they should be. Measurement scale. The variable being analysed should be measured on an interval or ratio scale, i.e. it should be a numeric variable. It doesn’t make much sense to apply a one-sample t-test to a variable that isn’t measured on one of these scales. Normality. The one-sample t-test will only produce completely reliable p-values if the variable is normally distributed in the population. This assumption is less important than many people think. The t-test is fairly robust to mild departures from normality when the sample size is small, and when the sample size is large the normality assumption matters even less. We don’t have the time to properly explain why the normality assumption is not too important for large samples, but we will at least state the reason: it is a consequence of something called the ‘central limit theorem’. How do we evaluate these assumptions? The first two are really aspects of experimental design, i.e. we can only evaluate them by thinking carefully about how the data were gathered and what was measured. What about the 3rd assumption? One way to evaluate the normality assumption is by plotting the sample distribution using something like a histogram or a dot plot. If the sample size is small, and the sample looks approximately normal when we visualise its distribution, then it is probably fine to use the t-test. If we have a large sample we don’t need to worry much about moderate departures from normality. It’s hard to define what constitutes a ‘large’ sample, but 100s of observations would often be safe. 12.3 Carrying out a one-sample t-test in R You should work through the example in this section. We’re going to use the plant morph example again to learn how to carry out a one-sample t-test in R. Remember, the data were ‘collected’ to 1) compare the frequency of purple morphs to a prediction and 2) compare the mean dry weight of purple and green morphs. Neither of these questions can be tackled with a one-sample t-test. Instead, let’s pretend that we have unearthed a report from 30 years ago that found the mean size of purple morphs to be 710 grams. We want to evaluate whether the mean size of purple plants in the contemporary population is different from this expectation, because we think they may have adapted to local conditions. Read the data in MORPH_DATA.CSV into an R data frame, giving it the name morph.data. We only need the purple morph data for this example, so we need to also filter the data to get hold of only the purple plants: # read in the data morph.data &lt;- read.csv(file = &quot;MORPH_DATA.CSV&quot;) # get just the purple morphs morph.data &lt;- filter(morph.data, Colour == &quot;Purple&quot;) Next, we need to explore the data… 12.3.1 Visualising the data and checking the assumptions We should calculate a few summary statistics and then visualise the sample distribution of purple morph dry weights. We already did this in the Comparing populations chapter. Here is the dplyr code to produce the descriptive statistics again: morph.data %&gt;% summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## mean sd samp_size ## 1 766.5638 156.0316 77 We have 77 purple plants in the sample. Not bad, but we should keep an eye on the normality assumption. Let’s check this assumption: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) Figure 12.2: Size distributions of purple morph dry weight sample These is nothing too ‘non-normal’ about this sample distribution—it’s roughly bell-shaped—so it seems reasonable to assume it came from normally distributed population. 12.3.2 Carrying out the test It is fairly straightforward to carry out a one-sample t-test in R. The function we use is called t.test (no surprises there). We read the data into a data frame called morph.data. This has two columns: Weight contains the dry weight biomass of purple plants, and Colour is an index variable that indicates which sample (plant morph) an observation belongs to. We don’t need the Colour column at this point. Here’s the R code to carry out a one-sample t-test: t.test(morph.data$Weight, mu = 710) We have suppressed the output because we want to first focus on how to use t.test function. We have to assign two arguments to control what the function does: The first argument (morph.data$Weight) is simply a numeric vector containing the sample values. We can’t give t.test a data frame when doing a one-sample test. Instead, we have to pull out the column we’re interested in using the $ operator. The second argument (called mu) sets the expected value we want to compare the mean to, so mu = 710 tells the function to compare the mean to a value of 710. This can be any value we like, depending on the question we’re asking. That’s it for setting up the test. Let’s take a look at the output: t.test(morph.data$Weight, mu = 710) ## ## One Sample t-test ## ## data: morph.data$Weight ## t = 3.1811, df = 76, p-value = 0.002125 ## alternative hypothesis: true mean is not equal to 710 ## 95 percent confidence interval: ## 731.1490 801.9787 ## sample estimates: ## mean of x ## 766.5638 The first line tells us what kind of t-test we used. This says: One Sample t-test. So we know that we used the one-sample t-test. The next line reminds us about the data. This says: data: morph.data$Weight, which is R-speak for ’we compared the mean of the Weight variable to an expected value. Which value? This is given later. The third line of text is the most important. This says: t = 3.1811, df = 76, p-value = 0.002125. The first part of this, t = 3.1811, is the test statistic, i.e. the value of the t-statistic. The second part, df = 76, summarise the ‘degrees of freedom’. This is essentially a measure of how much power our statistical test has (see the box below). The third part, p-value = 0.002125, is the all-important p-value. The p-value indicates that there is a statistically significant difference between the mean dry weight biomass and the expected value of 710 g (p is less than 0.05). Because the p-value is less than 0.01 but greater than 0.001, we report this as ‘p &lt; 0.01’. Read through the Presenting p-values section again if this logic is confusing. Don’t ignore the fourth line of text (alternative hypothesis: true mean is not equal to 710). This reminds us what the alternative to the null hypothesis is (H1). It tells us what expected value was used in the test (710). The next two lines show us the ‘95% confidence interval’ for the difference between the means. We don’t really need this information, but we can think of this interval as a rough summary of the likely values of the true mean. In reality, a confidence interval is more complicated than that. The last few lines summarise the sample mean. This is only useful if we had not bothered to calculate this already. A bit more about degrees of freedom Degrees of freedom (abbreviated d.f. or df) are closely related to the idea of sample size. The greater the degrees of freedom associated with a test, the more likely it is to detect an effect if it’s present. To calculate the degrees of freedom, we start with the sample size and then we reduce this number by one for every quantity (e.g. a mean) we had to calculate to construct the test. Calculating degrees of freedom for a one-sample t-test is easy. The degrees of freedom are just n-1, where n is the number of observations in the sample. We lose one degree of freedom because we have to calculate one sample mean to construct the test. 12.3.3 Summarising the result Having obtained the result we need to write the conclusion. Remember, we are testing a scientific hypothesis, so always go back to the original question to write the conclusion. In this case the appropriate conclusion is: The mean dry weight biomass of purple plants is significantly different from the expectation of 710 grams (t = 3.18, d.f. = 76, p &lt; 0.01). This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which value was used in the comparison. It is sometimes appropriate to give the values of the sample mean in the conclusion: The mean dry weight biomass of purple plants (767 grams) is significantly different from the expectation of 710 grams (t = 3.18, d.f. = 76, p &lt; 0.01). Notice that we include details of the test in the conclusion. However, keep in mind that when writing scientific reports, the end result of any statistical test should be a conclusion like the one above. Simply writing t = 3.18 or p &lt; 0.01 is not an adequate conclusion. There are a number of common questions that arise when presenting t-test results: What do I do if t is negative? Don’t worry. A t-statistic can come out negative or positive, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that determines the p-value, when presenting the results, just ignore the minus sign and always give t as a positive number. How many significant figures for t? The t-statistic is conventionally given to 3 significant figures. This is because, in terms of the p-value generated, there is almost no difference between, say, t = 3.1811 and t = 3.18. Upper or lower case The t statistic should always be written as lower case when writing it in a report (as in the conclusions above). Similarly, d.f. and p are always best as lower case. Some statistics we encounter later are written in upper case but, even with these, d.f. and p should be lower case. How should I present p? There are various conventions in use for presenting p-values. We discussed these in the Hypotheses and p-values chapter. Learn them! It’s not possible to understand scientific papers or prepare reports properly without knowing these conventions. p = 0.00? It’s impossible! p = 1e-16? What’s that? Some computer packages (e.g. Minitab) sometimes give a probability of p = 0.00. This does not mean the probability was actually zero. A probability of zero would mean something was impossible. Since we cannot show something to be impossible by taking samples, we should never say this. When a computer package reports p = 0.00 it just means that the probability was ‘very small’. R uses a different convention for presenting small probabilities. A very small probability is usually given as p-value &lt; 2.2e-16. What does 2.2e-16 mean? This is R-speak for scientific notation, i.e. 2.2e−16 is equivalent to 2.2 × 10−16. In terms of reporting the result, we just write p &lt; 0.001 in this case. "],
["two-sample-t-test.html", "Chapter 13 Two-sample t-test 13.1 When do we use a two-sample t-test? 13.2 How does the two-sample t-test work? 13.3 Carrying out a two-sample t-test in R", " Chapter 13 Two-sample t-test 13.1 When do we use a two-sample t-test? The two-sample t-test is a parametric version of the permutation procedure we studied in the Comparing populations chapter. The test is used to compare the means of a numeric variable sampled from two independent populations. The aim of a two-sample t-test is to evaluate whether or not this mean is different in the two populations. Here are two examples: We’re studying how the dietary habits of Scandinavian eagle-owls vary among seasons. We suspect that the dietary value of a prey item is different in the winter and summer. To evaluate this prediction, we measure the size of Norway rat skulls in the pellets of eagle-owls in summer and winter, and then compare the mean size of rat skulls in each season using a two-sample t-test. We’re interested in the costs of anti-predator behaviour in Daphnia spp. We conducted an experiment where we added predator kairomones—chemicals that signal the presence of a predator—to jars containing individual Daphnia. There is a second control group where no kairomone was added. The change in body size of individuals was measured after one week. We could use a two-sample t-test to compare the mean growth rate in the control and treatment groups. 13.2 How does the two-sample t-test work? Imagine that we have taken a sample of a variable (called ‘X’) from two populations, labelled ‘A’ and ‘B’. Here’s an example of how these data might look if we took a sample of 50 items from each population: Figure 13.1: Example of data used in a two-sample t-test The two distributions overlap quite a lot. However, this particular observation isn’t all that relevant here. We’re not interested in the raw values of ‘X’ in the two samples. It’s the difference between the means that matters. The red lines are the mean of each sample—sample B obviously has a larger mean than sample A. The question is, how do we decide whether this difference is ‘real’, or purely a result of sampling variation? Using a frequentist approach, we tackle this question by first setting up the appropriate null hypothesis. The null hypothesis here is that there is no difference between the population means. We then have to work out what the ‘null distribution’ looks like. Here, this is sampling distribution of the differences between sample means under the null hypothesis. Once we have the null distribution worked out we can calculate a p-value. How is this different from the permutation test? The logic is virtually identical. The one important difference is that we have to make an extra assumption to use the two-sample t-test. We have to assume the variable is normally distributed in each population. If this assumption is valid, then the null distribution will have a known form, which is closely related to the t-distribution. We only need to use a few pieces of information to carry out a two-sample t-test. These are basically the same quantities needed to construct the one-sample t-test, except now there are two samples involved. We need the sample sizes of A and B, the sample variances of A and B, and the estimated difference between the sample means. That’s it. No permutations of labels is needed. How does it actually work? The two-sample t-test is carried out as follows: Step 1. Calculate the two sample means, then calculate the difference between these estimates. This estimate is our ‘best guess’ of the true difference between means. As with the one-sample test, its role in the two-sample t-test is to allow us to construct a test statistic. Step 2. Estimate the standard error of the difference between the sample means under the null hypothesis of no difference. This gives us an idea of how much sampling variation we expect to observe in the estimated difference, if there were actually no difference between the means. There are a number of different options for estimating this standard error. Each one makes a different assumption about the variability of the two populations. Which ever choice we make, the calculation always boils down to a simple formula involving the sample sizes and sample variances. The standard error gets smaller when the sample sizes grow, or when the sample variances shrink. That’s the important point really. Step 3. Once we have estimated the difference between sample means and its standard error, we can calculate the test statistic. This is a type of t-statistic, which we calculate by dividing the difference between sample means (from step 1) by the estimated standard error of the difference (from step 2): \\[\\text{t} = \\frac{\\text{Difference Between Sample Means}}{\\text{Standard Error of the Difference}}\\] This t-statistic is guaranteed to follow a t-distribution if the normality assumption is met. This knowledge leads to the final step… Step 4. Compare the test statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the observed difference. That is, we calculate the probability that we would have observed a difference between means with a magnitude as large as, or larger than, the observed difference, if the null hypothesis were true. That’s the p-value for the test. We could step through the various calculations involved in these steps, but there isn’t much to be gained by doing this. The formula for the standard two-sample t-test and its variants are summarised on the t-test Wikipedi page. There’s no need to learn these—we’re going to let R to handle the heavy lifting again. Let’s review the assumptions of the two-sample t-test first… 13.2.1 Assumptions of the two-sample t-test There are several assumptions that need to be met for a two-sample t-test to be valid. These are basically the same assumptions that matter for the one-sample version. We start with the most important and work down the list in decreasing order of importance: Independence. Remember what we said in our discussion of the one-sample t-test. If the data are not independent, the p-values generated by the test will be too small, and even mild non-independence can be a serious problem. The same is true of the two-sample t-test. Measurement scale. The variable that we are working with should be measured on an interval or ratio scale. It makes little sense to apply a two-sample t-test to a variable that isn’t measured on one of these scales. Normality. The two-sample t-test will produce exact p-values if the variable is normally distributed in both populations. Just as with the one-sample version, the two-sample t-test is fairly robust to mild departures from normality when the sample sizes are small, and this assumption matters even less when the sample sizes are large. How do we evaluate the first two assumptions? As with the one-sample test, these are aspects of experimental design—we can only evaluate them by thinking about the data. The normality assumption may be checked by plotting the distribution of each sample. The simplest way to do this is with histograms or dot plots. Note that we have to examine the distribution of each sample, not the combined distribution of both samples. If both samples looks approximately normal then it’s fine to proceed with the two-sample t-test, and if we have a large sample we don’t need to worry too much about moderate departures from normality. 13.2.2 What about the equal variance assumption? It is sometimes said that when applying a two-sample t-test the variance (i.e. the dispersion) of each sample must be the same, or at least quite similar. This would be true if we’re using original version of Student’s two-sample t-test. However, R doesn’t use this version of the test by default. R uses the “Welch” version of the two-sample t-test. The Welch two-sample t-test does not rely on the equal variance assumption. As long as we stick with this version of the t-test, the equal variance assumption isn’t one we need to worry about. Is there ever any reason not to use the Welch two-sample t-test? The alternative is to use the original Student’s t-test. This version of the test is a little more powerful than Welch’s version, in the sense that it is more likely to detect a difference in means. However, the increase in statistical power is really quite small when the sample sizes of each group are similar, and the original test is only correct when the population variances are identical. Since we can never prove the ‘equal variance’ assumption—we can only ever reject it—it is generally safer to just use the Welch two-sample t-test. One last warning. Student’s two-sample t-test assumes the variances of the populations are identical. It is the population variances, not the sample variances, that matter. There are methods for comparing variances, and people sometimes suggest using these to select ‘the right’ t-test. This is bad advice. For reasons just outlined, there’s little advantage to using Student’s version of the test if the variandes really are the same. What’s more, the process of picking the test based on the results of another statistical test affects the reliability of the resulting p-values. 13.3 Carrying out a two-sample t-test in R You should work through the example in this section. We’ll work with the plant morph example one last time to learn how to carry out a two-sample t-test in R. We’ll use the test to evaluate whether or not the mean dry weight of purple plants is different from that of green plants. Read the data in MORPH_DATA.CSV into an R data frame, giving it the name morph.data: morph.data &lt;- read.csv(file = &quot;MORPH_DATA.CSV&quot;) Now, let’s work through the analysis… 13.3.1 Visualising the data and checking the assumptions We start by calculating a few summary statistics and visualising the sample distributions of the green and purple morph dry weights. We did this in the Comparing populations chapter, but here’s the dplyr code for the descriptive statistics again: morph.data %&gt;% # group the data by plant mnorph group_by(Colour) %&gt;% # calculate the mean, standard deviation and sample size summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## # A tibble: 2 x 4 ## Colour mean sd samp_size ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Green 707.8424 149.8092 173 ## 2 Purple 766.5638 156.0316 77 The sample sizes 173 (green plants) and 77 (purple plants). These are good sized samples, so hopefully the normality assumption isn’t a big deal here. Nonetheless, we still need to check the distributional assumptions: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) + facet_wrap(~Colour, ncol = 1) Figure 13.2: Size distributions of purple and green morph samples There is nothing too ‘non-normal’ about the two samples so it’s reasonable to assume they both came from normally distributed populations. 13.3.2 Carrying out the test The function we need to carry out a two-sample t-test in R is the t.test function, i.e. the same one used for the one-sample test. Remember, morph.data has two columns: Weight contains the dry weight biomass of each plant, and Colour is an index variable that indicates which sample (plant morph) an observation belongs to. Here’s the code to carry out the two-sample t-test t.test(Weight ~ Colour, morph.data) We have suppressed the output for now so that we can focus on how to use the function. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ must be the variable whose mean we want to compare (Weight). The variable on the right must be the indicator variable that says which group each observation belongs to (Colour). The second argument is the name of the data frame that contains the variables listed in the formula. Let’s take a look at the output: t.test(Weight ~ Colour, morph.data) ## ## Welch Two Sample t-test ## ## data: Weight by Colour ## t = -2.7808, df = 140.69, p-value = 0.006165 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -100.46812 -16.97476 ## sample estimates: ## mean in group Green mean in group Purple ## 707.8424 766.5638 The first part of the output reminds us what we did. The first line reminds us what kind of t-test we used. This says: Welch two-sample t-test, so we know that we used the Welch version of the test that accounts for the possibility of unequal variance. The next line reminds us about the data. This says: data: Weight by Colour, which is R-speak for ‘we compared the means of the Weight variable, where the sample membership is defined by the values of the Colour variable’. The third line of text is the most important. This says: t = -2.7808, d.f. = 140.69, p-value = 0.006165. The first part, t = -2.7808, is the test statistic (i.e. the value of the t-statistic). The second part, df = 140.69, summarise the ‘degrees of freedom’ (see the box below). The third part, p-value = 0.006165, is the all-important p-value. This says there is a statistically significant difference in the mean dry weight biomass of the two colour morphs, because p&lt;0.05. Because the p-value is less than 0.01 but greater than 0.001, we would report this as ‘p &lt; 0.01’. The fourth line of text (alternative hypothesis: true difference in means is not equal to 0) simply reminds us of the alternative to the null hypothesis (H1). We can ignore this. The next two lines show us the ‘95% confidence interval’ for the difference between the means. Just as with the one sample t-test we can think of this interval as a rough summary of the likely values of the true difference (again, a confidence interval is more complicated than that in reality). The last few lines summarise the sample means of each group. This could be useful if we had not bothered to calculate these already. A bit more about degrees of freedom In the original version of the two-sample t-test (the one that assumes equal variances) the degrees of freedom of the test are give by (nA-1) + (nB-1), where nA is the number of observations in sample A, and nB the number of observations in sample B. The plant morph data included 77 purple plants and 173 green plants, so if we had used the original version of the test we would have (77-1) + (173-1) = 248 d.f. The Welch version of the t-test reduces the numbers of degrees of freedom by using a formula that takes into account the difference in variance in the two samples. The greater the difference in the two sample sizes the smaller the number of degrees of freedom becomes: When the sample sizes are similar, the adjusted d.f. will be close to that in the original version of the two-sample t-test. When the sample sizes are very different, the adjusted d.f. will be close to the sample size of the smallest sample. Note that the ‘unequal sample size accounting’ results in degrees of freedom that are not whole numbers. It’s not critical that you remember all this. Here’s the important point: whatever flavour of t-test we’re using, a test with high degrees of freedom is more powerful than one with low degrees of freedom, i.e. the higher the degrees of freedom, the more likely we are to detect an effect if it is present. This is why degrees of freedom matter. 13.3.3 Summarising the result Having obtained the result we need to report it. We should go back to the original question to do this. In our example the appropriate summary is: Mean dry weight biomass of purple and green plants differs significantly (Welch’s t = 2.78, d.f. = 140.7, p &lt; 0.01), with purple plants being the larger. This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger. Always indicate which mean is the largest. It is sometimes appropriate to also give the values of the means: The mean dry weight biomass of purple plants (767 grams) is significantly greater than that of green plants (708 grams) (Welch’s t = 2.78, d.f. = 140.7, p &lt; 0.01) When we are writing scientific reports, the end result of any statistical test should be a statement like the one above—simply writing t = 2.78 or p &lt; 0.01 is not an adequate conclusion! "],
["correlation.html", "Chapter 14 Correlation 14.1 Introduction 14.2 Pearson’s product-moment correlation coefficient", " Chapter 14 Correlation 14.1 Introduction The t-tests we encountered in the last two chapters were concerned with how to compare mean(s) of numeric variables. We learned how to: (1) compare one mean to any particular value via the one-sample t-test, (2) compare means among two groups or conditions via the two-sample t-test. One way to think about the two-sample t-tests is that they evaluate whether or not the variable changes among two groups or experimental conditions. Membership of the different groups/conditions can be encoded by a categorical variable. In R, we use a formula involving the numeric (num_var) and categorical (cat_var) variables to set up the test (e.g. num_var ~ cat_var). The formula reflects the fact that we can conceptualise the t-tests as considering a relationship between a numeric and categorical variable2. In this chapter we’ll move on to discuss correlations, which are statistical measures that quantify an association between two numeric variables. An association is any relationship between the variables that makes them dependent in some way: knowing the value of one variable gives you information about the possible values of the second variable. The terms association and correlation are often used interchangeably, but strictly speaking correlation has a narrower definition. A correlation quantifies, via a correlation coefficient, the degree to which an association tends to a certain pattern. There are a variety of methods for quantifying correlation, but these all share common properties: If there is no relationship between the variables then the correlation coefficient will be zero. The closer to 0 the value, the weaker the relationship. A perfect correlation will be either -1 or +1, depending on the direction. This is illustrated below… The value of a correlation coefficient indicates the direction and strength of the association, but says nothing about the steepness of the relationship. A correlation coefficient is just a number, so it can not tell us exactly how one variable depends on the other. A correlation coefficient doesn’t tell us whether an apparent association is likely to be real or not. It is possible to construct a statistical test to evaluate whether a correlation is different from zero. Like any statistical test, this requires certain assumptions about the variables to be met. There are several different measures of correlation between two variables. Here, we will consider probably the most commonly used method, Pearson’s product-moment correlation (\\(r\\)), often called Pearson’s correlation for convenience3: 14.2 Pearson’s product-moment correlation coefficient Pearson’s correlation, being a parametric technique, makes some reasonably strong assumptions: The data are on an interval or ratio scale. The relationship between the variables is linear. Both variables are normally distributed in the population. The requirements are fairly simple and shouldn’t need any further explanation. It is worth making one comment though. Strictly speaking, only the linearity assumption needs to be met for Pearson’s correlation coefficient (\\(r\\)) to be a valid measure of association. As long as the relationship between two variables is linear, \\(r\\) produces a sensible measure. However, if the first two assumptions are not met, it is not possible to construct a valid significance test via the standard ‘parametric’ approach. In this course we will only consider the Pearson’s correlation coefficient in situations where it is appropriate to rely on this approach to calculate p-values. This means the first two assumptions need to be met. We’ll work through an example to learn about Pearson’s correlation. 14.2.1 Pearson’s product-moment correlation coefficient in R Bracken fern (Pteridium aquilinum) is a common plant in many upland areas. One concern is whether there is any association between bracken and heather (Calluna vulgaris) in these areas. To determine whether the two species are associated, an investigator sampled 22 plots at random and estimated the density of bracken and heather in each plot. The data are the mean Calluna standing crop (g m-2) and the number of bracken fronds per m2. The data are in the file BRACKEN.CSV. Read these data into a data frame, calling it bracken: bracken &lt;- read.csv(&quot;BRACKEN.CSV&quot;) glimpse(bracken) ## Observations: 22 ## Variables: 2 ## $ Calluna &lt;int&gt; 980, 760, 613, 489, 498, 416, 589, 510, 459, 680, 471,... ## $ Bracken &lt;dbl&gt; 2.3, 1.4, 4.0, 3.6, 4.3, 4.0, 6.3, 6.5, 8.3, 8.2, 8.1,... There are only two variables in this data set: Calluna and Bracken. The first thing we should do is summarise the distribution of each variable: ggplot(bracken, aes(x = Calluna)) + geom_dotplot(binwidth = 100) ggplot(bracken, aes(x = Bracken)) + geom_dotplot(binwidth = 2) It looks like we’re dealing with numeric variables (ratio scale), each of which could be normally distributed. What we really want to assess is the association. A scatter plot is obviously the best way to visualise this: ggplot(bracken, aes(x = Calluna, y = Bracken)) + geom_point() It seems clear that the two plants are negatively associated, but we should confirm this with a statistical test. We’ll base this on Pearson’s correlation. Are the assumptions met? The scatter plot indicates that the relationship between the variables is linear, so Pearson’s correlation is a valid measure of association. Is it appropriate to carry out a significance test though? The data are of the right type—both variables are measured on a ratio scale—and the two dot plots above suggest the normality assumption is reasonable. Let’s proceed with the analysis… Carrying out a correlation analysis in R is straightforward. We use the cor.test function to do this: cor.test(~ Calluna + Bracken, method = &quot;pearson&quot;, data = bracken) ## ## Pearson&#39;s product-moment correlation ## ## data: Calluna and Bracken ## t = -5.2706, df = 20, p-value = 3.701e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8960509 -0.5024069 ## sample estimates: ## cor ## -0.7625028 We use method = &quot;pearson&quot; to control which kind of correlation coefficient was calculated. There are three options, and although the default method is Pearson’s correlation, it is a good idea to be explicit. We use R’s formula system to determine which pair of variables are analysed. However, instead of placing a variable on the left hand side and a variable on the right hand side (e.g. Calluna ~ Bracken), both two variables appear to the right of the ~ separated by a + symbol. This convention makes good sense if you think about where we use correlation: a correlation analysis examines association, but it does not imply the existence of predictor and response variables. To emphasise the fact that neither variable has a special status, the cor.test function expects both variables to appear to the right of the ~, with nothing on the left. The output from the cor.test is very similar to that produced by the t.test function. We won’t step through most of this output, as its meaning should be clear. The t = -5.2706, df = 20, p-value = 3.701e-05 line is the one we care about. Here are the key points: The first part says that the test statistic associated with a Pearson’s correlation coefficient is a type of t-statistic. We’re not going to spend time worrying about where this came from, other than to note that it is interpreted in exactly the same way as any other t-statistic. Next we see the degrees of freedom for the test. Can you see where this comes from? It is \\(n-2\\), where \\(n\\) is the sample size. Together, the degrees of freedom and the t-statistic determine the p-value… The t-statistic and associated p-value are generated under the null hypothesis of zero correlation (\\(r = 0\\)). Since p &lt; 0.05, we conclude that there is a statistically significant correlation between bracken and heather. What is the actual correlation between bracken and heather densities? That’s given at the bottom of the test output: \\(-0.76\\). As expected from the scatter plot, there is quite a strong negative association between bracken and heather densities. 14.2.2 Reporting the result When using Pearson’s method we report the value of the correlation coefficient, the sample size, and the p-value4. Here’s how to report the results of this analysis: There is a negative correlation between bracken and heather among the study plots (r=-0.76, n=22, p &lt; 0.001). Notice that we did not say that bracken is having a negative effect on the heather, or vice versa. It is perfectly possible to evaluate differences among means in more than two categories, but we don’t use t-tests to do this. Instead, we us a more sophisticated tool called Analysis of Variance (ANOVA). We’ll learn about ANOVA in later chapters.↩ People sometimes just refer to ‘the correlation coefficient’ without stating which measure they are using. When this happens, they probably used the most common method: Pearson’s product-moment correlation.↩ People occasionally report the value of the correlation coefficient, the t-statistic, the degrees of freedom, and the p-value. We won’t do this.↩ "],
["relationships-and-regression.html", "Chapter 15 Relationships and regression 15.1 Introduction 15.2 Correlation or regression? 15.3 What does linear regression do? 15.4 How does simple linear regression work? 15.5 What do you get out of a regression?", " Chapter 15 Relationships and regression 15.1 Introduction Much of biology is concerned with relationships between numeric variables. For example… We sample fish and measure their length and weight because we want to understand how weight scales with respect to length. We survey grassland plots and measure soil pH and species diversity because we want to understand how species diversity depends on soil pH. We manipulate temperature and measure fitness in insects because we want to characterise their thermal tolerance. In the previous chapter we learnt about one technique for analysing associations between numeric variables (correlation). A correlation coefficient only quantifies the strength and direction of an association between two variables. It will be close to zero if there is no association between the variables; a strong association is implied if the coefficient is near -1 or +1. A correlation coefficient tells us nothing about the form of a relationship. Nor does it allow us to make predictions about the value of one variable from the value of a second variable. Regression, which is the focus of this chapter, does allow this because it involves fitting a line through the data. In the relationships mentioned above the goal is to understand how one numeric variable depends on the values of another. Graphically, we evaluate such dependencies using a scatter plot. We may be interested in knowing: Are the variables related or not? There’s not much point studying a relationship that isn’t there: Is the relationship positive or negative? Sometimes we can answer a scientific question just by knowing the direction of a relationship: Is the relationship a straight line or a curve? It is important to know the form of a relationship if we want to make predictions: Although sometimes it may be obvious that there is a relationship between two variables from a plot of one against the other, at other times it may not. Take a look at the following: We might not be very confident in judging which, if either, of these plots provides evidence of a positive relationship between the two variables. Maybe the pattern that we perceive can just be explained by sampling variation, or maybe it can’t. Clearly it would be useful to have a measure of how likely it is that the relationship could have arisen as a result of sampling variation. In addition to judging the statistical significance of a relationship, we may also be interested in describing the relationship mathematically – i.e. finding the equation of the best fitting line through the data. A linear regression analysis allows us to do all this. 15.2 Correlation or regression? Whilst regression and correlation are both concerned with associations between numeric variables they are different techniques and each is appropriate under different circumstances. This is a frequent source of confusion. Which technique is required for a particular analysis depends on the way the data were collected and the purpose of the analysis. There are two broad questions to consider: Where do the data come from? Think about how the data have been collected. If the data are from an experimental study where one of the variables has been manipulated, then choosing the best analysis is easy. We should use a regression analysis, in which the predictor variable is the experimentally manipulated variable and the response variable is the measured outcome. The fitted line from a regression analysis describes how the outcome variable depends on the manipulated variable—it describes the causal relationship between them. It is generally inappropriate to use correlation to analyse data from an experimental setting. A correlation analysis examines association but does not imply the dependence of one variable on another. Since there is no distinction of response or predictor variables, it doesn’t matter which way round we do a correlation. (The phrase ‘which way round’ doesn’t even make sense in the context of a correlation.) If the data are from an observational study, either method may be appropriate. Time to ask another question… What is the goal of the analysis? Think about what question is being addressed. A correlation coefficient only quantifies the strength and direction of an association between two variables, it tells us nothing about the form of a relationship. Nor does it allow us to make predictions about the value of one variable from the value of a second variable. A regression does allow this because it involves fitting a line through the data—i.e. there’s a model for the relationship. This means that if the goal of an analysis is to understand the form of a relationship between two variables, or to use a fitted model to make predictions, we have to use regression. If we just want to know whether two variables are associated or not, the direction of the association, and whether the association is strong or weak, then a correlation analysis is sufficient. It is better to use a correlation analysis when the extra information produced by a regression is not needed, because the former will be simpler and potentially more robust. 15.3 What does linear regression do? Simple linear regression allows us to predict how one variable (the response variable) responds to another (the predictor variable), using a straight-line relationship. What does the word ‘simple’ mean here? A simple linear regression is a regression model which only accounts for one predictor variable. If more than one predictor variable is considered, the correct term to describe the resulting model is ‘multiple regression’. Multiple regression is a very useful tool but we’re only going to study simple regression in this book. What does the word ‘linear’ mean here? In statistics, the word linear is used in two slightly different, but closely related ways. When discussing simple linear regression the term linear is often understood to mean that the relationship follows a straight line. That’s all. The more technical definition concerns the relationship between the parameters of a statistical model. We don’t need to worry about that one here. Writing ‘simple linear regression’ all the time becomes tedious, so we’ll often write ‘linear regression’ or ‘regression’. Just keep in mind that we’re always referring to simple linear regression in this book. These regression models account for a straight line relationship between two numeric variables, i.e. they describe how the response variable changes in response to the values of the predictor variable. It is conventional to label the response variable as ‘\\(y\\)’ and the predictor variable as ‘\\(x\\)’. When we present such data graphically, the response variable always goes on the \\(y\\)-axis and the predictor variable on the \\(x\\)-axis. Try not to forget this convention! ‘response vs. predictor’ or ‘dependent vs. independent’? Another way to describe linear regression is that it finds the straight-line relationship which best describes the dependence of one variable (the dependent variable) on the other (the independent variable). The dependent vs. independent and response vs. predictor conventions for variables in a regression are essentially equivalent. They only differ in the nomenclature they use to describe the variables involved. To avoid confusion, we will stick with response vs. predictor naming convention in this course. How do we decide how to select which is to be used as the response variable and which as the predictor variable? The decision is fairly straightforward in an experimental setting: the manipulated variable is the predictor variable, and the measured outcome is the response variable. Consider the thermal tolerance example from earlier. Temperature was manipulated in this experiment, so it must be designated the predictor variable. Moreover, a priori (before conducting the experiment), we can reasonably suppose that changes in temperature may cause changes in enzyme activity, but the reverse seems pretty unlikely. Things may not be so clear cut when we are working with data from an observational study as it may not be obvious that one variable depends upon the other (in a causal sense). In regression it matters which way round we designate the response and predictor variables. If you have two variables A and B, the relationship you find from a regression will not be the same for A against B as for B against A. 15.4 How does simple linear regression work? 15.4.1 Finding the best fit line If we draw a straight line through a set of points on a graph then, unless they form a perfect straight line, some points will lie close to the line and others further away. The vertical distances between the line and each point (i.e. measured parallel to the \\(y\\)-axis) have a special name. They are called the residuals. Here’s a visual example: Figure 15.1: Example of data (blue points) used in a simple regression. A fitted line and the associated residuals (vertical lines) are also shown In this plot the blue points are the data and the vertical lines represent the residuals. The residuals represent the variation that is ‘left over’ after the line has been fitted through the data. They give an indication of how well the line fits the data. If all the points lay close to the line the variability of the residuals would be low relative to the variation in the response variable, \\(y\\). When the observations are more scattered around the line the variability of the residuals would be large relative to the variation in the response variable, \\(y\\). Regression works by finding the line which minimises the size of the residuals in some sense. We’ll explain exactly how in a moment. The following illustration indicates the principle of this process: The data are identical in all four graphs, but in the top left hand graph a horizontal line (i.e. no effect of \\(x\\) on \\(y\\)) has been fitted, while on the remaining three graphs sloping lines of different magnitude have been fitted. To keep the example simple, we assume we know the true intercept of the line, which is at \\(y=0\\), so all four lines pass through \\(x=0\\), \\(y=0\\) (the ‘origin’). Which line is best? One of the four lines is the ‘line of best’ fit from a regression analysis. Spend a few moments looking at the four figures. Which line seems to fit the data best? Why do you think this line is ‘best’? Let’s visualise the data, the candidate lines and the residuals: We said that regression works by finding the intercept and slope that minimises the vertical distances between the line and each observation in some way5. In fact, it minimises something called the ‘sum of squares’ of these distances: we calculate a sum of squares for a particular set of observations and a fitted line by squaring the residual distances and adding all of these up. This quantity is called the residual sum of squares. The line with the lowest residual sum of squares is the best line because it ‘explains’ the most variation in the response variable. You should be able to see that, for the horizontal line (‘A’), the residual sum of squares is larger than any of the other three plots with the sloping lines. This suggests that the sloping lines fit the data better. Which one is best among the three we’ve plotted? To get at this we need to calculate the residual sum of squares for each line. These are… ## Line Residual Sum of Squares ## 1 A 17.55067 ## 2 B 11.97966 ## 3 C 10.12265 ## 4 D 12.79674 So it looks like the line in panel C is the best fitting line among the candidates. In fact, it is the best fit line among all possible candidates. Did you manage to guess this by looking at the lines and the raw data? If not, think about why you got the answer wrong. Did you consider the vertical distances or the perpendicular distances? It is very important that you understand what a residual from a regression represents. Residuals pop up all the time when evaluating statistical models (not just regression). If you’re confused about what they represent be sure to ask a TA to explain them to you. 15.5 What do you get out of a regression? A regression analysis involves two activities: Interpretation. When we ‘fit’ a regression model to data we are estimating the coefficients of a best-fit straight line through the data. This is the equation that best describes how the \\(y\\) (response) variable responds to the \\(x\\) (predictor) variable. To put it in slightly more technical terms, it describes the \\(y\\) variable as a function of the \\(x\\) variable. This model may be used to understand how the variables are related or make predictions. Inference. It is not enough to just estimate the regression equation. Before we can use it we need to determine whether there is a statistically significant relationship between the \\(x\\) and \\(y\\) variables. That is, the analysis will tell us whether an apparent association is likely to be real, or just a chance outcome resulting from sampling variation. Let’s consider each of these two activities… 15.5.1 Interpreting a regression What is the form of the relationship? The equation for a straight line relationship is \\(y = a + b \\times x\\), where \\(y\\) is the response variable, \\(x\\) is the predictor variable, \\(a\\) is the intercept (i.e. the value at which the line crosses the \\(y\\) axis), and \\(b\\) is the slope of the line. The \\(a\\) and the \\(b\\) are referred to as the coefficients (or parameters) of the line. The slope of the line is often the coefficient we care about most. It tells us the amount by which \\(y\\) changes for a change of one unit in \\(x\\). If the value of \\(b\\) is positive (i.e. a plus sign in the above equation) this means the line slopes upwards to the right. A negative slope (\\(y = a - bx\\)) means the line slopes downwards to the right. The diagram below shows the derivation of an equation for a straight line. Having the equation for a relationship allows us to predict the value of the \\(y\\) variable for any value of \\(x\\). For example, in the thermal tolerance example, we want an equation that will allow us to work out how fitness changes with temperature. Such predictions can be made by hand (see below) or using R (details later). In the above diagram, the regression equation is: \\(y = 1 + 0.66 x\\). So to find the value of \\(y\\) at \\(x = 2\\) we use: \\(y = 1 + (0.667 \\times 2) = 2.32\\). Obviously, by finding \\(y\\) values for 2 (or preferably 3) different \\(x\\) values from the equation, the actual line can easily be plotted on a graph manually if required—plot the values and join the dots! It’s much easier to use R to do this kind of thing though. Regression involves a statistical model A simple linear regression is underpinned by a statistical model. If you skim back through the Parametric statistics chapter you will see that the equation y = a + b × x represents the ‘systematic component’ of the regression model. This bit describes the component of variation in y that is explained by the model for the dependence of y on x. The residuals correspond to the ‘random component’ of the model. These represent the component of variation in the y variable that our regression model fails to describe. 15.5.2 Evaluating hypotheses (‘inference’) There is more than one kind of significance test that can be carried out with a simple linear regression. We’re going to focus on the most useful test: the F test of whether the slope coefficient is significantly different from 0. How do we do this? We play exactly the same kind of ‘gambit’ we used to develop the earlier tests: We start with a null hypothesis of ‘no effect’. This corresponds to the hypothesis that the slope of the regression is zero. We then work out what the distribution of some kind of test statistic should look like under the null hypothesis. The test statistic in this case is called the F-ratio. We then calculate a p-value by asking how likely it is that we would see the observed test statistic, or a more extreme value, if the null hypothesis were really true. It’s really not critical that you understand the mechanics of an F-test. However, there are several terms involved that are good to know about because having some sense of what they mean may help to demystify the output produced by R. Let’s step through the calculations involved in the F test. We’ll use the example data shown in the four-panel plot from earlier to do this… Total variation First we need to calculate something called the total sum of squares. The figure below shows the raw data (blue points) and the grand mean (i.e. the sample mean). The vertical lines show the distance between each observation and the grand mean. These vertical lines are just the residuals from a model where the slope of the line is set to zero. What we need to do is quantify the variability of these residuals. We can’t just add them up, because by definition, they have to sum to zero, i.e. they are calculated relative to the grand mean. Instead we calculate the total sum of squares by taking each residual in turn, squaring it, and then adding up all the squared values. We call this the total sum of squares because it is a measure of the total variability in the response variable, \\(y\\). This number is 17.55 for the data in the figure above. Residual variation Next we need to calculate the residual sum of squares. We have already seen how this calculation works because it is used in the calculation of the best fit line—the best fit line is the one that minimises the residual sum of squares. Let’s plot this line along with the associated residuals of this line again: The vertical lines show the distance between each observation and the best fit line. We need to quantify the variability of these residuals. Again, we can’t just add up the deviations because they have to sum to zero as a result of how the best fit line is found. Instead we calculate the residual sum of squares by taking each residual in turn, squaring it, and then adding up all the squared values. This number is 10.12 for the figure above. We call this the residual, or error, sum of squares because it is a measure of the variation in \\(y\\) that is ‘left over’ after accounting for the influence of the predictor variable \\(x\\). Explained variation Once the total sum of squares and the residual sum of squares are known, we can calculate the quantity we really want: the explained sum of squares. This is a measure of the variation in \\(y\\) that is explained by the influence of the predictor variable \\(x\\). We calculate this by subtracting the residual sum of squares from the total sum of squares. This makes intuitive sense: if we subtract the variation in \\(y\\) we can’t explain (residual) from all the variation in \\(y\\) (total), we end up with the amount ‘explained’ by the regression. This number is 7.43 for the example. Degrees of freedom, mean squares and F tests The problem with sums of squares is that they are a function of sample size. The more data we have, the larger our sum of squares will get. The solution to this problem is to convert them into a measure of variability that doesn’t scale with sample size. We need to calculate degrees of freedom (written as df, or d.f.) to do this. We came across the concept of degrees of freedom when we studied the t-test. The idea is closely related to sample size. It is difficult to give a precise definition, but roughly speaking the degrees of freedom of a statistic is a measure of how much ‘information’ it contains. Each of the measures of variability we just calculated for the simple linear regression has a degrees of freedom associated with it. We need the explained and error degrees of freedom: Explained d.f. = 1 Error d.f. = (Number of observations - 2) Don’t worry if those seem a little cryptic. We don’t need to carry out degrees of freedom calculations by hand because R will do them for us. We’ll think about degrees of freedom a bit more when we start to learn about ANOVA models. The reason degrees of freedom matter is because we can use them to standardise the sum of squares to account for sample size. The calculations are very simple. We take each sum of squares and divide it by its associated degrees of freedom. The resulting quantity is called a mean square (it’s the mean of squared deviations): \\[ \\text{Mean Square} = \\frac{\\text{Sum of Squares}}{\\text{Degrees of Freedom}} \\] A mean square is actually an estimate of variance. Remember the variance? It is one of the standard measures of a distribution’s dispersion, or spread. Now for the important bit. The two mean squares can be compared by calculating the ratio between them, which is designated by the letter F: \\[F = \\mbox{Variance Ratio} = \\frac{\\mbox{Explained Mean Square}}{\\mbox{Residual Mean Square}}\\] This is called the F ratio, or sometimes, the variance ratio. If the explained variation is large compared to the residual variation then the F ratio will be large. Conversely, if the explained variation is relatively small then F will be small. We can see where this is heading… The F ratio is a type of test statistic—if the value of F is sufficiently large then we judge it to be statistically significant. In order for this judgement to be valid we have to make one key assumption about the population from which the data has been sampled: we assume the residuals are drawn from a normal distribution. If this assumption is correct, it can be shown that the distribution of the F ratio under the null hypothesis (the ‘null distribution’) has a particular form: it follows a theoretical distribution called an F-distribution. And yes, that’s why the variance ratio is called ‘F’. All this means we can assess statistical significance of the slope coefficient by comparing the F ratio calculated from a sample to this theoretical distribution. This procedure is called an F test. The F ratio is 7.34 in our example. This is quite high which indicates that the slope is likely to be significantly different from 0. However, in order to actually calculate the p-value we also need to consider the degrees of freedom of the test, and because the test involves an F ratio, there are two different degrees of freedom to consider: the explained and residual df’s. Remember that! We could go one to actually calculate the p-value, but it is much better to let R do this for us when we work directly with a regression model. We’ll leave significance tests alone for now… Notice that it is the vertical distance that matters, not the perpendicular distance from the line.↩ "],
["simple-regression-in-r.html", "Chapter 16 Simple regression in R 16.1 Carrying out a simple linear regression in R 16.2 First steps 16.3 Model fitting and significance tests 16.4 Presenting results 16.5 What about causation?", " Chapter 16 Simple regression in R Our goal in this chapter is to learn how to work with regression models in R. We’ll do this by working through an example. We’ll start with the problem and the data, and then work through model fitting, significance testing, and finally, presenting the results. 16.1 Carrying out a simple linear regression in R A plant physiologist studying the process of germination in the broad bean (Vicia faba) is interested in the relationship between the activity of the enzyme amylase, and the temperature at which the germinating beans are kept. As part of this work she carries out an experiment to find the relationship between glucose release (from the breakdown of starch by amylase) and temperature (over the range 2 - 20C). The data obtained from such an experiment are given below. Temperature (\\(C\\)) 2 4 6 8 10 12 14 16 18 20 Glucose (\\(\\mu g\\) \\(mg^{-1}\\) dry weight) 1.0 0.5 2.5 1.5 3.2 4.3 2.5 3.5 2.8 5.6 What we want to do is work out whether there a statistically significant relationship between temperature and glucose release (and hence, presumably, amylase activity). That’s obviously a job for linear regression… Walk through example You should work through the example in the next few sections. 16.2 First steps The data are in a CSV file called GLUCOSE.CSV. Downloaded the data and read it into an R data frame, giving it the name vicia_germ: vicia_germ &lt;- read.csv(file = &quot;GLUCOSE.CSV&quot;) Make sure you use View, glimpse, etc to examine the data before you proceed. Run through all the usual questions… How many variables (columns) are in the data? How many observations (rows) are there? What kind of variables are we working with? This is a fairly simple data set. It contains two numeric variables. The first column (Temperature) contains the information about the experimental temperature treatments, and the second column (Glucose) contain the glucose measurements. Notice that we refer to the different temperatures as ‘experimental treatments’. This is because these data are from an experiment where temperature was controlled by the investigator. We’ll discuss this terminology in more detail in the Principles of Experimental Design section. 16.2.1 Visualising the data We should visualise the data next so that we understand it more. We just need to produce a simple scatter plot with ggplot2: Remember, Glucose is the response variable and Temperature is the predictor variable, so they belong on the \\(y\\) and \\(x\\) axes, respectively. Variables and axes Be careful when you produce a scatter plot to summarise data in a regression analysis. You need to make sure the two variables are plotted the right way around with respect to the x and y axes: place the response variable on the y axis and the predictor on the x axis. Nothing says, “I don’t know what I’m doing,” quite like mixing up the axes. As linear regression involves fitting a straight line through our data it only makes sense to fit this model if the relationship between the two variables is linear. Plotting our data lets us see whether or not there appears to be a linear relationship. The scatter plot we produced above suggests that in this case the relationship between \\(x\\) and \\(y\\) is linear. Assumptions A linear relationship between the predictor and response variables is not the only assumption that we have to make when we fit a linear regression. We’ll come back to the other assumptions and how to check whether they have been met in the [Assumptions] and Diagnostics chapters respectively. For now you should just be aware that there are assumptions that you must check when working with your own data. 16.3 Model fitting and significance tests Carrying out a regression analysis in R is a two step process. The first step involves a process known as fitting the model (or just model fitting). In effect, this is the step where R calculates the best fit line, along with a large amount of additional information needed to generate the results in step two. We call this step model fitting because, well, we end up fitting the straight line model to the data. How do we fit a linear regression model in R? We will do it using the lm function. The letters ‘lm’ in this function name stand for ‘linear model’. We won’t say much more at this point other than point out that a linear regression is a special case of a general linear model. R doesn’t have a special regression function. Here is how we fit a linear regression in R using the enzyme data: vicia_model &lt;- lm(Glucose ~ Temperature, data = vicia_germ) This should look quite familiar. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ should be the response variable. The variable name on the right should be the predictor variable. These are Glucose and Temperature, respectively. Make sure you get these the right way round when carrying out regression. The second argument is the name of the data frame that contains the two variables listed in the formula (vicia_germ). How does R knows we want to carry out a regression? How does R know we want to use regression? After all, we didn’t specify this anywhere. The answer is that R looks at what type of variable Temperature is. It is numeric, and so R automatically carries out a regression. If it had been a factor or a character vector (representing a categorical variable) R would have carried out a different kind of analysis, called a one-way Analysis of Variance (ANOVA). Most of the models that we examine in this course are very similar, and can be fitted using the lm function. The only thing that really distinguishes them is the type of variables that appear to the right of the ~ in a formula: if they are categorical variables we end up carrying out ANOVA, while numeric variables lead to a regression. The key message is that you have to keep a close eye on the type of variables you are modelling to understand what kind of model R will fit. Notice that we did not print the results to the console. Instead, we assigned the result a name (vicia_model). This now refers to a fitted model object. What happens if we print a regression model object to the console? print(vicia_model) ## ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) ## ## Coefficients: ## (Intercept) Temperature ## 0.5200 0.2018 This prints a quick summary of the model we fitted and some information about the ‘coefficients’ of the model. The coefficients are the intercept and slope of the fitted line: the intercept is always labelled (Intercept) and the slope is labelled with the name of the predictor variable (Temperature in this case). We’ll come back to these coefficients once we have looked at how to compute p-values. The second step of a regression analysis involves using the fitted model to assess statistical significance. We usually want to determine whether the slope is significantly different from zero. That is, we want to know if the relationship between the \\(x\\) and \\(y\\) variables is likely to be real or just the result of sampling variation. Carrying out the required F test is actually very easy. The test relies on a function called anova. To use this function, all we have to do is pass it one argument: the name of the fitted regression model object… anova(vicia_model) ## Analysis of Variance Table ## ## Response: Glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temperature 1 13.4411 13.4411 14.032 0.005657 ** ## Residuals 8 7.6629 0.9579 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s step through the output to see what it means. The first line informs us that we are looking at an Analysis of Variance Table—a set of statistical results derived from a general tool called Analysis of Variance. The second line just reminds us what response variable we analysed (Glucose). Those parts are simple to describe at least, though the Analysis of Variance reference may seem a little cryptic. Essentially, every time we carry out an F-test we are performing some kind of Analysis of Variance because the test boils down to a ratio of variances (or more accurately, mean squares). The important part of the output is the table at the end: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temperature 1 13.4411 13.4411 14.032 0.005657 ** ## Residuals 8 7.6629 0.9579 This summarises the different parts of the F-test calculations: Df – degrees of freedom, Sum Sq – the sum of squares, Mean Sq – the mean square, F value – the F-statistic, Pr(&gt;F) – the p-value. If you followed along in the last chapter these should be at least somewhat familiar. The F-statistic (variance ratio) is the key term. When working with a regression model, this quantifies how much variability in the data is explained when we include the best fit slope term in the model. Larger values indicate a stronger relationship between \\(x\\) and \\(y\\). The p-value gives the probability that the relationship could have arisen through sampling variation, if in fact there were no real association. As always, a p-value of less than 0.05 is taken as evidence that the relationship is real, i.e. the result is statistically significant. We should also note down the two degrees of freedom given in the table as these will be needed when we report the results. 16.3.1 Extracting a little more information There is a second function, called summary, that can be used to extract a little more information from the fitted regression model: summary(vicia_model) ## ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.35273 -0.77909 -0.08636 0.74227 1.35818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52000 0.66858 0.778 0.45909 ## Temperature 0.20182 0.05388 3.746 0.00566 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9787 on 8 degrees of freedom ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.5915 ## F-statistic: 14.03 on 1 and 8 DF, p-value: 0.005657 This is easiest to understand if we step through the constituent parts of the output. The first couple of lines just remind us about the model we fitted ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) The next couple of lines aren’t really all that useful—they summarise some properties of the residuals–so we’ll ignore these. The next few lines comprise a table that summarises some useful information about the coefficients of the model (the intercept and slope): ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52000 0.66858 0.778 0.45909 ## Temperature 0.20182 0.05388 3.746 0.00566 ** The Estimate column shows us the estimated the intercept and slope of the regression. We saw these earlier when we printed the fitted model object to the console. Staying with this table, the next three columns (Std. Error, t value and Pr(&gt;|t|)) show us the standard error associated with each coefficient, the corresponding t-statistics, and the p-values. Remember standard errors? These are a measure of the variability of the sampling distribution associated with something we estimate from a sample. We discussed these in the context of sample means. One can calculate a standard error for many different kinds of quantities, including the intercept and slope of a regression model. And just as with a mean, we can use the standard errors to evaluate the significance of the coefficients via t-statistics. In this case, the p-values associated with these t-statistics indicate that the intercept is not significantly different from zero (p&gt;0.05), but that the slope is significantly different from zero (p&lt;0.01). Notice that the p-value associated with the slope coefficient is the same as the one we found when we used the anova function. This is not a coincidence—anova and summary test the same thing when working with simple linear regression models. This is not generally true for other kinds of model involving the lm function. The only other part of the output from summary that is of interest now is the line containing the Multiple R-squared value: ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.5915 This shows the \\(R\\)-squared (\\(R^{2}\\)) of our model. It tells you what proportion (sometimes expressed as a percentage) of the variation in the data is explained, or accounted for, by the fitted line. If \\(R^{2}=1\\) the line passes through all the points on the graph (all the variation is accounted for) and if \\(R^{2}\\approx 0\\%\\) the line explains little or none of the variation in the data. The \\(R^{2}\\) value here is 0.64. This is very respectable, but still indicates that there are other sources of variation (differences between beans, inaccuracies in the assay technique, etc.) which remain unexplained by the line6. 16.4 Presenting results From the preceding analysis we can conclude… There is a significant positive relationship between the incubation temperature (°C) and glucose released (\\(\\mu g mg^{-1}\\) dry weight) in germinating bean seeds (\\(y=0.52+0.20x\\), F=14, d.f.=1,8, p&lt;0.01). Don’t forget to quote both degrees of freedom in the result. These are obtained from the ANOVA table produced by anova and should be given as the slope degrees of freedom first (which is always 1), followed by the error degrees of freedom. If the results are being presented only in the text it is usually appropriate to specify the regression equation as well as the significance of the relationship as this allows the reader to see in which direction and how steep the relationship is, and to use the equation in further calculations. It may also be useful to give the units of measurement—though these should already be stated in the Methods. Often, however, we will want to present the results as a figure, showing the original data and the fitted regression line. In this case, most of the statistical detail can go in the figure legend instead. Let’s see how to present the results as a figure… 16.4.1 Plotting the fitted line and the data We already know how to make a scatter plot. The only new trick we need to learn is how to add the fitted line. Remember the output from the summary table—this gave us the intercept and slope of the best fit line. We could extract these (there is a function called coef that does this), and using our knowledge of the equation of a straight line, use them to then calculate a series of points on the fitted line. However, there is an easier way to do this using the predict function. Don’t worry too much if this next segment on predictions is confusing. It looks more complicated than it is, but you may have to come back to it a few times before it all sinks in. At first reading, try to focus on the logic of the calculations without worrying too much about the details. You won’t be assessed on your ability to plot a fitted line and the data together but you might need to be able to do this to prepare a report. In order to use predict we have to let R know the values of the predictor variable for which we want predictions. In the bean example the temperature was varied from 2-20 °C, so it makes sense to predict glucose concentrations over this range. Therefore the first step in making predictions is to generate a sequence of values from 2 to 20, placing these inside a data frame: pred.data &lt;- data.frame(Temperature = seq(2, 20, length.out = 25)) We learned about the seq function last year. Here, we used it to make a sequence of 25 evenly spaced numbers from 2 to 20. If you can’t remember what it does, ask a demonstrator to explain it to you (and use View to look at pred.data). Notice that we gave the sequence the exact same name as the predictor variable in the regression (Temperature). This is important: the name of the numeric sequence we plan to make predictions from has to match the name of the predictor variable in the fitted model object. Once we have set up a data frame to predict from (pred.data) we are ready to use the predict function: predict(vicia_model, pred.data) ## 1 2 3 4 5 6 7 ## 0.9236364 1.0750000 1.2263637 1.3777273 1.5290909 1.6804546 1.8318182 ## 8 9 10 11 12 13 14 ## 1.9831818 2.1345455 2.2859091 2.4372727 2.5886364 2.7400000 2.8913636 ## 15 16 17 18 19 20 21 ## 3.0427273 3.1940909 3.3454545 3.4968182 3.6481818 3.7995454 3.9509091 ## 22 23 24 25 ## 4.1022727 4.2536363 4.4050000 4.5563636 This take two arguments: the first is the name of the model object (vicia_model); the second is the data frame (pred.data) containing the values of the predictorc variable at which we want to make predictions. The predict function generated the predicted values in a numeric vector and printed these to the console. To be useful, we need to capture these somehow, and because we want to use ggplot2, these need to be kept inside a data frame. We can use mutate to do this: pred.data &lt;- mutate(pred.data, Glucose = predict(vicia_model, pred.data)) Look at the first 10 rows of the resulting data frame: head(pred.data, 10) ## Temperature Glucose ## 1 2.00 0.9236364 ## 2 2.75 1.0750000 ## 3 3.50 1.2263637 ## 4 4.25 1.3777273 ## 5 5.00 1.5290909 ## 6 5.75 1.6804546 ## 7 6.50 1.8318182 ## 8 7.25 1.9831818 ## 9 8.00 2.1345455 ## 10 8.75 2.2859091 The pred.data is set out much like the data frame containing the experimental data. It has two columns, called Glucose and Temperature, but instead of data, it contains predictions from the model. Plotting these predictions along with the data is now easy: ggplot(pred.data, aes(x = Temperature, y = Glucose)) + geom_line() + geom_point(data = vicia_germ) + xlab(&quot;Temperature (°C)&quot;) + ylab(&quot;Glucose concentration&quot;) + theme_bw(base_size = 22) Notice that we have to make ggplot2 use the vicia_germ data (i.e. the raw data) when adding the points. We also threw in a little theming to make the plot look nicer. Let’s summarise what we did: 1) using seq and data.frame, we made a data frame with one column containing the values of the predictor variable we want predictions at; 2) we then used the predict function to generate these predictions, adding them to the prediction data with mutate; 3) finally, we used ggplot2 to plot the predicted values of the response variable against the predictor variable, remembering to include the data. 16.5 What about causation? No discussion of regression would be complete without a little homily on the fact that just because you observe a (significant) relationship between two variables this does not necessarily mean that the two variables are causally linked. If we find a negative relationship between the density of oligochaete worms (the response variable) and the density of trout (the predictor variable) in a sample of different streams, this need not indicate that the trout reduce the numbers of oligochaetes by predation – in fact oligochaete numbers are often very high in slow-flowing, silty streams where they live in the sediments, trout prefer faster flowing, well oxygenated, stony streams – so a negative correlation could occur simply for that reason. There are many situations in biology where a relationship between two variables can occur not because there is a causal link between them but because each is related to a third variable (e.g. habitat). This difficulty must always be borne in mind when interpreting relationships between variables in data collected from non-experimental situations. However, it is often assumed that because of this problem regression analysis can never be used to infer a causal link. This is incorrect. What is important is how the data are generated, not the statistical model used to analyse them. If a set of ten plants were randomly assigned to be grown under ten different light intensities, with all other conditions held constant, then it would be entirely proper to analyse the resulting data (for, let us say, plant height) by a regression of plant height (\\(y\\)) against light level (\\(x\\)) and, if a significant positive straight-line relationship was found, to conclude that increased light level caused increased plant height. Of course this conclusion still depends on the fact that another factor (e.g. temperature) isn’t varying along with light and causing the effect. But the fact that you are experimentally producing an effect, in plants randomly allocated to each light level (i.e. plants in which it is highly improbable that the heights happened to be positively related to light levels at the start) which gives you the confidence to draw a conclusion about causality. Light might not be the direct causal agent, but it must be indirectly related to plant growth because it was experimentally manipulated. The Adjusted R-squared: value can be ignored in this analysis—it is used when doing a form of regression called multiple regression, in which there is more than one \\(x\\) variable.↩ "],
["introduction-to-one-way-anova.html", "Chapter 17 Introduction to one-way ANOVA 17.1 Introduction 17.2 Why do we need ANOVA models? 17.3 How does ANOVA work? 17.4 Different kinds of ANOVA model 17.5 Some common questions about ANOVA", " Chapter 17 Introduction to one-way ANOVA 17.1 Introduction The two-sample t-tests evaluate whether or not the mean of a numeric variable changes among two groups or experimental conditions, which can be encoded by a categorical variable. We pointed out that we could conceptualise these t-tests as evaluating a relationship between between the numeric and categorical variable. The obvious question is, what happens if we need to evaluate differences among means of more than two groups? The ‘obvious’ thing to do might seem to be to test each pair of means using a t-test. However this procedure is tedious and, most importantly, statistically flawed. In this chapter we will introduce an alternative method that allows us to assess the statistical significance of differences among several means at the same time. This method is called Analysis of Variance (abbreviated to ANOVA). ANOVA is one of those statistical terms that unfortunately has two slightly different meanings: In its most general sense ANOVA refers to a methodology for evaluating statistical significance. It appears when working with a statistical model known as the ‘general linear model’. Simple linear regression is a special case of the general linear model. Essentially, whenever we see an F-ratio in a statistical test we’re carrying out an Analysis of Variance of some kind. We saw this crop up when we tested the significance of the regression slope. In its more narrow sense the term ANOVA is used to describe a particular type of statistical model. When used like this ANOVA refers to models that compare means among two or more groups (ANOVA models are also examples of general linear models). The ANOVA-as-a-model is the focus of this chapter. ANOVA models underpin the analysis of many different kinds of experimental data; they are one of the main ‘work horses’ of basic data analysis. As with many statistical models we can use ANOVA without really understanding the details of how it works. However, when it comes to interpreting the results of statistical tests associated with ANOVA, it is important to at least have a basic conceptual understanding of how it works. The goal of this chapter is to provide this basic understanding. We’ll do this by exploring the simplest type of ANOVA model: a one-way Analysis of Variance. 17.2 Why do we need ANOVA models? The corncrake, Crex crex, underwent severe declines in the UK thought to be due to changes in farming practices. Captive breeding and reintroduction programmes were introduced to try to help supplement the wild population. The scientists in charge of the breeding programme wanted to determine the success of 4 different supplements (the predictor variable) for increasing initial growth (the response variable) in the captively bred hatchlings. They conducted an experiment in which groups of 8 hatchlings were fed with different supplements. A fifth group of 8 hatchlings served as the control group—they were given the base diet with no supplements. At the end of the experiment they measured how much weight each hatchling had gained over the week. We can plot the weight gain of 8 hatchlings on each of the supplements (this is the raw data), along with the means of each supplement group, the standard error of the mean, and the sample mean of all the data: The grey points are the raw data, the means and standard error of each group are in blue, and the overall sample mean is shown by the dashed red line. We can see that there seem to be differences among the means: hatchlings in each of the different groups often deviate quite a lot from the overall average for all of the hatchlings in the study (the dashed line). This would seem likely to be an effect of the supplements they are on. At the same time, there is still a lot of variation within each of the groups: not all of the hatchlings on the same supplements has the same weight gain. Perhaps all of this could be explained away as sampling variation—i.e. the supplements make no difference at all to weight gain. Obviously we need to apply a statistical test to decide whether these differences are ‘real’. It might be tempting to use t-tests to compare each mean value with every other. However, this would involve 10 t-tests. Remember, if there is no effect of supplement, each time we do a t-test there is a chance that we will get a false significant result. If we use the conventional p = 0.05 significance level, there is a 1 in 20 chance of getting such ‘false positives’. Doing a large number of such tests increases the overall risk of finding a false positive. In fact doing ten t-tests on all possible comparisons of the 5 different supplements gives about 40% chance of at least one test giving a false significant difference, even though each individual test is conducted with p = 0.05. That doesn’t sound like a very good way to do science. We need a reliable way to determine whether there is a significance of differences between several means without increasing the chance of getting a spurious result. That’s the job of Analysis of Variance (ANOVA). Just as a two sample t-test compares means between two groups, ANOVA compares means among two or more groups. The fundamental job of an ANOVA model is to compares means. So why is it called Analysis of Variance? Let’s find out… 17.3 How does ANOVA work? The key to understanding ANOVA is to realise that it works by examining the magnitudes of different sources of variation in the data. We start with the total variation in the response variable—the variation among all the units in the study—and then partition this into two sources: Variation due to the effect of experimental treatments or control groups. This is called the ‘between-group’ variation. This describes the variability that can be attributed to the different groups in the data (e.g. the supplement groups). This is the same as the ‘explained variation’ described in the Relationships and regression chapter. It quantifies the variation that is ‘explained’ by the different means. Variation due to other sources. This second source of variation is usually referred to as the ‘within-group’ variation because it applies to experimental units within each group. This quantifies the variation due to everything else that isn’t accounted for by the treatments. Within-group variation is also called the ‘error variation’. We’ll mostly use this latter term because it is a bit more general. ANOVA does compare means, but it does this by looking at changes in variation. That might seem odd, but it works! If the amount of variation among treatments is sufficiently large compared to the within-group variation, this suggests that the treatments are probably having an effect. This means that in order to understand ANOVA we have to keep three sources of variation in mind: the total variation, the between-group variation, and the error variation. We’ll get a sense of how this works by carrying on with the corn crake example. We’ll look at how to quantify the different sources of variation, and then move on to evaluate statistical significance using these quantities. The thing to keep in mind is that the logic of these calculations is no different from that used to carry out a regression analysis. The only real difference is that instead of fitting a line through the data, we fit means to different groups when working with an ANOVA model. Total variation The figure below shows the weight gain of each hatchling in the study and the grand mean (i.e. we have not plotted the group-specific means). The vertical lines show the distance between each observation and the grand mean—we have ordered the data within each group so that the plot is a little tidier. A positive deviation occurs when a point is above the line, and a negative deviation corresponds to a case where the point is below the line. We’re not interested in the direction of these deviations. What we need to quantify is the variability of the deviations, which is a feature of their magnitude (the length of the lines). What measure of variability should we use? We can’t add up the deviations because they add to zero. Instead, we apply the same idea introduced in the Relationships and regression chapter: the measure of variability we need is based on the ‘sum of squares’ (abbreviated SS) of the deviations. A sum of squares is calculated by taking each deviation in turn, squaring it, and adding up the squared values. Here are the numeric values of the deviations shown graphically above: ## [1] -5.05 1.95 -1.05 -2.05 -3.05 -4.05 -8.05 1.95 -3.05 -0.05 ## [11] -8.05 3.95 4.95 0.95 6.95 1.95 1.95 5.95 -1.05 -2.05 ## [21] 1.95 1.95 -3.05 -2.05 3.95 -13.05 -8.05 -2.05 -7.05 -1.05 ## [31] 1.95 -5.05 -1.05 6.95 5.95 6.95 6.95 0.95 8.95 2.95 The sum of squares of these numbers is 967.9. This is called the total sum of squares, because this measure of variability completely ignores the information about treatment groups. It is a measure of the total variability in the response variable, calculated relative to the grand mean. Residual variation The next component of variability we need relates to the within-group variation. Let’s replot the original figure showing the weight gain of each hatchling (points), the mean of each supplement group (horizontal blue lines), and the grand mean: The vertical lines show something new this time. They display the distance between each observation and the group-specific means, which means they summarise the variation among hatchlings within treatment groups. Here are the numeric values of these deviations: ## [1] -9.250 -5.625 -9.000 -4.250 -3.250 -2.625 -1.250 -1.625 -0.625 -4.000 ## [11] -3.500 0.375 -2.500 -2.500 1.750 1.375 -1.500 2.750 -5.875 -1.000 ## [21] 0.000 -3.875 4.375 4.375 1.000 1.500 1.500 1.500 5.750 -1.875 ## [31] 3.000 7.750 4.000 5.500 1.125 6.000 2.125 2.125 2.125 4.125 These values are a type of residual: they quantify the variation that is ‘left over’ after accounting for differences due to treatment groups. Once again, we can summarise this variability as a single number by calculating the associated sum of squares, calculated by taking each deviation in turn, squaring it, and adding up the squared values. The sum of squares of these numbers is (610.25). This is called the residual sum of squares7. It is a measure of the variability that may be attributed to differences among individuals after controlling for the effect of different groups. Between-group variation The last component of variability we need relates to the between group variation. We’ll replot the figure one more time, but this time we’ll show just the group-specific means (blue points), the overall grand mean (dashed red line), and the deviations of each group mean from the grand mean: Now the vertical lines show the distance between each group-specific mean and the grand mean. We have five different treatment groups, so there are only five lines. These lines show the variation due to differences among treatment groups. Here are the numeric values of these deviations: ## [1] -2.425 0.950 0.450 -3.800 4.825 These values quantify the variation that can be attributed to differences among treatments. Once again, we can summarise this variability as a single number by calculating the associated sum of squares—this number is called the treatment sum of squares. This is the same as the ‘explained sum of squares’ discussed in the context of regression. It is a measure of the variability attributed to differences among treatments. This is 44.71 in the corn crake example. Notice that this is much smaller than the total sum of squares and the residual sum of squares. This isn’t all that surprising as it is based on five numbers, whereas the other two measures of variability are based on all the observations. 17.3.1 Degrees of freedom The problem with the raw sums of squares in ANOVA is that they are a function of sample size and the number of groups. In order to be useful, we need to convert them into measures of variability that don’t scale with sample size. We use degrees of freedom (written as df, or d.f.) to do this. We came across the concept of degrees of freedom when we studied regression: the degrees of freedom associated with a sum of squares is a measure of how much ‘information’ it is based on. Each of the three sums of squares we just calculated has a different degrees of freedom calculation associated with it: Total d.f. = (Number of observations - 1) Treatment d.f. = (Number of treatment groups - 1) Error d.f. = (Number of observations - Number of treatment groups) The way to think about these is as follows. We start out with a degrees of freedom that is equal to the total number of deviations associated with a sum of squares. We ‘lose’ one degree of freedom for every mean we have to calculate to work out the deviations. Here is how this works in the corn crake example: Total d.f. = 40 - 1 = 39 — The total sum of squares was calculated using all 40 observations in the data, and the deviations were calculated relative to 1 mean (the grand mean). Treatment d.f. = 5 - 1 = 4 — The treatment sum of squares was calculated using the 5 treatment group means, and the deviations were calculated relative to 1 mean (the grand mean). Error d.f. = 40 - 5 = 35 — The error sum of squares was calculated using all 40 observations in the data, and the deviations were calculated relative to 5 means (the treatment group means). Don’t worry too much if that seems confusing. We generally don’t have to carry out degrees of freedom calculations by hand because R will do them for us. We have explained them because knowing where they come from helps us understand the output of an ANOVA significance test. 17.3.2 Mean squares, variance ratios, and F-tests Once we know how to calculate the degrees of freedom we can use them to standardise each of the sums of squares. The calculations are very simple. We take each sum of squares and divide it by its associated degrees of freedom. The resulting quantity is called a mean square (abbreviated as MS): \\[ \\text{Mean square} = \\frac{\\text{Sum of squares}}{\\text{Degrees of freedom}} \\] We stated what a mean square represents when discussing regression: it is an estimate of a variance. The mean squares from an ANOVA quantify the variability of the whole sample (total MS), the variability explained by treatment group (treatment MS), and the unexplained residual variation (residual MS). ANOVA quantifies how strong the treatment effect is by comparing the treatment mean square to the residual mean square. When the treatment MS is large relative to the residual MS this suggests that the treatments are more likely to be having an effect. In reality, they are compared by calculating the ratio between them (designated by the letter F): \\[F = \\mbox{Variance ratio} = \\frac{\\mbox{Variance due to treatments}}{\\mbox{Error variance}}\\] This is the same as the F-ratio mentioned in the context of regression. When the variation among treatment means (treatment MS) is large compared to the variation due to other factors (residual MS) then the F-ratio will be large too. If the variation among treatment means is small relative to the residual variation then the F-ratio will be small. How do we decide when the F-ratio is large enough? That is, how do we judge a result to be statistically significant? We play out the usual ‘gambit’: We assume that there is actually no difference between the population means of each treatment group. That is, we hypothesise that the data in each group are sampled from a single population with one mean. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation. The ‘information’ in this case are the mean squares. We then ask, ‘if there is no difference between the groups, what is the probability that we would observe a variance ratio that is the same as, or more extreme than, the one we actually observed in the sample?’ If the observed variance ratio is sufficiently improbable, then we conclude that we have found a ‘statistically significant’ result, i.e. one that is inconsistent with the hypothesis of no difference. In order to work through these calculations we make one key assumption about the population from which the data in each treatment group has been sampled. We assume that the residuals are normally distributed. Once we make this assumption the distribution of the F-ratio under the null hypothesis (the ‘null distribution’) has a particular form: it follows an F distribution. This means we assess the statistical significance of differences between means by comparing the F-ratio calculated from a sample of data to the theoretical F distribution. This procedure is a type of F-test—it is really no different from the significance testing methodology outlined for regression models. The important message is that ANOVA works by making just one comparison: the treatment variation and the error variation, rather than the ten t-tests that would have been required to compare all the pairs. 17.4 Different kinds of ANOVA model There are many different flavours of ANOVA model. The one we’ve just been learning about is called a one-way ANOVA. It’s called one-way ANOVA because it involves only one factor: supplement type (this includes the control). If we had considered two factors—e.g. supplement type and amount of food—we would have to use something called a two-way ANOVA. A design with three factors is called a three-way ANOVA, and… you get the idea. There are many other ANOVA models, each of which is used to analyse a specific type of experimental design. We are only going to consider three different types of ANOVA in this book: one-way ANOVA, two-way ANOVA, and ANOVA for one-way, blocked design experiments. 17.5 Some common questions about ANOVA To finish off with, three common questions that often arise: 17.5.1 Can ANOVA only be applied to experimental data? We have been discussing ANOVA in the context of a designed experiment (i.e. we talked about treatments and control groups). Although ANOVA was developed to analyse experimental data—and that is where it is most powerful—it can be used in an observational setting. As long as we’re careful about how we sample different kinds of groups (i.e. at random), we can use ANOVA to analyse differences between them. The main difference between ANOVA for experimental and observational studies arises in the interpretation of the results. If the data aren’t experimental, we can’t say anything concrete about the causal nature of the among-group differences we observe. 17.5.2 Do we need equal replication? So far we have only considered the use of ANOVA with data in which each treatment has equal replication. One of the frequent problems with biological data is we often don’t have equal replication, even if we started with equal replication in our design. Plants and animals have a habit of dying before we have gathered all our data; a pot may get dropped, a culture contaminated, all sorts of things conspire to upset even the best designed experiments. Fortunately one-way ANOVA does not require equal replication, it will work even where sample sizes differ between treatments. 17.5.3 Can ANOVA be done with only two treatments? Although the t-test provides a convenient way of testing means from two treatments, there is nothing to stop you doing an ANOVA on two treatments. A t-test (assuming equal variances) and ANOVA on the same data should give the same p-value (in fact the F-statistic from the ANOVA will be the square of the t-value from the t-test). One advantage to the t-test, however, is that you can do the version of the test that allows for unequal variances—something a standard ANOVA does not do. There is a version of Welch’s test for one-way ANOVA, but we won’t study it in this course (look at the oneway.test function if you are interested). You will sometimes see something called error sum of squares, or possibly, the within-group sum of squares. These are just different names for the residual sum of squares.↩ "],
["one-way-anova-in-r.html", "Chapter 18 One-way ANOVA in R 18.1 Introduction 18.2 Factors in R 18.3 Visualising the data 18.4 Fitting the ANOVA model 18.5 Interpreting the results 18.6 Summarising and presenting the results of ANOVA", " Chapter 18 One-way ANOVA in R 18.1 Introduction Our goal in this chapter is to learn how to work with one-way ANOVA models in R. Just as we did with regression, we’ll do this by working through an example. We’ll start with the problem and the data, and then work through model fitting, significance testing, and finally, presenting the results. Before we can do this though, we need to side track a bit and learn about ‘factors’ in R… Walk through You should begin working through the corn crake example from this point. You will need to download the CORN_CRAKE.CSV file from MOLE and place it in your working directory. 18.2 Factors in R Remember factors? An experimental factor is a controlled variable whose levels (‘values’) are set by the experimenter. R is primarily designed to carry out data analysis so we shouldn’t be surprised that it has a special type of vector to represent factors. This kind of vector in R is called, rather sensibly, a factor. We have largely ignored factors in R up until now because we haven’t needed to use them8. However, we now need to understand how they work because much of R’s plotting and statistical modelling facilities rely on factors. We’ll look at the corn crake data (stored in corn_crake) to begin getting a sense of how they work: corn_crake &lt;- read.csv(file = &quot;CORN_CRAKE.CSV&quot;) First, we need to be able to actually recognise a factor when we see one. Here is the result of using glimpse with the diet data: glimpse(corn_crake) ## Observations: 40 ## Variables: 2 ## $ Supplement &lt;fctr&gt; None, None, None, None, None, None, None, None, Si... ## $ WeightGain &lt;int&gt; 13, 20, 17, 16, 15, 14, 10, 20, 15, 18, 10, 22, 23,... This tells us that there are 40 observations in the data set and 2 variables (columns), called Supplement and WeightGain. The text next to $ Supplement says &lt;fctr&gt;. We can guess what that stands for…. it is telling us that the Supplement vector inside corn_crake is a factor. The Supplement factor was created automatically when we read the data stored in CORN_CRAKE.CSV into R. When we read in a column of data that is non-numeric, read.csv will often decide to turn it into a factor for us9. R is generally fairly good at alerting us to the fact that a variable is stored as a factor. For example, look what happens if we extract the Supplement column from corn_crake and print it to the screen: corn_crake$Supplement ## [1] None None None None None None None ## [8] None Sizefast Sizefast Sizefast Sizefast Sizefast Sizefast ## [15] Sizefast Sizefast Linseed Linseed Linseed Linseed Linseed ## [22] Linseed Linseed Linseed Allvit Allvit Allvit Allvit ## [29] Allvit Allvit Allvit Allvit Earlybird Earlybird Earlybird ## [36] Earlybird Earlybird Earlybird Earlybird Earlybird ## Levels: Allvit Earlybird Linseed None Sizefast This obviously prints out the values of each element of the vector, but look at the last line: ## Levels: Allvit Earlybird Linseed None Sizefast This alerts us to the fact that Supplement is a factor, with 5 levels (the different supplements). Look at the order of the levels: these are alphabetical by default. Remember that! The order of the levels in a factor can be important as it controls the order of plotting in ggplot2 and it affects the way R presents the summaries of statistics. This is why we are introducing factors now—we are going to need to manipulate the levels of factors to alter the way our data are plotted and analysed. 18.3 Visualising the data As always before we fit a model we should plot our data. We can do this using a boxplot. ggplot(corn_crake, aes(x = Supplement, y = WeightGain)) + geom_boxplot() At this point we just want to get an idea of what our data look like. We don’t need to worry about customising the plot to make it look nicer. Examine the plot Have a look at the plot and think about what it means biologically e.g. which supplement seems to have had the biggest effect? Do all of the supplements increase growth relative to the control? 18.4 Fitting the ANOVA model Carrying out ANOVA in R is quite simple, but as with regression, there is more than one step. The first involves a process known as fitting the model (or just model fitting). This is the step where R calculates the relevant means, along with the additional information needed to generate the results in step two. We call this step model fitting because an ANOVA is a type of model for our data: it is a model that allows the mean of a variable to vary among groups. How do we fit an ANOVA model in R? We will do it using the lm function again. Remember what the letters ‘lm’ stand for? They stand for ‘(general) linear model’. So… an ANOVA model is just another special case of the general linear model. Here is how we fit a one-way ANOVA in R, using the corncrake data: corncrake_model &lt;- lm(WeightGain ~ Supplement, data = corn_crake) Hopefully by now this kind of thing is starting to look familiar. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ must be the numeric response variable whose means we want to compare among groups. The variable on the right should be the indicator (or predictor) variable that says which group each observation belongs to. These are WeightGain and Supplement, respectively. The second argument is the name of the data frame that contains the two variables listed in the formula. Why does R carry out ANOVA? How does R know we want to use an ANOVA model? After all, we didn’t specify this anywhere. The answer is that R looks at what type of vector Supplement is. It is a factor, and so R automatically carries out an ANOVA. It would do the same if Supplement had been a character vector. However, if the levels of Supplement had been stored as numbers (1, 2, 3, …) then R would not have fitted an ANOVA. We’ve already seen what would have happened. It would have assumed we meant to fit a regression model. This is why we don’t store categorical variables as numbers in R. Avoid using numbers to encode the levels of a factor, or any kind of categorical variable, if you want to avoid making mistakes in R. Notice that we did not print the results to the console. Instead, we assigned the result a name —corncrake_model now refers to a model object. What happens if we print this to the console? corncrake_model ## ## Call: ## lm(formula = WeightGain ~ Supplement, data = corn_crake) ## ## Coefficients: ## (Intercept) SupplementEarlybird SupplementLinseed ## 14.250 8.625 4.250 ## SupplementNone SupplementSizefast ## 1.375 4.750 Not a great deal. Printing a fitted model object to the console is not very useful when working with ANOVA. We just see a summary of the model we fitted and some information about the coefficients of the model. Yes, an ANOVA model has coefficients, just like a regression does. 18.5 Interpreting the results What we really want is a p-value to help us determine whether there is statistical support for a difference among the group means. That is, we need to calculate things like degrees of freedom, sums of squares, mean squares, and the F-ratio. This is step 2. We use the anova function to do this: anova(corncrake_model) ## Analysis of Variance Table ## ## Response: WeightGain ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Supplement 4 357.65 89.413 5.1281 0.002331 ** ## Residuals 35 610.25 17.436 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that all we did was pass the anova function one argument: the name of the fitted model object. Let’s step through the output to see what it means. The first line just informs us that we are looking at an ANOVA table, i.e. a table of statistical results from an analysis of variance. The second line just reminds us what variable we analysed. The important information is in the table that follows: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Supplement 4 357.65 89.413 5.1281 0.002331 ** ## Residuals 35 610.25 17.436 This is an Analysis of Variance Table. It summarises the parts of the ANOVA calculation: Df – degrees of freedom, Sum Sq – the sum of squares, Mean Sq – the mean square, F value – the F-ratio (i.e. variance ratio), Pr(&gt;F) – the p-value. The F-ratio (variance ratio) is the key term. This is the test statistic. It provides a measure of how large and consistent the differences between the means of the five different treatments are. Larger values indicate clearer differences between means, in just the same way that large values of Student’s t indicate clearer differences between means in the two sample situation. The p-value gives the probability that the observed differences between the means, or a more extreme difference, could have arisen through sampling variation under the null hypothesis. What is the null hypothesis: it is one of no effect of treatment, i.e. the null hypothesis is that all the means are the same. As always, the p-value of 0.05 is used as the significance threshold, and we take p &lt; 0.05 as evidence that at least one of the treatments is having an effect. For the corncrake data, the value is 5.1, and the probability (p) of getting an F-ratio this large is given by R as 0.0023, i.e. less than 0.05. This provides good evidence that there are differences in weight gain between at least some of the treatments. So far so good. The test that we have just carried out is called the global test of significance. It goes by this name because it doesn’t tell us anything about which means are different. The analyses suggest that there is an effect of supplement on weight gain, but some uncertainty remains because we have only established that there are differences among at least some supplements. A global test doesn’t say which supplements are better or worse. This could be very important. If the significant result is generated by all supplements being equally effective (hence differing from the control but not from each other) we would draw very different conclusions than if the result was a consequence of one supplement being very effective and all the others being useless. Our result could even be produced by the supplements all being less effective than the control! So having got a significant result in the ANOVA, we should always look at the means of the treatments to understand where the differences actually lie. We did this in the previous chapter but here is the figure again anyway: What looking at the means tells us is that the effect of the supplements is generally to increase weight gain (with one exception, ‘Allvit’) relative to the control group (‘None’), and that it looks as though ‘Earlybird’ is the most effective, followed by ‘Sizefast’ and ‘Linseed’. Often inspection of the means in this way will tell us all we need to know and no further work will be required. However, sometimes it is desirable to have a more rigorous way of testing where the significant differences between treatments occur. A number of tests exist as ‘add ons’ to ANOVA which enable you to do this. These are called post hoc multiple comparison tests (sometimes just ‘multiple comparison tests’). We’ll see how to conduct these in the Multiple comparison tests chapter. 18.6 Summarising and presenting the results of ANOVA As with all tests it will usually be necessary to summarise the result from the test in a written form. With an ANOVA on several treatments, we always need to at least summarise the results of the global test of significance, for example: There was a significant effect of supplement on the weight gain of the corncrake hatchlings (ANOVA: F=5.1; d.f.= 4,35; p&lt;0.01). There are several things to notice here: The degrees of freedom are always quoted as part of the result, and…there are two values for the degrees of freedom to report in ANOVA because it involves F-ratios. These are obtained from the ANOVA table and should be given as the treatment degrees of freedom first, followed by the error degrees of freedom. Order matters. Don’t mix it up. The degrees of freedom are important because, like a t-statistic, the significance of an F-ratio depends on the degrees of freedom, and giving them helps the reader to judge the result you are presenting. A large value may not be very significant if the sample size is small, a smaller may be highly significant if the sample sizes are large. The F-ratio rarely needs to be quoted to more than one decimal place. When it comes to presenting the results in a report, it helps to present the means, as the statement above cannot entirely capture the results. We could use a table to do this, but tables are ugly and difficult to interpret. A good figure is much better. You won’t be assessed on your ability to produce summary plots such as those below. Nonetheless, you should try to learn how to make them because you will need to produce these kinds of figures in your own projects. Box and whiskers plots and multi-panel dot plots / histograms are exploratory data analysis tools. We use them at the beginning of an analysis to understand the data, but we don’t tend to present them in project reports or scientific papers. Since ANOVA is designed to compare means, a minimal plot needs to show the point estimates of each group-specific mean, along with a measure of their uncertainty. We often use the standard error of the means to summarise this uncertainty. In order to be able to plot these quantities we first have to calculate them. We can do this using dplyr. Here’s a reminder of the equation for the standard error of a mean: \\[ SE = \\frac{\\text{Standard deviation of the sample}}{\\sqrt{\\text{Sample size}}} = \\frac{SD}{\\sqrt{n}} \\] So, the required dplyr code is: # get the mean and the SE for each supplement corncrake_stats &lt;- corn_crake %&gt;% group_by(Supplement) %&gt;% summarise(Mean = mean(WeightGain), SE = sd(WeightGain)/sqrt(n())) # print to the console corncrake_stats ## # A tibble: 5 x 3 ## Supplement Mean SE ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Allvit 14.250 1.979809 ## 2 Earlybird 22.875 1.231107 ## 3 Linseed 18.500 1.069045 ## 4 None 15.625 1.209154 ## 5 Sizefast 19.000 1.690309 Notice that we used the n function to get the sample size. The rest of this R code should be quite familiar by now. We gave the data frame containing the group-specific means and standard errors the name corncrake_stats. We have a couple of different options for making a good summary figure. The first plots a point for each mean and places error bars around this to show ±1 SE. In order to do this using ggplot2 we have to add two layers—the first specifies the points (the means) and the second specifies the error bar (the SE). Here is how to do this: ggplot(data = corncrake_stats, aes(x = Supplement, y = Mean, ymin = Mean - SE, ymax = Mean + SE)) + # this adds the means geom_point(colour = &quot;blue&quot;, size = 3) + # this adds the error bars geom_errorbar(width = 0.1, colour = &quot;blue&quot;) + # controlling the appearance scale_y_continuous(limits = c(0, 30)) + # use sensible labels xlab(&quot;Supplement treatment&quot;) + ylab(&quot;Weight gain (g)&quot;) + # flip x and y axes coord_flip() + # use a more professional theme theme_bw() First, notice that we set the data argument in ggplot to be the data frame containing the summary statistics (not the original raw data). Second, we set up four aesthetic mappings: x, y, ymin and ymax. Third, we added one layer using geom_point. This adds the individual points based on the x and y mappings. Fourth, we added a second layer using geom_errorbar. This adds the error bars based on the x, ymin and ymax mappings. Finally we adjusted the y limits and the labels (this last step is optional). Ask a demonstrator to step through this with you if you are confused by it. Take a close look at that last figure. Is there anything wrong with it? The control group in this study is the no diet group (‘None’). Conventionally, we display the control groups first. R hasn’t done this because the levels of Supplement are in alphabetical order by default. If we want to change the order of plotting, we have to change the way the levels are organised. Here is how to to this using dplyr and a function called factor: corncrake_stats &lt;- corncrake_stats %&gt;% mutate(Supplement = factor(Supplement, levels = c(&quot;None&quot;, &quot;Sizefast&quot;, &quot;Linseed&quot;, &quot;Allvit&quot;, &quot;Earlybird&quot;))) We use mutate to update Supplement, using the factor function to redefine the levels of Supplement and overwrite the original. Now, when we rerun the ggplot2 code we end up with a figure like this: The treatments are presented in the order specified with the levels argument. Problem solved! A bar plot is another popular visualisation for summarising the results of an ANOVA. We only have to change one thing about the last chunk of ggplot2 code to make a bar plot. Instead of using geom_point, we use geom_col (we’ll drop the coord_flip bit too): ggplot(data = corncrake_stats, aes(x = Supplement, y = Mean, ymin = Mean - SE, ymax = Mean + SE)) + # this adds the means geom_col(fill = &quot;lightgrey&quot;, colour = &quot;grey&quot;) + # this adds the error bars geom_errorbar(width = 0.1, colour = &quot;black&quot;) + # controlling the appearance scale_y_continuous(limits = c(0, 30)) + # use sensible labels xlab(&quot;Supplement treatment&quot;) + ylab(&quot;Weight gain (g)&quot;) + # use a more professional theme theme_bw() We have worked with factors, but only when reordering the labels of a categorical variable plotted in ggplot2.↩ This behaviour isn’t really all that helpful. It would be better if R let us, the users, decide whether or not to turn something into a factor vector. However, in the early days of R computers had limited memory, which meant storing a categorical variable as a factor made sense; it saved memory. This benefit of factors has long since disappeared, but we’re stuck with the turn-everything-into-a-factor behaviour.↩ "],
["assumptions-and-diagnostics.html", "Chapter 19 Assumptions and diagnostics 19.1 Understanding data 19.2 Assumptions of regression 19.3 Regression diagnostics 19.4 Assumptions of one-way ANOVA", " Chapter 19 Assumptions and diagnostics We usually have an analysis in mind when we design an experiment or observational data collection protocol. It may be tempting to jump straight into this analysis without carefully examining the data first. This is never a good idea. In the past few chapters we have repeatedly emphasised that careful data analysis always begins with inspection of the data. Visualising a dataset helps us to understand the data and evaluate whether or not the assumptions of a statistical tool are likely to be violated. As we learnt in the last section regression and one-way ANOVA are both types of linear model. They are parametric techniques and as such they make a number of assumptions that we need to be aware of. If our data do not meet the assumptions of the tests then we cannot rely on the results (i.e. the p-values) given by those tests. 19.1 Understanding data We’ve been using ‘well-behaved’ data sets in this book so far, which tends to give the impression that visual inspections of the data are not all that necessary. Here’s an example of why it matters. Imagine we are interested in quantifying the relationship between two variables, called \\(x\\) and \\(y\\). We might be tempted to carry out a linear regression analysis without first inspecting these data to get straight to ‘the answer’: the coefficients of the linear regression model. This could be very misleading. Take a look at these four scatter plots: These four artificial data sets were constructed by the statistician Francis Anscombe. The means and variances of \\(x\\) and \\(y\\) are nearly identical in all four data sets, and what’s more, the intercepts and slopes of the best fit regression lines are almost identical (the intercept and slope are 3.00 and 0.500, respectively). The nature of the relationship between \\(x\\) and \\(y\\) is quite obviously different among the four cases: “Case 1” shows two linearly related, normally distributed variables. This is the kind of data we often hope for in a statistical analysis. “Case 2” shows two variables that are not normally distributed, but there is a perfect non-linear relationship between the two. “Case 3” shows an example the variables are perfectly linearly associated for all but one observation which ruins the perfect relationship. “Case 4” shows an example where a single outlier generates an apparent relationship where the two variables are otherwise unrelated. Each of these plots tells a different story about the relationship between \\(x\\) and \\(y\\), yet the linear regression model says the same thing is happening in each case. These are obviously somewhat pathological examples, but they clearly illustrate the kinds of issues that can, and do, arise with real data. There is a real risk we will apply an inappropriate analysis if we fail to detect these kinds of problems. Every statistical model makes certain assumptions about the data10. Even if a dataset doesn’t exhibit the very obvious problems seen in the Anscombe examples, we still need to assess whether the assumptions of the statistical model we want to use are likely to be valid. For example, when working with a linear regression model, we started with a scatter plot of the response variable vs. the predictor variable. This allowed us to assess whether the two variables are linearly related. However, as we noted at the time linearity is not the only assumption we need to think about when carrying out a linear regression. In the rest of this chapter we’ll go through the remaining assumptions for regression and also consider the assumptions for a one-way ANOVA. In the [Introduction to regression diagnostics] and Using regression diagnostics chapters we’ll move on to how to check whether these assumptions are valid with your data. 19.2 Assumptions of regression Let’s consider each of the assumptions of a regression, in their approximate order of importance: Independence. The residuals must be independent. Another way of stating this assumption is that the value of each residual does not depend on the value of any others. This can be difficult to check. If the data are from a carefully designed experiment, everything should be OK. If the data are observational, then we need to be a lot more careful. This assumption matters because when the residuals are not independent any p-values we generate will be unreliable. Measurement scale. The response (\\(y\\)) and predictor (\\(x\\)) variables are measured on an interval or ratio scale. It doesn’t really make sense to use categorical data in a regression11. This one is easy to assess. Linearity. The relationship between the predictor \\(x\\) variable and the response \\(y\\) variable is linear. Obviously there is little point in fitting a straight line to data which clearly don’t form a straight line relationship. There may also be circumstances in which it is theoretically unlikely for a relationship to be linear, e.g. the length and weight of an animal will not be well described by a linear relationship because weight is roughly a cubic function of length. If the data fail this assumption then applying a mathematical transformation of \\(x\\) or \\(y\\) can help. We will come back to this idea in the Data transformations chapter. Constant variance. The variance of the residuals is constant. This assumption essentially means the variability of the residuals is not related to the value of the predictor \\(x\\) variable. It is violated if the magnitude of the residuals increase or decrease markedly as \\(x\\) gets larger. If the data fail this assumption then again, sometimes applying a mathematical transformation of \\(y\\) will help. Normality. The residuals are drawn from a normal distribution. This essentially means that for a particular value of \\(x\\) we would expect there to be a range of responses in \\(y\\) which follow a normal distribution. It is the distribution of the deviations of \\(y\\) from the fitted line (the residuals) that are assumed to be normal, not the raw \\(y\\) values. This means that we can generally only test this assumption after the line has been fitted. It does not make sense to evaluate this assumption by looking at the raw data. Measurement error. The values of the predictor \\(x\\) variable are determined with negligible measurement error12. It is often hard to obtain the \\(x\\) values with absolutely no measurement error, but the error \\(x\\) in should at least be smaller than that in the \\(y\\) values. So for example, in the thermal tolerance experiment the temperature values (set by the experimenter) almost certainly have little error, so it is appropriate to use regression. Next, we will learn how to check whether each assumption applies to your data. Assumptions 1 (independence), 2 (measurement scale) and 6 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data. This is why it’s very important to consider how you are going to analyse your data before you start collecting it. You don’t want to spend lots of time and effort collecting data that you won’t be able to analyse. A special set of tools called ‘regression diagnostics’ allow us to evaluate the other assumptions (linearity, constant varaince and normality). Regression diagnostics use properties of the fitted model to understand how well the model fits the data and evaluate the model assumptions. 19.3 Regression diagnostics We’ll learn how to use regression diagnostics by working through an example. A survey was carried out to establish whether the abundance of hedgerows in agricultural land had an effect on the abundance of grey partridge. From an area of agricultural land covering several farms, 40 plots were selected which had land uses as similar as possible, but differed in the density of hedgerows (km hedgerow per km2). The density of partridges was established by visiting all fields in a study plot once immediately after dawn and once just before dusk, when partridges are most likely to be seen. Counts of birds observed were made on each visit and the dawn and dusk data were averaged to give a value for partridge abundance for each study plot. Assumption 2 (measurement scale) is easy to evaluate. Assumptions 1 (independence) and 6 (measurement error) can’t be checked by just looking at the data; we have to think about the data to decide if there are any obvious reasons why they might not be valid. We’ll assume here that the independence assumption is true. Why do you think we averaged the dawn and dusk partridge counts rather than including both counts in the model? Which assumption would this have invalidated? If you’re not sure ask a TA. We’ll come back to this more in the Principles of experimental design chapter. As hedges cannot move there should be relatively little measurement error in the values of the predictor variable in this study. This leaves assumptions 3 (linearity), 4 (constant variance) and 5 (normality). There is a specific diagnostic plot for each of these. The data are stored in a CSV file PARTRIDG_BIGSTUDY.CSV (not PARTRIDG.CSV!). The density of hedgerows (km per km2) is in the Hedgerow variable and the density of partridges (no. per km2) is in the Partridge variable. Read the data into R, calling it partridge. As always we’ll start by plotting our data. Spend some time looking at the scatter plot. Do you think these data satisfy the linearity assumption? 19.3.1 Fitted values In order to understand regression diagnostics we have to know what a fitted value is. The phrase ‘fitted value’ is just another expression for ‘predicted value’. Look at the plot below: This shows the raw data (black points), the line of best fit (blue line), the residuals (the vertical grey lines), and the fitted values (red points). We find the fitted values by drawing a vertical line from each observation to the line of best fit. The values of the response variable (Partridge in this case) at the point where these touch the line of best fit are the ‘fitted values’. This means the fitted values are just predictions from the statistical model, generated for each value of the predictor variable. We can use the fitted function to extract these from a fitted model: fitted(partridge_model) ## 1 2 3 4 5 6 7 8 9 10 11 12 ## -11.6 -11.0 -8.4 -7.2 -4.9 -1.4 1.8 7.3 8.2 10.2 11.1 14.3 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 22.1 22.7 27.0 28.2 28.8 35.2 37.8 38.1 43.0 45.4 48.3 52.3 ## 25 26 27 28 29 30 31 32 33 34 35 36 ## 57.6 61.0 62.2 62.2 63.7 68.3 71.2 77.0 86.6 87.8 88.1 90.4 ## 37 38 39 40 ## 91.6 96.8 98.0 101.7 Notice that some of the fitted values are below zero. Why do we see negative fitted values? This doesn’t make much sense biologically (negative partridges?). Do you think it is a problem? 19.3.2 Checking the linearity assumption The linearity assumption states that the general relationship between the response and predictor variable should look like a straight line. We can evaluate this assumption by constructing a residuals vs. fitted values plot. This is a two-step process. First use the fitted and resid functions to construct a data frame containing the fitted values and residuals from the model: plt_data &lt;- data.frame(Fitted = fitted(partridge_model), Resids = resid(partridge_model)) We called the data frame plt_data. Once we have made this data frame, we use ggplot2 to plot the residuals against the fitted values: ggplot(plt_data, aes(x = Fitted, y = Resids)) + geom_point() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) This plot indicates that the residuals tend to be positive at the largest and smallest fitted values, and that they are generally negative in the middle of the range. This U-shaped pattern is indicative of a problem with our model. It tells us that there is some kind of pattern in the association between the two variables that is not being accommodated by the linear regression model we fitted. The U-shape indicates that the relationship is non-linear, and that it ‘curves upward’. We can see where this pattern comes from when we look at the raw data and fitted model again: There is obviously some curvature in the relationship between partridge counts and hedgerow density, yet we fitted a straight line through the data. The U-shape in the residuals vs. fitted value plot comes from an ‘accelerating’ (or ‘convex’) relationship between the response and predictor variables. What other kinds of patterns might we see in a residuals vs. fitted value plot? Two are particularly common: U-shapes and hump-shapes. Look at the two artificial data sets below… The left hand plot is similar to the partridge data: it exhibits a curved, accelerating relationship between the response variable and the predictor variable. The right plot shows a different kind of relationship: there is a curved, decelerating relationship between the two variables. We can fit a linear model to each of these data sets, and then visualise the corresponding residuals vs. fitted value plots: Here we see the characteristic U-shape and hump-shape patterns we mentioned above, these occur when there is an accelerating or decelerating relationship respectively between the response variable and predictor variable. This may seem like a lot of extra work to evaluate an aspect of the model that we can assess by just plotting the raw data. This is true when we are working with a simple linear regression model. However, it can much harder to evaluate the linearity assumption when working with more complicated models where there is more than one predictor variable13. In these situations, a residuals vs. fitted values plot gives us a powerful way to evaluate whether or not the assumption of a linear relationship is reasonable. That’s enough about the residuals vs. fitted values plot. Let’s move on to the normality evaluation… 19.3.3 Checking the normality assumption How should we evaluate the normality assumption of linear regression? That is, how do we assess whether or not the residuals are drawn from a normal distribution? We could extract the residuals from a model and plot their distribution, but there is a more powerful graphical technique to available to us: the normal probability plot. The normal probability plot is used to identify departures from normality. If we know what we are looking for, we can identify many different kinds of problems, but to keep life simple we will focus on the most common type of assessment: determining whether or not the distribution of residuals is excessively skewed. Remember the concept of distributional skew? A skewed distribution is just one that is not symmetric. For example, the first distribution below is skewed to the left (‘negative skew’), the second is skewed to the right (‘positive skew’), and the third is symmetric (‘zero skew’): The skewness in the first two distributions is easy to spot because they contain a lot of data and the skewness is quite pronounced. A normal probability plot allows us to pick up potential problems when we are not so lucky. The methodology underlying construction of a normal probability plot is quite technical, so we will only try to give a flavour of it here. Don’t worry if the next segment is confusing—interpreting a normal probability plot is much easier than making one. We’ll work with the partridge example again. We start by extracting the residuals from the fitted model into a vector, using the resids function, and then standardise these by dividing them by their standard deviation: mod_resids &lt;- resid(partridge_model) mod_resids &lt;- mod_resids / sd(mod_resids) The standardisation step is not essential, but dividing the raw residuals by their standard deviation ensures that the standard deviation of the new residuals is equal to 1. Standardising the residuals like this makes it a little easier to compare more than one normal probability plot. We call these new residuals the ‘standardised residuals’. The next step is to find the rank order of each residual. That is, we sort the data from lowest to highest, and find the position of each case in the sequence (this is its ‘rank’). The function order can do this: resid_order &lt;- order(mod_resids) resid_order ## [1] 35 27 30 32 22 13 23 21 17 15 14 26 24 28 25 9 40 12 20 19 16 29 18 ## [24] 8 7 11 31 10 38 5 6 1 2 3 34 4 33 37 36 39 This tells us that the first residual is the 35th largest, the second is the 27th largest, the third is the 30th largest, and so on. The last step is the tricky one. Once we have established the rank order of the residuals, we ask the following question: if the residuals really were drawn from a normal distribution, what is their most likely value, based on their rank? We can’t really explain how to do this without delving into the mathematics of distributions, so this will have to be a ‘trust us’ situation. As usual, R can do this for us, and we don’t even need the ranks—we just calculated them to help us explain what happens when we build a normal probability plot. The function we need is called qqnorm: all_resids &lt;- qqnorm(mod_resids, plot.it = FALSE) all_resids &lt;- as.data.frame(all_resids) The qqnorm doesn’t produce a data frame by default, so we had to convert the result using a function called as.data.frame. This extra little step isn’t really all that important. The all_resids object is now a data frame with two variables: x contains the theoretical values of normally distributed residuals, based on the rank orders of the residuals from the model, and y contains the actual standardised residuals. Here are the first 10 values: head(all_resids, 10) ## x y ## 1 0.7977768 0.6855052 ## 2 0.8871466 0.7431596 ## 3 0.9842350 0.9021176 ## 4 1.2133396 1.0174265 ## 5 0.6356570 0.5162945 ## 6 0.7143674 0.5478778 ## 7 0.2858409 0.2800927 ## 8 0.2211187 0.1759340 ## 9 -0.2858409 -0.3173156 ## 10 0.4887764 0.4693592 Finally, we can plot these against one another to make a normal probability plot: ggplot(all_resids, aes(x = x, y = y)) + geom_point() + geom_abline(intercept = 0, slope = 1) + xlab(&quot;Theoretical Value&quot;) + ylab(&quot;Standardised Residual&quot;) We used geom_abline(intercept = 0, slope = 1) to add the one-to-one (1:1) line. We haven’t used this function before and we won’t need it again. The one-to-one line is just a line with a slope of 1 and an intercept of 0—if an \\(x\\) value and \\(y\\) value are equal their corresponding point will lie on this line. Don’t worry too much if those calculations seem opaque. We said at the beginning of this section that it’s not important to understand how a normal probability plot is constructed. It is important to know how to interpret one. The important feature to look out for is the positioning of the points relative to the 1:1 line. If the residuals really are drawn from a normal distribution they should generally match the theoretical values, i.e. the points should lie on the 1:1 line. In the partridge example that is exactly what we see. A couple of the more extreme values diverge a little, but this isn’t something to worry about. We never expect to see a perfect 1:1 relationship in these kinds of plots. The vast majority of the points are very close to the 1:1 line though, which provides strong evidence that the residuals probably are sampled from a normal distribution. What does a normal probability plot look like when residuals are not consistent with the normality assumption? Deviations from a straight line suggest departures from normality. How do right skew (‘positive skew’) and left skew (‘negative skew’) manifest themselves in a normal probability plot? Here is the normal probability plot produced using data from the left-skewed distribution above: Rather than a straight line, we see a decelerating curved line. This is the signature of residuals that are non-normal, and left-skewed. We see the opposite sort of curvature when the residuals are right-skewed: You should always use normal probability plots to assess normality assumptions in your own analyses. They work with every kind of model fitted by the lm function. What is more, they also work reasonably well when we only have a few residuals to play with. Seven is probably the lowest number we might accept—with fewer points it becomes hard to distinguish between random noise and a real deviation from normality. That’s enough discussion of normal probability plots. Let’s move on to the constant variance evaluation… 19.3.4 Checking the constant variance assumption How do we evaluate the constant variance assumption of a linear regression? That is, how do we assess whether or not the variability of the residuals is constant or not? This assumption can be evaluated in a similar way to the linearity assumption, by producing something called a ‘scale-location plot’. We construct this by plotting residuals against the fitted values, but instead of plotting raw residuals, we transform them first using the following ‘recipe’: Standardise the residuals by dividing them by their standard deviation. Remember, this ensures the new residuals have a standard deviation of 1. Find the absolute value of the residuals produced in step 1. If they are negative, make them positive, otherwise, leave them alone. Take the square root of the residuals produced in step 2. These calculations are simple enough in R. We’ll demonstrate them using the partridge data set again: # extract the residuals sqrt_abs_resids &lt;- resid(partridge_model) # step 1. standardise them sqrt_abs_resids &lt;- sqrt_abs_resids / sd(sqrt_abs_resids) # step 2. find their absolute value sqrt_abs_resids &lt;- abs(sqrt_abs_resids) # step 3. square root these sqrt_abs_resids &lt;- sqrt(sqrt_abs_resids) Now we use the fitted function to extract the fitted values from the model and place these in a data frame with the transformed residuals: plt_data &lt;- data.frame(Fitted = fitted(partridge_model), Resids = sqrt_abs_resids) We called the data frame plt_data. Once we have made this data frame, we use ggplot2 to plot the transformed residuals against the fitted values: ggplot(plt_data, aes(x = Fitted, y = Resids)) + geom_point() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Square root of absolute residuals&quot;) This is a scale-location plot. Why is this useful? We want to assess whether or not the size of these new residuals increase or decrease as the fitted values get larger. If they do not—the relationship is essentially flat—then we can conclude that the variability in the residuals is constant. Otherwise, we have to conclude that the constant variance assumption is violated. Although the pattern is not exactly clear cut, in this example there seems to be a bit of an upward trend with respect to the fitted values. This suggests that the variability (more formally, the ‘variance’) of the residuals increases with the fitted values. Larger partridge counts seem to be associated with more variability. This is a very common feature of count data. Poor model fit complicates scale-location plots It is worth reflecting on the ambiguity in this pattern. It is suggestive, but it certainly isn’t as clear as the U-shape in the residuals vs. fitted values plot used earlier. There is one potentially important reason for this ambiguity. The model we have used to describe the relationship between partridge counts and hedgerow density is not a very good model for these data. There is curvature in the relationship that we failed to take account of, and consequently, this lack of fit is impacting the scale-location plot. When a model does not fit the data well, the scale-location plot does not only describe the variability in the residuals. It also reflects the lack of fit. The take-home message is that it is a good idea to fix a lack of fit problem before trying to evaluate the constant variance assumption. Non-constant variance can be a problem because it affects the validity of p-values associated with a model. You should aim to use scale-location plots to assess the constant variance assumption in your own analyses, but keep in mind that a scale-location plot may also reflect non-linearity. 19.4 Assumptions of one-way ANOVA As regression and ANOVA are both types of linear model it is unsurprising that the assumptions for these two types of model are similar. There are some differences however, for example the linearity assumption does not make any sense with an ANOVA as the response variable is not numeric. Let’s step through each of the assumptions of a one-way ANOVA: Independence. If the experimental units of the data are not independent, then the p-values generated by an F-test in an ANOVA will not be reliable. This one is important. Even mild non-independence can be a serious problem. Measurement scale. The response variable (\\(y\\)) should be measured on an interval or ratio scale. Equal variance. The validity of F-tests associated with ANOVA depends on an assumption of equal variance in the treatment groups. If this assumption is not supported by the data, then it needs to be addressed. If you ignore it, the p-values that you generate cannot be trusted. There is a version of one-way ANOVA that can work with unequal variances, but we won’t study it in this course. Normality. The validity of F-tests associated with ANOVA also depends on the assumption that the residuals are drawn from a normal distribution. ANOVA is reasonably robust to small departures from normality, but larger departures can start to matter. Unlike the t-test, having a large number of samples doesn’t make this assumption less important. Strictly speaking, assumptions 3 and 4 really apply to the (unobserved) population from which the experimental samples are derived, i.e., the equal variance and normality assumptions are with respect to the variable of interest in the population. However, we often just informally refer to ‘the data’ when discussing the assumptions of ANOVA. The term ‘regression diagnostic’ is a bit of a misnomer. A more accurate term might be ‘linear model diagnostic’ but no one really uses this. Regression diagnostics can be used with many different kinds of models. In fact, the diagnostic plots we have introduced above can be applied to any model fitted by the lm function, including ANOVA models. We’ll see this in the next chapter Even so-called ‘non-parametric’ models have underpinning assumptions; these are just not as restrictive as their parametric counterparts.↩ It sometimes makes sense to use a regression analysis when the predictor variable is an ordinal categorical variable. It depends what you want to do with the resulting model. However, some people disagree with this approach, so it’s best to avoid doing it unless you’re confident you can justify it.↩ It is the measurement error, not the sampling error, that matters. This means it is fine to use regression when the \\(x\\) variable represent a sample from a population.↩ This is the situation we face with multiple regression. A multiple regression is a type of regression with more than one predictor variable—we don’t study them in this course, but they are often used in biology.↩ "],
["using-regression-diagnostics.html", "Chapter 20 Using regression diagnostics 20.1 Introduction 20.2 Diagnostics for regression 20.3 Diagnostics for one-way ANOVA", " Chapter 20 Using regression diagnostics 20.1 Introduction In the last chapter we learnt how to carry out regression diagnostics. Fortunately we don’t have to do all of that work each time we fit a model. R has a built in facility to make these plots for us. In this chapter we’ll walk through how to use this with regression and one-way ANOVA models. 20.2 Diagnostics for regression We’ll use the glucose release example from the Simple regression in R chapter to step through the easy way to make diagnostic plots. Walk through example You should work through the example in the next few sections. You’ll need to read in the GLUCOSE.CSV file again and fit the regression using the lm function as before. vicia_germ &lt;- read.csv(file = &quot;GLUCOSE.CSV&quot;) vicia_model &lt;- lm(Glucose ~ Temperature, data = vicia_germ) The built in facility for diagnostic plots in R works via a function called plot. For example, to produce a residuals vs fitted values plot, we use: plot(vicia_model, which = 1, add.smooth = FALSE) The first argument is the name of fitted model object. The second argument controls the output of the plot function: which = 1 argument tells it to produce a residuals vs. fitted values plot. The add.smooth = FALSE is telling the function not to add a line called a ‘loess smooth’. This line is supposed to help us pick out patterns, but it tends to over fit the relationship and leads people to see problems that aren’t there. If we can’t see a clear pattern without the line, it probably isn’t there, so it’s better not to include it at all. Remember that this plot allows us to evaluate the linearity assumption. Here, we’re looking for any evidence of a systematic trend (e.g. a hump or U-shaped curve) here. As there’s no obvious pattern we can accept the linearity assumption and move on to the normality assumption. We use the plot function to plot a normal probability diagnostic by setting which = 2: plot(vicia_model, which = 2) This produces essentially the same kind of normal probability plot we made in the previous chapter, with one small difference. Rather than drawing a 1:1 line, the ‘plot’ function shows us a line of best fit. This just allows us to pick out the curvature a little more easily. There’s nothing here to make us worry about this assumption - the points are close to the line with no systematic trend away from it. Finally, we can produce a scale-location diagnostic plot using the plot function to assess the constant variance assumption. Here we use which = 3 and again we suppress the line (using add.smooth = FALSE) to avoid finding spurious patterns: plot(vicia_model, add.smooth = FALSE, which = 3) Remember in this plot we’re looking for any sign of the size of the transformed residuals either increasing or decreasing as the fitted values get larger. This plot looks good - as the relationship here looks pretty flat we can conclude that the variability in the residuals is constant. Don’t panic if your diagnostics aren’t perfect! The good news about regression is that it is quite a robust technique. It will often give us reasonable answers even when the assumptions are not perfectly fulfilled. We should be aware of the assumptions but should not become too obsessed by them. If the violations are modest, it is often fine to proceed. We just need to interpret results with care. Of course, we have to know what constitutes a ‘modest’ violation. There are no hard and fast rules. The ability to make that judgement is something that comes with experience. 20.3 Diagnostics for one-way ANOVA The term ‘regression diagnostic’ is a bit of a misnomer. A more accurate term might be ‘linear model diagnostic’ but no one really uses this. Regression diagnostics can be used with many different kinds of models. In fact, the diagnostic plots we have introduced above can be applied to any model fitted by the lm function, including ANOVA models. Walk through example You should work through the example in the next few sections. Let’s step through the diagnostics for a one way ANOVA now, using the corncrake hatchlings example from the One-way ANOVA in R chapter. Read the data into R again and fit the model as before. corn_crake &lt;- read.csv(file = &quot;CORN_CRAKE.CSV&quot;) corncrake_model &lt;- lm(WeightGain ~ Supplement, data = corn_crake) The first diagnostic plot we produced for the regression model above is the residuals vs. fitted values plot. In a regression this is used to evaluate the linearity assumption. What does it do in a one-way ANOVA? Not much of use to be honest. There isn’t much point making a residuals vs. fitted values plot for a one-way ANOVA. Why? Because the residuals will never show a trend with respect to the ‘fitted values’, which are just the group-specific means. That’s one thing less to worry about. The normal probability plot is used to identify departures from normality. This plot allows us to check whether the deviations from the group means (the residuals) are likely to have been drawn from a normal distribution. Here’s the normal probability plot for our example: plot(corncrake_model, which = 2) This looks very good. The points don’t deviate from the line in a systematic way (except for a couple at the lower end—this is nothing to worry about) so it looks like the normality assumption is satisfied. The scale-location plot allows us to evaluate the constant variance assumption. This allows us to see whether or not the variability of the residuals is roughly constant within each group. Here’s the scale-location plot for the corn crake example: plot(corncrake_model, which = 3, add.smooth = FALSE) The warning sign we’re looking for here is a systematic pattern. We want to see if the magnitude of the residuals tends to increase or decrease with the fitted values. If such a pattern is apparent then it suggests that variance changes with the mean. There is no such pattern in the above plot so it looks like the constant variance assumption is satisfied. 20.3.1 Aside: formal test of equality of variance It is not critical that you learn the material in this short section. It is provided so that you know how to test for equality of variance. You won’t be asked to do this in an assessment. Looking back over the scale-location plot, it seems like three of the treatment groups exhibit similar variability, while the remaining two are more variable. They aren’t wildly different, so it is reasonable to assume the differences are due to sampling variation. People are sometimes uncomfortable using this sort of visual assessment. They want to see a p-value… A number of statistical tests have been designed to evaluate the equality of variance assumption. The most widely used is the Bartlett test (the bartlett.test function in R). Here is how to use it: bartlett.test(WeightGain ~ Supplement, data = corn_crake) ## ## Bartlett test of homogeneity of variances ## ## data: WeightGain by Supplement ## Bartlett&#39;s K-squared = 3.6444, df = 4, p-value = 0.4563 This looks just like the t-test specification. We use a ‘formula’ (WeightGain ~ Supplement) to specify the response variable (WeightGain) and the grouping variable (Supplement), and use the data argument to tell the bartlett.test function where to look for these variables. The null hypothesis of a Bartlett test is that the variances are equal, so a non-significant p-value (&gt;0.05) indicates that the data are consistent with the equal variance assumption. That’s what we find here. Generally speaking, we don’t recommend that you carry out a statistical test to evaluate the equality of variance assumption. We have shown it because some people seem to think they are needed. Here is why we think they are wrong: formal tests of equality of variance are not very powerful (in the statistical sense). This means that in order to detect a difference, we either need a lot of data, or the differences need to be so large that they would be easy to spot using a graphical approach. "],
["data-transformations.html", "Chapter 21 Data transformations 21.1 Data that violate ANOVA assumptions 21.2 Data transformation: ANOVAs and t-tests 21.3 Carrying on anyway 21.4 Transforming the data 21.5 Types of transformations 21.6 What about other kinds of models? 21.7 Final thoughts", " Chapter 21 Data transformations 21.1 Data that violate ANOVA assumptions Up until now, the data we’ve examined have conformed, at least roughly, to the assumptions of the statistical models we’ve been studying. This is all very handy, but perhaps a little unrealistic. The real world being the messy place it is, biological data often don’t conform to the distributional assumptions of t-tests and ANOVA: The residuals may not be normally distributed. Variances may be unequal among groups. The same kinds of problems with these distributional assumptions can also arise when working in a regression (i.e. non-normal residuals, non-constant variance). Furthermore, we might run into additional problems if there is some kind of non-linearity in the relationship between the response and predictor (numeric) variables. Most biological data are unlikely to conform perfectly to all the assumptions, but experience has shown (fortunately) that t-tests, ANOVAs and regressions are generally quite robust—they perform reasonably well with data that deviate to some extent from the assumptions of the tests. However, in some cases residuals are clearly very far from normal, or variances change a lot across groups. In these cases steps may need to be taken to deal with the problem. This chapter deals with one way of tackling the analysis of data that don’t fit the assumptions: data transformation. We will mostly focus on ANOVA / t-test setting, but keep in mind that the ideas are equally applicable to regression analysis. 21.2 Data transformation: ANOVAs and t-tests 21.2.1 The data: foraging in ants ants &lt;- read.csv(&quot;./data_csv/ANTS1.CSV&quot;) Red wood ants, Formica rufa, forage for food (mainly insects and ‘honeydew’ produced by aphids) both on the ground and in the canopies of trees. Rowan, oak and sycamore support very different communities of insect herbivores (including aphids) and it would be interesting to know whether the foraging efficiency of ant colonies is affected by the type of trees available to them. As part of an investigation of the foraging of Formica rufa observations were made of the prey being carried by ants down trunks of rowan, oak and sycamore trees. The total biomass of prey being transported was measured over a 30 minute sampling period and the data were expressed as the biomass (dry weight in mg) of prey divided by the total number of ants leaving the tree to give the rate of food collection per ant per half hour. Observations were made on 28 rowan, 26 sycamore, and 27 oak trees. Walk through Work through the ants example. The data are the file ANTS1.CSV. The Tree variable contains the tree identities and the Food variable contains the food collection rates: ants &lt;- read.csv(&quot;ANTS1.CSV&quot;) glimpse(ants) ## Observations: 81 ## Variables: 2 ## $ Food &lt;dbl&gt; 11.9, 33.3, 4.6, 5.5, 6.2, 11.0, 24.3, 20.7, 5.7, 12.6, 1... ## $ Tree &lt;fctr&gt; Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, ... Let’s examine the data. We could make a dot plot… ggplot(ants, aes(x = Food)) + geom_dotplot(binwidth = 6) + facet_wrap(~ Tree) …or we could construct a box and whiskers plot: ggplot(ants, aes(x = Tree, y = Food)) + geom_boxplot() It doesn’t matter which plot we use. They tell the same story. The food collection rate is generally highest in Oaks and lowest in Rowans (Sycamores are in between). Notice too that the sample distribution of food collection rate is right-skewed. The test we are most likely to want to use with these data is an ANOVA, i.e. we want to assess whether the mean food collection rates are different among the three tree species. Already we have an indication that an ANOVA with the raw food values may be problematic. 21.2.2 Fit the model and checking the assumptions This chapter is about fixing models when the assumptions are not satisfied. What assumptions do we need to check? The test we are most likely to want to use with these data is an ANOVA, so the following assumptions must be evaluated: Independence. The experimental units of the data must be independent. Measurement scale. The response variable is measured on an interval or ratio scale. Normality. The residuals are normally distributed in each level of the grouping factor. Equal variance. The variance in each level of the grouping factor is the same. We’ll have to assume the first assumption is satisfied and the food collection rate (second assumption) is obviously measured on a ratio scale. The distributional assumptions (normality and equality of variance) are the ones we can address with a transformation. Let’s fit the ANOVA model and produce regression diagnostics to evaluate these—remember, we make these kinds of plots after we have fitted a statistical model: ant_mod &lt;- lm(Food ~ Tree, data = ants) We need to produce a ‘normal probability plot’ to assess the normality assumption: plot(ant_mod, which = 2) This plot exhibits the accelerating curvature that is indicative of right-skewed residuals. This probably isn’t just sampling variation because there is a systematic departure from the dashed line everywhere along it. So… it looks like there is a problem. This sort of pattern is quite common in biological data, especially when it involves counts. Clearly we might be a bit worried about using an ANOVA with these data since it assumes the residuals to be at least approximately normally distributed. Are the variances significantly different? Look at the box plots above. The data from the three samples seem to have rather different scatter. The sample from the rowan has less variation than that from the sycamore, and the sycamore has less variation than the oak. Does the scale-location plot tell the same story? plot(ant_mod, which = 3) This shows that the variance increases with the fitted values—it looks like there is also a problem with the constant variance assumption. Again, this pattern is very common in biological data. In the light of these evaluations, we have three options … To carry on and carry out an ANOVA anyway—hoping that the violation of the assumptions won’t matter too much. To try and transform the data in some way to make it fit the assumptions better, then carry out an ANOVA. To use a different sort of test which doesn’t require the data to conform to these assumptions. Such tests are known as nonparametric tests. We will consider the first two options below, and return to the third in the Non-parametric tests chapter. 21.3 Carrying on anyway Carrying on and performing an analysis anyway doesn’t sound like a very good idea if we’ve already decided that the assumptions are suspect, but don’t dismiss it straight away. Mild departures from the assumptions often do not make a huge difference to the results of ANOVA (i.e. the p-values). At the very least, it can be instructive to carry out an analysis without ‘fixing’ the apparent problems so that we can get a sense of whether they matter or not. We already fitted the ANOVA model to allow us to make the diagnostic plots. All we have to do is pass the model object to the anova function to get the F-ratio and p-value for the tree effects: anova(ant_mod) ## Analysis of Variance Table ## ## Response: Food ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tree 2 3317.1 1658.53 8.0305 0.0006741 *** ## Residuals 78 16109.2 206.53 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on these results, it looks like there is a highly significant difference in food collection rates across the three tree species. We know the data are problematic though, so the question is, does this result stand up when we deal with these problems? t-tests are robust The ‘carry on anyway’ strategy can often be justified if we just need to compare the sample means of two groups because in this situation we can use a two-sample t-test rather than an ANOVA. By default R uses a version of the t-test that allows for unequal sample variances. This at least deals with one potential problem. The t-test is also fairly robust to violations of the normality assumption when the sample sizes are small, and when the sample sizes are large, the normality assumption matters even less. The ability to do a t-test which doesn’t require equal variances is extremely useful. A word of warning though: some people advise carrying out a statistical test of equal variance, and if the variances are deemed not to be significantly different, using the version of a two-sample t-test that assumes equal variances. This is not good advice. Following this procedure leads to less reliable p-values. The reason for this effect is somewhat technical, but trust us, this procedure is not good statistical practise. 21.4 Transforming the data One approach to dealing with difficult data is to apply a mathematical function to it to make the transformed data fits the model assumptions better: a process called data transformation. This may sound a bit dubious, but it is a perfectly valid procedure that will often allow us to use the statistical model we want to even if the data don’t initially fit the assumptions. The key thing to keep in mind is that the transformation should be applied to the response variable. 21.4.1 The logarithmic transformation Let’s try a simple transformation on the food collection rate variable in the ant data set. Instead of using the original numbers we will convert them to their logarithms. We can use common logs (logs to the base 10, written log\\(_{10}\\)) or natural logs (logs to the base, written log\\(_{e}\\) or ln). It doesn’t matter: they have exactly the same effect on the data in terms of making it meet the assumptions of ANOVA (or not). Applying a log transform is quick and easy in R—there are built in functions to take common logs and natural logs, called log10 and log, respectively. We’ll use mutate to add a new variable, which is the common log of Food: ants &lt;- mutate(ants, logFood = log10(Food)) We stored the transformed variable in a new column called logFood. ant_mod_log &lt;- lm(logFood ~ Tree, data = ants) We need to produce a ‘normal probability plot’ to assess the normality assumption: plot(ant_mod_log, which = 2) The accelerating curvature (indicative of right-skewed residuals) has gone. The new normal probability plot is a bit better than before as now perhaps 60% of the cases are on the dashed line. It’s hardly perfect though—the tails of the distribution are not where we’d like them to be. What about the variances? plot(ant_mod_log, which = 3) The scale location-plot indicates that the constant variance assumption is now OK, i.e. the variance no longer increases with the fitted values. It looks like the log transformation seems to to have improved things quite a lot, but the diagnostics are still not perfect. The assumptions are closer to being satisfied. Let’s carry out ANOVA again using the model with the transformed food variable to see how the results change: anova(ant_mod_log) ## Analysis of Variance Table ## ## Response: logFood ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tree 2 1.4106 0.70530 7.2867 0.001255 ** ## Residuals 78 7.5498 0.09679 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What has happened? We still see evidence for a significant effect of tree (p&lt;0.01) with the transformed data, but the p-value is somewhat bigger than when we used the original data. This illustrates why it is important to evaluate assumptions, and to deal with them when they are obviously violated—the output of a statistical test is affected. It did not matter too much here, but in other settings we can end up with misleading or downright spurious results if we ignore problems with the assumptions of a statistical model. Values of 0 One thing to be aware of is that we cannot take the log of zero. If our data contain zeros we have add a small value (conventionally 1) to the variable before taking the logs (so the transformation is actually log(x + 1)). It is worth tying this —it’s easy to do and occasionally works well—but don’t be surprised if this transformation results in a model with poor diagnostics. The log(x + 1) transformation often doesn’t work when there are many zeros. Sadly, dealing with ‘non-normal’ data containing many zeros is tricky. 21.4.2 Presenting results from analyses of transformed data Having compared the transformed means, how should we present the results in a report? There are three alternatives. We’ll illustrate them using the log-transformation, but they applicable to other kinds of transformations. We could present the transformed means (having stated what the transformation was, e.g. \\(log_{e}(x+1)\\)). The disadvantage to this is that the numbers themselves convey little information about the data values on the original scale. This isn’t always a problem. For example, effects given on a log scale act in a ‘multiplicative’ manner, so a model with log-transformed response variable can still be interpreted if we know what we’re doing. We could back-transform the means of the log-transformed data by taking the antilogs: \\(10^{x}\\) (for logs to the base 10) and \\(e^{x}\\) (for natural logs)14. When we back-transform data, however, we need to be aware of two things: (1) The back-transformed mean will not be the same as a mean calculated from the original data; (2) We have to be careful when we back-transform standard errors. If we want to display the back-transformed means on a bar plot, with some indication of the variability of the data, we must calculate the standard errors and then back transform the upper and lower limits, which will not then be symmetrical about the mean. We could also present the means calculated from the original data but state clearly that the statistical analysis was carried out on transformed data. This is often the simplest way to proceed. 21.5 Types of transformations Clearly, in the case study above, a log-transformation alters the outcome of statistical tests applied to the data. It is not always the case that transforming the data will make the difference between a result being significant and not significant, or that the transformed data will give a less significant result. Never use p-values to judge the success of a transformation! We use diagnostic plots to make that assessment. What we hope is that we can transform the response variable so that it conforms, at least approximately, to the assumptions of the statistical model we want to use, making the result from associated tests as reliable as possible. Taking logarithms is only one of many possible transformations. Each type of transformation is appropriate to solving different problems in the data. The following is a summary of the three most commonly used transformations and the sort of situations they are useful for. 21.5.1 Logarithms Log transformation, as we’ve just seen, affects the data in two ways: A log-transformation stretches out the left hand side (smaller values) of the distribution and squashes in the right hand side (larger values). This is obviously useful where the data set has a long tail to the right as in the example above. The ‘squashing’ effect of a log-transformation is more pronounced at higher values. This means a log-transformation may also deal with another common problem in biological data (also seen in the ant data)—samples with larger means having larger variances. If we are carrying out an ANOVA and the scale-location plot exhibits a positive relationship—i.e. groups with larger means have larger variances—then a log transformation could be appropriate15. 21.5.2 Square roots Taking the square root of the data is often appropriate where the data are whole number counts (though the log transformation may also work here). This typically occurs where your data are counts of organisms (e.g. algal cells in fields of view under a microscope). The corresponding back-transformation is obviously \\(x^{2}\\). In R the square root of a set of data can be taken using the sqrt function. However, note that there is no square function in the list. Taking squares is done using the ^ operator with the number 2 on the right (e.g. if the variable is called x, use x^2). 21.5.3 Arcsine square root This transformation is generally used where the data are in the form of percentages or proportions. It can be shown in theory (even if not from the data you actually have) that such data are unlikely to be normally distributed. A correction for this effect is to take the inverse sine of the square roots of the original data, i.e. \\(\\arcsin \\sqrt{x}\\). Converting percentages to proportions Although this transformation is usually discussed in the context of percentages we cannot actually take the arcsine of numbers larger than 1. Obviously percentages range between 0 and 100. To get round this, when dealing with percentage data, simply express the percentages as proportions (e.g. 100% = 1, 20% = 0.2, 2% = 0.02, etc.) before doing the transformation. In R the transformation can be achieved by combining the sqrt and asin functions inside mutate. For example, if we need to transform a proportion stored in the x variable use something like… mydata &lt;- mutate(mydata, assqrt.x = asin(sqrt(x))) …where mydata is the name of hypothetical data frame containing the data. Just remember to apply sqrt and asin in the correct order. We used nested functions here, which are applied from the inside to the outside. 21.5.4 Squaring In addition to the standard transformations above there are a variety of others less commonly used. One problem which the above transformations don’t deal with is when data have a negative skew (i.e. a long tail to the left). This problem can sometimes be dealt with, or at least reduced, by squaring the data values. In R the transformation can be achieved by combining the ^ operator inside mutate. For example, we might use something like… mydata &lt;- mutate(mydata, sqr.x = x^2) …where mydata is again the name of hypothetical data frame containing the data. 21.5.5 Situations which cannot be dealt with by transformations There are some situations where no amount of transformation of the data will get round the fact that the data are problematic. Three in particular are worth noting… Multimodal distributions: these may in fact have only one actual mode, but nonetheless have two or more clear ‘peaks’ (N.B. not to be confused with distributions that are ‘spiky’ just because there are few data). Dissimilar distribution shapes: if the two (or more) samples have very different problems, e.g. one is strongly right-skewed and the other strongly left-skewed then no single transformation will be able to help—whatever corrects one sample will distort the other. Samples with many exactly equal values: with results that are small integer numbers (e.g. counts of numbers of eggs in birds’ nests) then there will be many identical values. If the non-normality results from lots of identical values forming a particular peak, for example, this cannot be corrected by transformation since equal values will still be equal even when transformed. Is it ‘fiddling’ the data? Changing the data by transformation can alter the results of a statistical test—so isn’t this a bit dodgy? The key thing here is to realise that the scales on which we measure things are, to some extent, arbitrary. Transforming data to a different scale replaces one arbitrary scale with another. The transformations we have discussed don’t alter the ordering of data points relative to each other—they only alter the size of the gaps between them. In some cases this rescaling can make the data more amenable to study, analysis or interpretation. In fact we often use data transformations, perhaps without realising it, in many situations other than doing statistical tests. For example, when we look at a set of pH readings we are already working with data on a log scale because pH units (0-14) are actually the negative logarithms of the hydrogen ion concentration in the water. Similarly measurements of noise in decibels (dB), and seismic disturbance on the Richter scale, are actually logarithmic scales. In fact, there are subtle ways in which using a transformation can affect what aspect of the biological system it is our measurements are characterising but this is an issue beyond the scope of this course. We’ll already mentioned one example: a logarithmic transformation turns a ‘multiplicative’ process into an ‘additive’ one. One final comment… Obviously we have to apply the same transformation to all the data, e.g. we can’t log transform the observations in one group and leave the other alone—that really would be cheating! 21.6 What about other kinds of models? We have focussed on ANOVA here for the simple reason that the assumptions are a bit easier to evaluate compared to regression. However, exactly the same ideas apply when working with other kinds of models that lm can fit. The workflow is the same in every case: Always check the diagnostic plots after we fit a regression or ANOVA (and do this before worrying about p-values). If there is evidence for a problem with the assumptions, try transforming the response variable. Refit the model using the transformed variable and generate new diagnostic plots to see if the new model is any better. Finally, keep in mind that this is often an iterative process. We might have to go through several rounds of transforming and model checking before we arrive at a good model. 21.7 Final thoughts Evaluating assumptions and picking transformations is as much ‘art’ as science. It takes time and experience to learn how to do it. R makes it very easy to try out different options, so don’t be afraid to do this. Frequently, with real biological data, no straightforward transformation really improves the form of the data or in correcting one problem you generate another. Fortunately, in many cases where the assumptions of a test are not reasonably well fulfilled, there is an alternative approach— which we will discuss in the Non-parametric tests chapter. N.B. if we used log\\((x+1)\\) we must remember to subtract the 1 again after taking the antilog.↩ Log transformations have a variety of other uses in statistics. One is in transforming a predictor variable when looking at the form of relationships between two variables.↩ "],
["multiple-comparison-tests.html", "Chapter 22 Multiple comparison tests 22.1 Introduction 22.2 Tukey’s HSD in R 22.3 How to summarise multiple-comparison results 22.4 Doing it the easy way… 22.5 Summarising and presenting the results of a Tukey test 22.6 Significant ANOVA but no differences in a Tukey test?", " Chapter 22 Multiple comparison tests 22.1 Introduction In the One-way ANOVA in R chapter we learned how to use ANOVA to examine the global hypothesis of no difference between means—we did not learn how to evaluate which means might be driving such a significant result. For example, we found evidence for a significant difference between the means in the corn crake example, but we were not able to say which supplements are better or worse, and did not make any statements about which supplements differ significantly from each other. The purpose of this chapter is to examine one method for assessing where the differences actually lie. The general method we will use is called a post hoc multiple comparisons test. The phrase ‘post hoc’ refers to the fact that these tests are conducted without any particular prior comparisons in mind. The words ‘multiple comparisons’ refer to the fact that they consider many different pairwise comparisons. There are quite a few multiple comparison tests—Scheffé’s test, the Student-Newman-Keuls test, Duncan’s new multiple range test, Dunnett’s test, … (the list goes on and on). Each one is applicable to particular circumstances, and none is universally accepted as ‘the best’. We are going to work with the most widely used test: the Tukey multiple comparison test. This test is also known as Tukey’s Honestly Significant Difference (Tukey HSD) test16. People tend to favour Tukey’s HSD test because it is ‘conservative’: the test has a low false positive rate compared to the alernatives. A false positive occurs when a test turns up a statistically significant result for an effect that is not really there. A low false positive rate is a good thing. It means that if we find a significant difference we can be more confident it is ‘real’. The cost of using the Tukey HSD test is that it isn’t as powerful as alternatives: the test turns up a lot of false negatives. A false negative occurs when a test fails to produce a statistically significant result for an effect when it is really present. A test with a high false negative rate tends to miss effects. There is one line of thinking that says post hoc multiple comparisons tests of any kind should never be undertaken. We shouldn’t carry out an experiment without a prior prediction of what will happen—we should know which comparisons need to be made and should only undertake those particular comparisons rather than making every possible comparison. Nonetheless, post hoc multiple comparisons test are easy to apply and widely used, so there is value in knowing how to use them. The Tukey HSD test at least tends to guard against picking up non-existent effects. 22.2 Tukey’s HSD in R Walk through example We are going to work with the corn crake example again. If you kept the CORN_CRAKE.CSV file from before all you need to do is make sure your working directory is set to this location. Otherwise you’ll need to download it again. Let’s work through the corn crake example again. Read the data into R we and fit an ANOVA model using the lm function. # 1. read in data corn_crake &lt;- read.csv(file = &quot;CORN_CRAKE.CSV&quot;) # 2. fit anova model corncrake_model &lt;- lm(WeightGain ~ Supplement, data = corn_crake) We stored the model object in corncrake_model. In the One-way ANOVA in R chapter we saw how to evaluate whether the means differ using anova on this object. We use a couple of new functions to carry out a Tukey HSD test. First, we have to convert the linear model object into a different kind of model object using the aov function: corncrake_aov &lt;- aov(corncrake_model) We don’t really need to understand what this is doing—aov prepares the model so that we can perform a Tukey HSD test. Notice that we gave the new object its own name (corncrake_aov) because we need to use it in the next step. It is easy to perform a Tukey HSD test once we have the ‘aov’ version of our model. There are a few different options. Here is how to do this using the TukeyHSD function: TukeyHSD(corncrake_aov, ordered = TRUE) Pay attention! We applied the TukeyHSD function to the ‘aov’ object, not the original lm object. We have suppressed the output for now. Before we review it we need to get an idea of what it is going to show us. The ordered = TRUE tells TukeyHSD that we want to order the treatment means from smallest to largest, and then apply every pairwise comparison, starting with the smallest mean (‘Allvit’) and working up through the order. Here are the means ordered from smallest to largest, working left to right: Supplement Allvit None Linseed Sizefast Earlybird Mean 14.3 15.6 18.5 19.0 22.9 So the TukeyHSD with ordered = TRUE will first compare ‘Allvit’ to ‘None’, then ‘Allvit’ to ‘Linseed’, then ‘Allvit’ to ‘Sizefast’, then ‘Allvit’ to ‘Earlybird’, then ‘None’ to ‘Linseed’, then ‘None’ to ‘Sizefast’, … and so on, until we get to ‘Sizefast’ vs. ‘Earlybird’. Let’s look at the output: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = corncrake_model) ## ## $Supplement ## diff lwr upr p adj ## None-Allvit 1.375 -4.627565 7.377565 0.9638453 ## Linseed-Allvit 4.250 -1.752565 10.252565 0.2707790 ## Sizefast-Allvit 4.750 -1.252565 10.752565 0.1771593 ## Earlybird-Allvit 8.625 2.622435 14.627565 0.0018764 ## Linseed-None 2.875 -3.127565 8.877565 0.6459410 ## Sizefast-None 3.375 -2.627565 9.377565 0.4971994 ## Earlybird-None 7.250 1.247435 13.252565 0.0113786 ## Sizefast-Linseed 0.500 -5.502565 6.502565 0.9992352 ## Earlybird-Linseed 4.375 -1.627565 10.377565 0.2447264 ## Earlybird-Sizefast 3.875 -2.127565 9.877565 0.3592201 This table look confusing at first glance. It enables you to look up every pair of treatments, and see whether they are significantly different from each other. Lets see how this works… The first four lines compare the Allvit treatment (‘Allvit’) with each of the other treatments in turn: ## diff lwr upr p adj ## None-Allvit 1.375 -4.627565 7.377565 0.9638453 ## Linseed-Allvit 4.250 -1.752565 10.252565 0.2707790 ## Sizefast-Allvit 4.750 -1.252565 10.752565 0.1771593 ## Earlybird-Allvit 8.625 2.622435 14.627565 0.0018764 So to look up the difference between the control treatment and the ‘Allvit’ treatment, we read the first results row in the table. This says the means differ by 1.375, the confidence interval associated with this difference is [-4.63, 7.38], and that the comparison has a p-value of 0.96. So in this case we would conclude that there was a no significant difference between the control treatment and the ‘Allvit’ treatment. We could look up any comparison of the ‘Allvit’ treatment with a different treatment in the next three lines of this portion of the table. This basic logic extends to the rest of the table. If we want to know whether the ‘Earlybird’ treatment is different from the control, we look up the ‘Earlybird-None’ row: ## diff lwr upr p adj ## Earlybird-None 7.250 1.247435 13.252565 0.0113786 It looks like the means of the ‘Earlybird’ and ‘None’ levels are significantly different at the p &lt; 0.05 level. Now we know how to look up any set of comparisons we need to see whether the difference is significant. The next question is: How should we summarise such a table? 22.3 How to summarise multiple-comparison results Summarising the results of multiple comparison tests can be a tricky business. The first rule is: don’t present the results like the TukeyHSD function does! A clear summary of the results will help us to interpret them correctly and makes it easier to explain them to others. How should we do this? Let’s list the means in order from smallest to largest again: Supplement Allvit None Linseed Sizefast Earlybird Mean 14.3 15.6 18.5 19.0 22.9 Now, using the table of Tukey results we can perform a sequence of pair-wise comparisons between the supplements starting with the smallest pair… ‘Allvit’ and ‘None’. The appropriate test is in the first table: ## diff lwr upr p adj ## 1.3750000 -4.6275646 7.3775646 0.9638453 The last column gives the p-value, which in this case is certainly not significant (it is much greater than 0.05), so we conclude there is no difference between the ‘Allvit’ and ‘None’ treatments. So now we continue with ‘Allvit’, but compare it to the next larger mean (‘Linseed’). In this case the values are: ## diff lwr upr p adj ## 4.250000 -1.752565 10.252565 0.270779 The last column gives the p-value, which again is not significant, so we conclude there is no difference between the ‘Allvit’ and ‘Linseed’ treatments. So now we continue with ‘Allvit’, but compare it to the next larger mean (‘Sizefast’). In this case the values are: ## diff lwr upr p adj ## 4.7500000 -1.2525646 10.7525646 0.1771593 Once again, this difference is not significant, so we conclude there is no difference between the ‘Allvit’ and ‘Sizefast’ treatments either. So again, we continue with ‘Allvit’, which we compare to the next larger mean (‘Earlybird’). ## diff lwr upr p adj ## 8.6250000 2.6224354 14.6275646 0.0018764 This time the p-value is clearly less than 0.05 so we conclude that this pair of treatments are significantly different. We record this by marking ‘Allvit’, ‘None’, ‘Linseed’ and ‘Sizefast’ to indicate that they don’t differ from each other. We’ll use the letter ‘b’ to do this. Supplement Allvit None Linseed Sizefast Earlybird Mean 14.3 15.6 18.5 19.0 22.9 b b b b The ‘b’ defines a group of treatment means—‘Allvit’, ‘None’, ‘Linseed’ and ‘Sizefast’—that are not significantly different from one another. It doesn’t matter which letter we use by the way (the reason for using ‘b’ here will become apparent in a moment). The means are ordered from smallest to largest which means we can forget about ‘None’, ‘Linseed’ and ‘Sizefast’ treatments for a moment—if they are not significantly different from ‘Allvit’ they can’t be significantly different from one another. We move on to ‘Earlybird’, but now, we work back down the treatments to see if we can define another overlapping group of means that are not significantly different from one another. When we do this, we find that ‘Earlybird’ is not significantly different from ‘Linseed’ and ‘Sizefast’, but that it is significantly different from ‘None’. This forms a second ‘not significantly different’ group. We will denote this with a new letter (‘a’) in our table: Supplement Allvit None Linseed Sizefast Earlybird Mean 14.3 15.6 18.5 19.0 22.9 b b b b a a a If there were additional treatments with a mean that was greater than ‘Earlybird’ we would have to carry on this process, working back up from ‘Earlybird’. Thankfully, there are no more treatments, so we are finished. This leaves us with a concise and complete summary of where the differences between treatments are, which greatly simplifies the task of interpreting the results. Treatments that share a letter form groups within which means are not different from each other. Treatments that are not linked are significantly different. 22.4 Doing it the easy way… The results table we just produced is concise and complete, but no reasonable person would say they were easy to arrive at. As you might expect, someone has written an R function to do this for us. It isn’t part of ‘base R’ though, so we have to install a package to use it. The package we need is called agricolae: install.packages(&quot;agricolae&quot;) Once this has been installed we use the library function to tell R that we want to use the package in the current session: library(&quot;agricolae&quot;) Now that the package is loaded we can carry out the Tukey HSD test and find the ‘not significantly different’ groups using the HSD.test function: HSD.test(corncrake_aov, &quot;Supplement&quot;, console=TRUE) ## ## Study: corncrake_aov ~ &quot;Supplement&quot; ## ## HSD Test for WeightGain ## ## Mean Square Error: 17.43571 ## ## Supplement, means ## ## WeightGain std r Min Max ## Allvit 14.250 5.599745 8 5 22 ## Earlybird 22.875 3.482097 8 17 27 ## Linseed 18.500 3.023716 8 15 24 ## None 15.625 3.420004 8 10 20 ## Sizefast 19.000 4.780914 8 10 25 ## ## Alpha: 0.05 ; DF Error: 35 ## Critical Value of Studentized Range: 4.065949 ## ## Minimun Significant Difference: 6.002565 ## ## Treatments with the same letter are not significantly different. ## ## WeightGain groups ## Earlybird 22.875 a ## Sizefast 19.000 ab ## Linseed 18.500 ab ## None 15.625 b ## Allvit 14.250 b The console = TRUE argument tells the function to print the results for us. That’s a lot of output, but we can ignore most of it. The part that matters most is the table at the very end. This shows the group identities as letters, the treatment names, and the treatment means. If we compare that table with the one we just made, we can see they convey the same information. The package labels each group with a letter. For example, we can see that ‘Linseed’ and ‘SizeFast’ are both members of the ‘a’ and ‘b’ group. Hopefully it is obvious why we used ‘b’ and then ‘a’ when building the table manually above. So, there is no need to build these Tukey HSD tables by hand. We just use the HSD.test function in the agricolae package. So why did we do it the long way? Well, the usual reasoning applies: it is important to know how to do this because it improves our understanding of what the ‘letters notation’ actually means. 22.5 Summarising and presenting the results of a Tukey test As with any statistical test it will usually be necessary to summarise the result from the Tukey HSD test in a written form. With an ANOVA test of global significance and multiple comparisons of the means, this can become quite complex and hard to follow. In most cases it is best to summarise the ANOVA results and either the main differences between means, or concentrate on those comparisons which relate to the original hypothesis we were interested in, and then refer to a table or figure for the additional detail. For example… There was a significant effect of supplement on the weight gain of hatchlings (ANOVA: F=5.1; d.f.= 4,35; p&lt;0.01) (Figure 1). The only supplement that led to a significantly higher rate of weight gain than the control group was Earlybird (Tukey multiple comparisons test, p &lt; 0.05). When it comes to presenting the results in a report, we really need some way of presenting the means, and the results of the multiple comparison test, as the statement above cannot entirely capture the form of the results. The information can often be conveniently incorporated into a table or figure, using more or less the same format as the output from the HSD.test function in the agricolae package. An example table might be: Supplement Mean weight gain (g) Earlybird 22.9a Sizefast 19.0ab Linseed 18.5ab None 15.6b Allvit 14.3b Note that, however we present it, we need to provide some explanation saying: (a) what test we did, (b) what the letter codes mean, and (c) the critical threshold we used to judge significance. In this case the information could be presented in a table legend: Table 1: Mean weight gain of hatchlings in the five supplement treatments. Means followed by the same letter did not differ significantly (Tukey test, p&gt;0.05). Letter coding can also be used effectively in a figure. Again, we must ensure all the relevant information is given in the figure legend. 22.6 Significant ANOVA but no differences in a Tukey test? It is possible to get a significant result from the global ANOVA test but find no significant differences in a Tukey test, though it doesn’t happen often. ANOVA and the Tukey HSD test (or indeed other multiple comparison tests) are different tests, with different null hypotheses. Because of this it is possible to end up with a significant result from ANOVA, indicating at least one difference between means, but fail to get any differences detected by the Tukey test. This usually happens when the ANOVA result is marginal (close to p = 0.05). If it happens then aside from running the experiment again with more replication, there isn’t much we can do except make the best interpretation we can from inspecting the data, and be suitably cautious in the conclusions we draw17. If in doubt seek some expert advice! N.B. Try to avoid a common mistake: it is Tukey, after its originator, the statistician Prof. John Tukey, not Turkey, a large domesticated bird which has made no useful contributions to statistical theory or practice.↩ It might be tempting to run a new post hoc analysis using a different kind of test. Don’t do this. It is a terrible strategy for doing statistics because this kind of practise is guaranteed to increase the overall false positive rate.↩ "],
["principles-of-experimental-design.html", "Chapter 23 Principles of experimental design 23.1 Introduction 23.2 Jargon busting 23.3 Replication 23.4 Controls 23.5 Confounded and noisy experiments 23.6 Dealing with confounding effects and noise 23.7 Ethics and practicality 23.8 Further reading", " Chapter 23 Principles of experimental design Hiawatha to convince them, Organised a shooting contest. Laid out in the proper manner Of designs experimental Recommended in the textbooks Mainly used for tasting tea (but sometimes used in other cases) Still they couldn’t understand it, so they couldn’t raise objections. (Which is what so often happens with analysis of variance.) Maurice Kendal (after Longfellow) from Hiawatha Designs an Experiment 23.1 Introduction The data we use to test hypotheses may be generated by recording information from natural systems (‘observational studies’), or by carrying out some sort of experiment in which the system under study is manipulated in some way (‘experimental studies’). There is often considerable scope for deliberately arranging the system to generate data in the best way to test a particular effect when conducting experiments. For this reason we tend to use the term ‘design’ primarily in the context of experiments. However, collection of data in both situations requires thought and planning, and many of the considerations of what is termed experimental design apply equally to observational and experimental studies18. The underlying principle of experimental design is: to extract data from a system in such a way that differences or variation in the data can be unambiguously attributed to the particular process we are investigating. In order to do this we need to know how to maximise the statistical power of an experiment or data collection protocol. Statistical power is the likelihood that a study will detect an effect when there really is an effect present. Broadly speaking, statistical power is influenced by: (1) the size of the effect and (2) the size of the sample used to detect it. Bigger effects are easier to detect than smaller effects, while large samples present greater test sensitivity than small samples. A second consideration is that the less variable the material we are using the smaller the effects we will be able to detect. Given these facts, there are obviously two things to do when designing an experiment: Use the maximum feasible sample sizes. Take steps to control the variability of the organisms/material being used; of the experimental conditions; and of the methods of measurement. Exactly what combination of these is appropriate will depend on the subject area. In a physiological experiment using complex apparatus and monitoring equipment the scope for replication may be very limited. Obviously here maximum effort should be put into experimentally controlling extraneous sources of variation. With the subject material this may mean using animals of the same age, reared under the same conditions, of the same stock; it may involve using clones of plant material. It will involve running the experiment under controlled conditions of light, temperature, and require that the measurement methods are as precise as possible. On the other hand an ecologist studying an organism in the field may have relatively little scope for experimental control of either the material studied or the environmental conditions, and may be forced to make relatively crude measurements. In this case the best approach is to control what can be controlled and then try and maximise the sample size. 23.2 Jargon busting Before we delve any further into experimental design concepts we need to introduce a little bit of statistical jargon. We’ll define the important terms, and then run through an example to better understand them: An experimental unit is the physical entity which can be assigned, at random, to a treatment (see next definition). Examples of possible experimental units are individual animals, plots or quadrats, populations, or even whole communities. A treatment is any kind of manipulation applied to experimental units. A group of experimental units that all receive the same treatment is called a treatment group. Most experiments include one or more complementary groups, called control groups. The experimental units in a control group receive either no treatment or some kind of standard treatment. A factor is just a collection of related treatments and controls, and the different treatments/controls are called the levels of that factor. An experimental factor can be viewed as a variable whose levels are set by the experimenter. Here’s an example. Suppose we wanted to compare the weight loss of people on 4 different diets to determine which diet is the most effective for losing weight. We conduct an experiment in which groups of 8 volunteers follow each of the diets for one month. A fifth group of 8 volunteers serves as the control group—they follow NHS diet guidelines. At the end of the experiment we measure how much weight each person has lost over the month. In this example the volunteers are the experimental units, diets are the treatments, and the NHS group are the control group. Together, the four ‘diet types’ and the ‘no diet’ (i.e. normal eating) control constitute the five levels of the ‘weight loss group’ factor. A word of warning: it is common to lump control groups and treatment groups together and just call them ‘treatments’. This is fine, but be aware of the distinction between the two. 23.3 Replication We cannot do any statistics without understanding the idea of replication—assigning several experimental units to the same treatment or combination of treatments. Why does replication matter? Replication affects the power of a statistical test—by increasingly the replication in a study we increase the sample size available to detect specific effects. Replication is fundamental to many of the statistical methods we use, and is particularly important in biology because the material we work with is often inherently variable and hard to make precise measurements on. It seems like a simple idea: increased replication = more statistical power. We have to be careful about how we replicate though… 23.3.1 Independence and pseudoreplication An assumption of most statistical tests is that the data are independent. Independence means that the value of a measurement from one object is not affected by the values of other objects. Common sources of non-independence in biology include: genetics - e.g. if a set of mice are taken from the litter of a single female, they are more likely to be similar to each other than mice taken from the litters of several different females. geography - e.g. samples from sites close together will experience similar microclimate, have similar soil type etc. sampling within biological ‘units’ - e.g. leaves on a tree will be more similar to each other than to leaves from other trees. experimental arrangements in the lab - e.g. plants grown together in a pot, or fish kept in one aquarium will all be affected by the conditions in that pot/aquarium. Non-independence occurs at many levels in biological data, and in statistical testing the common consequence of non-independence is pseudoreplication. Pseudoreplication is an artificial increase in the sample size (and hence degrees of freedom) caused by using non-independent data. It may be easiest to see what this means by example. Imagine we are interested in whether plants of a particular species produce flowers with different numbers of petals when grown in two different soil types. We have three plants in each soil type and each plant produces 4 flowers. As it turns out, the 4 flowers within each individual plant have identical numbers of petals. If we count the petals in a single flower from each plant, and then test the difference using a t-test we get the following result: Soil type Num. Petals Mean (Plant 1) (Plant 2) (Plant 3) Soil type A 3 4 5 4 (Plant 1) (Plant 2) (Plant 3) Soil type B 4 5 6 5 p = 0.29 The difference is not significant. Now instead of sampling a single flower from each plant we count the petals of all four flowers on each plant and (incorrectly) use all the values in the analysis (giving an apparent sample size of 12 in each treatment): Soil type Num. Petals Mean (Plant 1) (Plant 2) (Plant 3) Soil type A 3, 2, 3, 4 4, 4, 3, 5 3, 6, 7, 4 4 (Plant 1) (Plant 2) (Plant 3) Soil type B 4, 5, 4, 3 5, 7, 3, 5 6, 5, 7, 6 5 p = 0.009 Even with that proviso that the data might be a bit suspect with regard to normality, the same difference in the means now appears to be highly significant! The problem here is that the flowers within each plant are not independent - there is variation among plants in petal numbers, but within the plant (perhaps for genetic reasons) the number of petals produced are similar. Because of this non-independence the apparent significance in the final result is spurious. There are only three independent entities in each soil type treatment—the plants—so the first of the two tests here is correct, the second is pseudoreplicated. To illustrate the effect in a still more obvious way, consider if we were interested in the heights of plants in the two soil types, but we actually only had one plant in Soil A and one in Soil B. If we measure the plants and find they differ somewhat in height, we cannot tell whether this is due to the soil, or just because no two plants are identical. With one plant in each soil we cannot carry out a statistical test to compare the heights. Now, if it was suggested that we measure the height of each plant 20 times and then used those numbers to do a statistical test to compare the plant heights in the two soils we would realise that this was an entirely pointless exercise. There is no more information about the effect of soil type in the two sets of 20 measurements than there was in the single measurement (except we now know how variable our measuring technique is). And why stop at 20? Why not just keep remeasuring until we have enough numbers to get a significant difference?! Clearly this is nonsense. Put this way, the pitfall of pseudoreplication seems obvious. However, it can creep into biological studies in quite subtle ways and occurs in a significant number of published studies. One very common problem occurs in ecological studies where different habitats, or experimental plots, are being compared. Say we are looking at zooplankton abundance in two lakes, one with fish and one without. We would normally take a number of samples from each lake and could obviously compare the zooplankton numbers between these two sets of samples. It would be tempting to attribute any differences we observe to the effect of fish. However this would not be correct. We have measured the difference in zooplankton between the two lakes (and this is quite a valid thing to do) but the lakes may differ in any number of ways, not just the presence of fish, so it is not correct to interpret our result in relation to the effect of fish. To do this, we would really need data on zooplankton abundance in several lakes with fish, and several without. In other words, for testing the effect of fish, our replicates should be whole lakes with and without the relevant factor (fish), not samples from within a single lake. But surely it is still better to take lots of samples from each site than just one; it must give a more accurate picture? This is true. Taking several measurements or samples from each object guards against the possibility of the results being influenced by a single, possibly unusual, sample; so the accuracy of the information about the object is increased. It would be much more reliable to have twenty zooplankton samples from a lake than just one. This is important, but it is not the same as having measurements from more objects (lakes)—true replication—which increases the power of the statistical test to detect differences among objects with respect to the particular factor (e.g. fish / no fish) we are interested in. So in cases such as those above, the best strategy would be to measure petal number on all the flowers on each plant, but then calculate a mean for each plant and use those means in the statistical test. The same idea applies in the lake situation—several plankton samples could be taken from each of a number of lakes, then combined to give one estimate of plankton density for each lake. Though of course, we couldn’t do much in the way of statistical analysis on the two means. So, in summary, when carrying out an investigation the key question to ask is: What is the biological unit of replication relevant to the effect we trying to test? As this implies, the appropriate unit of replication may vary depending on what we are investigating. If we want to test for a difference in the plankton density between two lakes, then taking 10 samples from each lake and comparing them would be the correct approach. But if, as above, we wanted to assess the effect of fish on plankton density, it would be inappropriate—the correct unit of replication in this case is the whole lake and we would therefore want to sample several lakes with and without fish. 23.4 Controls We are told repeatedly, probably starting at primary school, that every experiment must have a control—a reference treatment against which the other treatments can be compared. The idea does, however, sometimes generate confusion since it is not always clear what is being controlled for, and some experiments do not require a control while others require more than one. In some cases the appropriate control is obvious. In a toxicity test we are interested in the mortality due to the toxicant, and clearly we want the control to tell us what the background mortality rate (without toxicant) would be under those experimental conditions. However, if we are measuring the movement rates of slugs on surfaces of differing moisture content there is no control required — indeed none possible. Slugs encounter many different moisture conditions in their daily lives and there isn’t a ‘control’ moisture level. So the first message is that there may not be a control for all experiments. More tricky is the situation where the objects we are investigating are affected not just by the treatment we are administering, but also by other effects of applying that treatment. This too can sometimes be addressed by the use of control treatments, but these are now not simply the ‘natural’ situation, they may have to be quite specifically designed to mimic certain aspects of the experiment, and not others. These sorts of controls are discussed in more detail below. 23.5 Confounded and noisy experiments Unwanted variation comes in two forms. The first is confounding variation. This occurs when there are one or more other sources of variation that work in parallel to the factor we are investigating and make it hard, or impossible, to unambiguously attribute any effects we see to a single cause. Confounding variation is particularly problematic in observational studies because, by definition, we don’t manipulate the factors we’re interested in. The second is noise. This describes variation that is unrelated to the factor we are investigating but adds variability to the results so that it is harder to see, and detect statistically, any effect of that factor. As noted above, much of experimental design is about improving our ability to account for noise in a statistical analysis. We will consider these together, as some of the techniques for dealing with them are be applicable to both. 23.5.1 Confounding The potential for confounding effects may sometimes be easy to recognise. If we measure growth rates in plants growing at sites of differing altitude, there are several factors which all change systematically with altitude (temperature, ultraviolet radiation, precipitation, wind speed etc.) and it may be hard to use such data to examine effects of any one of these factors alone. The important thing to remember is that observing a relationship between two variables (e.g. a negative relationship between plant growth and increased precipitation up a mountain) does not necessarily indicate a causal link (plant growth may be determined by one or more of the other factors that vary with altitude). Confounding effects can also be much more subtle. We may find that eagle owls take more large Norway rats at a particular time of year—but the factor we are interested in (rat size) is related to sex (males are larger) and the males spend more time moving around (hence out of cover and exposed to predation) at that time of year. So what seems to be a size effect, may actually be produced by sex-specific behaviour and not due to eagle owls selecting larger prey at all. Confounding doesn’t just occur in observational studies. Confounding occurs when administration of a treatment itself generates other unwanted effects where the treatment is applied. An example might be in the administration of nutrients to plants. Changing the supply of nitrogen may be done by supplying different levels of a nitrate (NO3) salt (e.g. Mg(NO3)2 or Ca(NO3)2), but how can we be sure that the effects we see are a consequence of nitrogen addition, rather than effects of the magnesium or calcium cations? 23.5.2 Noise Noise in the data can be generated by the same processes that generate confounding. The difference is that noise is generated even when the confounding factors don’t align with the treatments. So, going back to measuring growth rates in plants, if we were looking at growth rates of different subspecies of plant on a mountain then we might find that we can get five samples from each different subspecies, but the samples are scattered across very different altitudes on the mountain. This will add variation to the estimates of growth rate due to effects of altitude—this variation is unwanted noise. On the other hand, we might find that the subspecies each grow predominantly at different altitudes and in this situation the variation due to altitude is confounded with the variation due to subspecies—we cannot tell whether the subspecies are inherently different, or the differences are just down to altitude. 23.6 Dealing with confounding effects and noise Confounding effects occur often in biological work and noise of some sort is always present. Techniques for dealing with such effects include: randomisation blocking experimental control additional treatments. We’ll consider each of these in turn… 23.6.1 Randomisation Randomisation is fundamental to experimental design. Although there may be specific confounding factors we can identify and explicitly counter using experimental techniques, we can never anticipate all such factors. Randomisation provides an ‘insurance’ against the unpredictable confounding effects encountered in experiments. The basic principle is that each experimental unit should be selected, or allocated to a particular treatment, ‘at random’. This may involve selecting which patients to give a drug and which a placebo at random or it may involve setting out experimental plots at random locations in a field. The important thing is that of all the possible patients or plots, the ones that get a particular treatment are randomly selected. Randomisation guards against a variety of possible biases and confounding effects, including the inadvertent biases that might be introduced simply in the process of setting up an experiment. For example, if in a toxicological experiment with freshwater invertebrates the chemical treatment is set up first and then the control, it may be that the animals caught most easily from the stock tank (the largest? the weakest?) will all end up in the chemical treatment and the remainder in the control, with consequent bias in the death rates observed in the subsequent experiment. Randomisation is a critical method for guarding against confounding effects. It is the best insurance we have against unwittingly getting some other factor working in parallel to a treatment. It does not, of course, do anything to reduce noise in the data, in fact if randomisation removes confounding effectively, it can appear to increase that variation—but it is a necessary cost to pay for being able to interpret treatment effects correctly. What does ‘at random’ mean in practise? The random bit of the word randomisation has a specific meaning: objects chosen ‘at random’ are chosen independently with equal probabilities. How do we achieve this in practice? First we need a set of random numbers. For example, if we need to assign 10 experimental units to treatments we might start with a set of random integers: 4, 3, 5, 8, 7, 1, 10, 9, 6, 2. Attaining a set of random numbers is easy enough. Tables of random numbers are published in most statistics books expressly for use in setting up experiments, or R can also be used to find a set of random numbers (e.g. sample(1:10)). Exactly how these numbers are used in setting up the experiment will depend on what is practical. In the toxicological experiment the best thing to do would be to place animals in each of the test containers to be used for the experiment, number each container and then use the first half of the set of random numbers to randomly select half the containers to be the test and use the remainder as the controls. In a field experiment, a grid could be mapped out and pairs of random numbers used to select co-ordinates at random for each plot—in this case we would generate random co-ordinate values instead of using integers. 23.6.2 Blocking Another way of tackling potential confounding effects, and the general heterogeneity of biological material leading to noise, is organise experimental material into ‘blocks’. This technique, called blocking, is arguably the most important experimental design concept after replication. It works as follows: Group the objects being studied into blocks such that variation among objects within blocks is small; variation between blocks may be larger. Each treatment should occur at least once within each block19. For example, in an experiment in which mice are reared on three different diets (I, II, III), we might expect the responses of mice from within a particular litter to be fairly similar to each other, but they might be rather different to the responses of mice from different litters. If we have five litters of mice (A … E) it would be sensible to select three mice from each litter (at random) to be allocated to each treatment. I \\(A_{1}\\) \\(B_{1}\\) \\(C_{1}\\) \\(D_{1}\\) \\(E_{1}\\) II \\(A_{2}\\) \\(B_{2}\\) \\(C_{2}\\) \\(D_{2}\\) \\(E_{2}\\) II \\(A_{3}\\) \\(B_{3}\\) \\(C_{3}\\) \\(D_{3}\\) \\(E_{3}\\) (Where \\(A_{1}\\) is the first randomly chosen animal from litter \\(A\\), \\(A_{2}\\) the second, etc..). This type of blocking should, if there are differences between litters, increase the power of the experiment to detect effects of the treatment and guards against the possibility that we might by chance end up with one diet having mice from, say, only two litters. In the case of only two treatments (e.g. if we just had diets I and II), this type of blocking is simply the pairing of treatments we have encountered in the paired-sample t-test. Blocked designs with more than two blocks are typically analysed using Analysis of Variance (ANOVA). We will learn how to apply ANOVA to a blocked experimental design this in later chapters. Note that randomisation is important here also. Mice were selected at random from each litter to be allocated to each treatment and litters are essentially ‘random’ in the sense that they are not deliberately chosen to be different in any particular way, we just anticipate that they are likely to be different in some ways. Blocking crops up in all sorts of experimental (and non-experimental) study designs. Some examples are given below. If plants in an experiment on soil water levels are being grown in pots on greenhouse benches, there may be differences in light or temperature at differing distances from the glass. Treatments could be blocked along the gradient—at each position on the bench we have one pot from each treatment. This way, every treatment is represented at each position along the gradient. If a field experiment involving several treatments is set up in an environment known to have some spatial variation (e.g., different parts of a field, sections of a river, etc.) setting up one replicate of each treatment in blocks at different locations ensures that no one treatment ends up confounded by some environmental difference, and helps remove noise due to environmental effects in the final analysis. An immunity response is being tested using insects kept in a parallel set of laboratory cultures. There are insufficient insects from a single culture to run the whole experiment, so we could set up one replicate of each treatment using insects from each culture. The cultures would be the blocks—we are not particularly interested in the differences between cultures, but we want to be able to control and remove any variation due to differences between cultures, so as to stand the best chance of detecting a treatment effect. In a comparison of three new diagnostic techniques for measuring the frequency of abnormalities in tissue samples the techniques could be dependent on the person who carries them out (experience, standard of working etc.). The same workers could carry out all three techniques (in random order) and the results compared using individual workers as blocks to increase the power of the analysis to detect differences. If the process of collecting and analysing samples from an experiment is very time consuming (relative to the rate at which things might change) then we could block the experiment in time. Set up one replicate of each treatment on each of a sequence of days, and then collect the samples after a particular time, again over the same sequence of days. Each replicate has then been run for the same length of time (we would randomize the order in which treatments were sampled each day), and we could then include ‘days’ as a block within the analysis to control for any unknown differences resulting from the different setup, or sample days. It’s worth saying again: blocking is one of the most important experimental design concepts. Most experimental settings lend themselves to some kind of blocking scheme. If there is a way to block an experiment, we should do it. Why? Because a blocked experiment is more powerful, in the statistical sense, than the equivalent non-blocked version. That is, a study is more likely to detect an effect if it uses a blocked design. We will see how to analyse a blocked experiment in a later chapter. 23.6.3 Experimental control It is obvious that some unwanted variation in data will arise if there is poor measurement, or careless implementation of the treatments (imprecise administration of doses, sloppy timing of trial periods, etc.). In every study we do we should look at the ‘protocol’ issues and see if they can be made tighter. This means considering the precision of the measurements we are making, etc. in relation to the sizes of effects we are interested in, and the resources available to carry out the work. There would be no point in timing measurement intervals over which seedling growth was determined to the millisecond, or determining the soil pH to 5 decimal places, but it would be good to measure seedling height using a standard approach (natural growth form, or stretched out to maximum length? Starting from where?) and to the nearest millimetre, rather than centimetre. A second form of experimental control is where we can use experimental manipulation of some sort to control for factors that might vary among replicates or treatments. At its simplest obviously this involves controlling the other conditions (for example temperature) so that all treatments experience identical conditions (though note that it may not always be necessary for the conditions to be constant—it may be sufficient that whatever variation occurs is the same for all treatments). More complex problems arise where the unwanted variation is directly produced as a by product of the treatment we are administering (confounding again). So for example, if we were interested in the effect of decomposition of leaf litter on the microbial communities in soils we might have an experimental treatment that involves varying the amount of leaf litter placed on the soil surface in the test plots. The problem is that this will vary not just the amount of decomposing material entering the soil, but also the physical presence of the leaf litter layer will affect the microclimate at the soil surface (so for example the dryness in the surface of the soil). So we might create some sort of artificial litter which can be mixed in with the real litter , but which does not decompose, so that each plot has a constant volume of ‘litter’ on the surface, but different amounts of decomposing material entering the soil. Other situations in which this type of experimental ‘adjustment’ can be used include experiments in which different nutrient solutions have to be adjusted so that they have the same pH or where different temperature treatments have to have humidity adjusted to ensure that it remains constant. In general this type of approach can be very useful but it depends on the necessary adjustment being known, and sometimes requires continuous monitoring to keep the adjustments correct. 23.6.4 Additional treatments: ‘designing in’ unwanted variation Often we are faced by situations in which the unwanted variation — in particular confounding effects — cannot be removed by manipulating the treatments themselves, but has to be tackled by creating additional treatments whose function is to measure the extent of the unwanted variation, and then allow us to remove it statistically, from the data after the experiment is done. In other words, instead of just designing the experiment with the factor we are interested in, we ‘design in’ the sources of unwanted variation. 23.6.4.1 Transplants and cross-factoring Imagine we had an investigation that involved looking at effects of air pollution on the ability of trees to defend themselves chemically against attack by leaf-mining insects. The obvious thing to do would be to look at trees along a gradient of air pollution and monitor leaf damage by the insects. We might find that the trees in polluted areas are more attacked by the insects. However the problem here is that the trees growing in areas of high air pollution might be attacked more because they are stressed and less able to invest resources in defending themselves (as we hypothesised), or because the insects are more abundant there because their own natural enemies (birds and parasitoids) are less abundant in areas of high air pollution and so cannot control the abundance of the leaf-miners. One way of escaping this confounding effect would be to take tree saplings from polluted and unpolluted areas and do reciprocal transplants — moving trees from polluted areas into clean areas, and vice versa. This then enables us to separate out to a large extent the effect of tree quality from the effect of insect abundance as we can compare trees that have grown with and without air pollution, in both polluted and unpolluted areas. It is also possible that by careful choice of location, or other elements of design, we can include the unwanted variation as an additional factor in the design without necessarily physically manipulating the subjects, but by sampling material systematically with regard to both the thing we are interested in and the additional unwanted factor(s), so that we can cross-factor the two. For example, if we were interested in how habitat use determines gut parasite load in dogfish, then we might sample dogfish from different habitats, but also record the sex, and age, or size, of the fish. It would then be possible to separate out the effects of sex, or age, from those of where the fish were living. If we didn’t do this, then both factors would probably contribute unwanted variation, either noise, or possibly confounding effects (for example male and female dogfish have somewhat different habitat preferences). 23.6.4.2 Procedural controls Confounding effects are not only a problem along natural gradients, they can often be introduced by the experimental procedures. For example, a marine biologist investigating the effect of crab predation on the density of bivalve molluscs in an estuarine ecosystem might have cages on the mud flats from which crabs are removed, and in which any change in bivalve settlement and survival can be monitored. The obvious control for this would be equivalent plots on the adjacent mudflats with normal crab numbers. Obviously if the experiment just compares the bivalve density in cages with reduced crab numbers and in the adjacent mud flat any effects observed could be attributable to crab density, environmental changes brought about by the cages, or disturbance due to the repeated netting to remove crabs. To address this problem there are several additional controls that might be useful here. In addition to the proper treatment, bivalve density could be monitored in: a ‘no cage / no disturbance control’—open mud flat adjacent to the experiment (so no cage effects, no added disturbance). a ‘cage control’—crabs at normal density but with a cage (usually done as cage with openings to allow crabs to enter and leave). a ‘disturbance control’—crabs at normal densities, but subject to the same disturbance as the reduced density treatments (cages netted to remove crabs, but all crabs returned to the cages) The latter two could be combined if it wasn’t important to separate disturbance and cage effects, but even so in some circumstances it is quite possible for an experiment to have as many controls as there are actual treatments. The additional treatments in this sort of situation are effectively additional controls—in fact they be termed procedural controls—but they are not simply the natural ‘background’ conditions. A classic example of this type of control is the use of placebo treatments in medical trials. For example if we are investigating the effect of a drug then there may be a confounding effect due to psychological, behavioural or even physiological changes in patients resulting simply from the process of being treated, rather than any active compound in the drug. It is common, therefore, to give the drug to one group of patients and a ‘placebo’ (equivalent treatment process, but with no active component in the substance administered) to another group. The placebo is a secondary manipulation designed to equalise the effect of simply ‘being treated’. There are many other examples of similar experimental controls: a treatment involving surgical implantation of some sort of device, may require a control group who have the surgery, but without the implantation itself, or even with implantation of an inactive device, to allow us to factor out the confounding effect of surgical trauma, or the body’s reaction to the implant itself. 23.7 Ethics and practicality Although experimental design is often fairly straight forward in principle, the ideal design to test an hypothesis may turn out to be impractical, unaffordable or unethical. All experiments are constrained by practicality, most by finance and a rather smaller, but important set, by ethical considerations. Ethical factors obviously constrain experiments in subjects such as psychology and animal physiology and even in ecology where experiments in studies of rare species, species introductions, or environmental damage may be technically possible, but ethically unacceptable. However, nowhere is the problem more pronounced than in medicine. Drug testing presents the classic difficulty. Effective testing of the efficacy of a drug depends on the comparison of patients receiving the drug with closely equivalent patients not doing so, or receiving some alternative treatment. Since it is highly likely that one of the treatments will be better than another, then by definition, at least one group of people are having an available and better treatment withheld from them (e.g., Aspinal and Goodman 1995). Thus, as soon as the experimental evidence gives some indication of which treatment is best, it is very hard to justify withholding it from all patients, even if the experimenter feels that further work is necessary. Good experimental design and appropriate analysis cannot remove ethical, practical or financial problems, but they can help to ensure that where time and money are invested in investigating a problem, the maximum useful information is returned. 23.8 Further reading Barnard, C., Gilbert, F. and McGregor, P. (2007) Asking questions in biology. Longman. Ruxton, G. D. and Colegrave, N. (2010) Experimental design for the life sciences. Oxford Univ. Press. It is worth noting that in reports experiment and observation should always be distinguished. If we have carried out observations on a natural system of any sort, but where there has been no experimental manipulation of any aspect of the system, that is not an experiment. It would be inappropriate to write in a report: “This experiment consisted of measuring mean stomatal density from thirty trees growing at a range of altitudes.” Instead, we might write: “We conducted an observational study measuring mean stomatal density from thirty trees growing at a range of altitudes.”↩ Actually, there are special types of experimental design that use blocking, but where each treatment does not appear in every block. These are much more advanced than anything we will cover in this book.↩ "],
["paired-sample-t-test.html", "Chapter 24 Paired-sample t-test 24.1 When do we use a paired-sample t-test? 24.2 Why do we use a paired-sample design? 24.3 How do you carry out a t-test on paired-samples? 24.4 Carrying out a paired-sample t-test in R", " Chapter 24 Paired-sample t-test In the previous chapter we learnt about how blocking is a useful technique for dealing with confounding effects. However, we didn’t think about how we would analyse such data. If we’ve used a block design we can’t just throw our data into the kinds of statistical tests that we’ve already covered (e.g. t-tests and one-way ANOVAs). Remember that one of the assumptions of these tests is independence. If we’ve used a block design that assumption will be violated. In this chapter and the next we’ll consider the blocked design equivalents of a t-test and then an ANOVA. 24.1 When do we use a paired-sample t-test? We learned before how to use a two-sample t-test to compare means among two populations. However, there are situations in which data may naturally form pairs of non-independent observations: the first value in a sample A is linked in some way to the first value in sample B, the second value in sample A is linked with the second value in sample B, and so on. This is known, unsurprisingly, as a paired-sample design. A common example of a paired-sample design is the situation where we have a set of organisms, and we record some measurement from each organism before and after an experimental treatment. For example, if we were studying heart rate in relation to position (sitting vs. standing) we might measure the heart rate of a number of people in both positions. In this case the heart rate of a particular person when sitting is paired with the heart rate of the same person when standing. In biology, we often have the problem that there is a great deal of variation between the items we’re studying (individual organisms, forest sites, etc). There may be so much among-item variation that the effect of any difference among the situations we’re really interested in is obscured. A paired-sample design gives us a way to control for this variation. However, we should not use a two-sample t-test when our data have this kind of structure. Let’s find out why. 24.2 Why do we use a paired-sample design? Consider the following. A drug company wishes to test two drugs for their effectiveness in treating a rare illness in which glycolipids are poorly metabolised. An effective drug is one that lowers glycolipid concentrations in patients. The company is only able to find 8 patients willing to cooperate in the early trials of the two drugs. What’s more, the 8 patients vary in their age, sex, body weight, severity of symptoms and other health problems. One way to conduct an experiment that evaluates the effect of the new drug is to randomly assign the 8 patients to one or other drug and monitor their performance. However, this kind of design is very unlikely to detect a statistically significant differences between the treatments. This is because it provides very little replication, yet can we expect considerable variability from one person to another in the levels of glycolipid before any treatment is applied. This variability would to lead to a large standard error in the difference between means. A solution to this problems is to treat each patient with both drugs in turn and record the glycolipid concentrations in the blood, for each patient, after a period taking each drug. One arrangement would be for four patients to start with drug A and four with drug B, and then after a suitable break from the treatments, they could be swapped over onto the other drug. This would give us eight replicate observations on the effectiveness of each drug and we can determine, for each patient, which drug is more effective.20 The experimental design, and one hypothetical outcome, is represented in the diagram below… Figure 24.1: Data from glycolipid study, showing paired design. Each patient is denoted by a unique number. Each patient is represented by a unique number (1-8). The order of the drugs in the plot does not matter—it doesn’t mean that Drug A was tested before Drug B just because Drug A appears first. Notice that there is a lot of variability in these data, both in the glycolipid levels of each patient, and in the amount by which the drugs differ in their effects (e.g. the drugs have roughly equal effects for patient 5, while drug B appears to be more effective for patient 2). What can also be inferred from this pattern is that although the glycolipid levels vary a good deal between patients, Drug B seems to reduce glycolipid levels more than Drug A. The advantage to using a paired-sample design in this case is clear if we look at the results we might have obtained on the same patients, but where they have been divided into two groups of four, giving one group Drug A and one group Drug B: Figure 24.2: Data from glycolipid study, ignoring paired design. The patients and their glycolipid levels are identical to those in the previous diagram, but only patients 2, 3, 4 and 8 (selected at random) were given Drug A, while only patients 1, 5, 6, and 7 were given Drug B. The means of the two groups are different, with Drug B performing better, but the associated standard error would also be large relative to this difference. A two-sample t-test would certainly fail to identify a significant difference between the two drugs. So, it would be quite possible to end up with two groups where there was no clear difference in the mean glycolipid levels between the two drug treatments even though Drug B seems to be more effective in the majority of patients. What the pairing is doing is allowing us to factor out (i.e. remove) the variation among individuals, and concentrate on the differences between the two treatments. The result is a much more sensitive evaluation of the effect we’re interested in. The next question is, how do we go about analysing paired data in a way that properly accounts for the structure in the data? 24.3 How do you carry out a t-test on paired-samples? It should be clear why a paired-sample design might be useful, but how do we actually construct the right test? The ‘trick’ is to work directly with the differences between pairs of values. In the case of the glycolipid levels illustrated in the first diagram, we noted that there was a greater decrease of glycolipids in 75% of patients using Drug B compared with Drug A. If we calculate the actual differences (i.e. subtracted the value for Drug A from the value for Drug B) for each patient we might see something like… ## -3.9 -4.3 2.5 -4.5 0.5 -3.9 -7.1 -2.6 Notice that there are only two positive values in this sample of differences, one of which is fairly close to 0. The mean difference is -2.9, i.e. on average, glycolipid levels are lower with Drug B. Another way of stating this observation is that within subjects (patients), the mean difference between drug B and drug A is negative. A paired-sample design focusses on the within-subject (or more generally, within-item) change. If, on the other hand, the two drugs had had similar effects then what would we expect to see? We would expect no consistent difference in glycolipid levels between the Drug A and Drug B treatments. Glycolipid levels are unlikely to remain exactly the same over time, but there shouldn’t be any pattern to these changes with respect to the drug treatment: some patients will show increases, some decreases, and some no change at all. The mean of the differences in this case should be somewhere around zero (though sampling variation will ensure it isn’t exactly equal to zero). So, to carry out a t-test on paired-sample data we have to: 1) find the mean of the difference of all the pairs and 2) evaluate whether this is significantly different from zero. We already know how to do this! This is just an application of the one-sample t-test, where the expected value (i.e. the null hypothesis) is 0. The thing to realise here, is that although we started out with two sets of values, what matters is the sample of differences between pairs and the population we’re interested in a ‘population of differences’. When used to analyse paired data in this way, the test is referred to as a paired-sample t-test. This is not wrong, but it important to remember that a paired-sample t-test is just a one-sample t-test applied to the sample of differences between pairs of associated observations. A paired-sample t-test isn’t really a new kind of test. Instead, it is a one-sample t-test applied to a new kind of situation. 24.3.1 Assumptions of the paired-sample t-test The assumptions of a paired-sample t-test are no different from the one-sample t-test. After all, they boil down to the same test! We just have to be aware of the target sample. The key point to keep in mind is that it is the sample of differences that is important, not the original data. There is no requirement for the original data to be drawn from a normal distribution because the normality assumption applies to the differences. This is very useful, because even where the original data seem to be drawn from a non-normal distribution, the differences between pairs can often be acceptably normal. The differences do need to be measured on an interval or ratio scale, but this is guaranteed if the original data are on one of these scales. 24.4 Carrying out a paired-sample t-test in R R offers the option of a paired-sample t-test to save us the effort of calculating differences. It calculates the differences between pairs for us and then carries out a one-sample test on those differences. We’ll look at how to do it the ‘old fashioned’ way first—calculating the differences ourselves and running a one-sample test—before using the short-cut method provided by R. You should work through the example in this section. Staying with the problem of trials of two drugs for controlling glycolipid levels, the serum glycolipid concentration data from the trial illustrated above are stored in the GLYCOLIPID.CSV file. Download the this file from MOLE and place it in the working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name glycolipid. As always, we should start by looking at the raw data: glimpse(glycolipid) ## Observations: 16 ## Variables: 4 ## $ Patient &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8 ## $ Sex &lt;fctr&gt; Male, Female, Male, Female, Female, Male, Female, ... ## $ Drug &lt;fctr&gt; A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B ## $ Glycolipid &lt;dbl&gt; 142.9, 140.6, 144.7, 144.0, 142.4, 146.0, 149.1, 15... There are four variables in this data set: Patient indexes the patient identity, Sex is the sex of the patient (we don’t need this), Drug denotes the drug treatment, and Glycolipid is the glycolipid level. Next, we need to calculate the differences between each pair. We can do this with the dplyr functions group_by and summarise: glycolipid_diffs &lt;- glycolipid %&gt;% group_by(Patient) %&gt;% summarise(Difference = diff(Glycolipid)) What we did was group the data by the values of Patient, and then used a function called diff to calculate the difference between the two Glycolipid concentrations within each patient. We stored the result of this calculation in a new data frame called glycolipid_diffs. This is the data we’ll use to carry out the paired-sample t-test: glycolipid_diffs ## # A tibble: 8 x 2 ## Patient Difference ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -3.8 ## 2 2 -4.2 ## 3 3 2.6 ## 4 4 -4.6 ## 5 5 0.6 ## 6 6 -3.8 ## 7 7 -7.0 ## 8 8 -2.5 We should try to check that the differences could plausibly have been drawn from a normal distribution, though normality is quite hard to assess with only 8 observations: ggplot(glycolipid_diffs, aes(x = Difference)) + geom_dotplot() + theme_grey(base_size = 22) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Figure 24.3: Within-individual differences from glycolipid study The data seems roughly normal, so let’s carry out a one-sample t-test on the calculated differences. This is test is easy to do in R: t.test(glycolipid_diffs$Difference) ## ## One Sample t-test ## ## data: glycolipid_diffs$Difference ## t = -2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -5.397549 -0.277451 ## sample estimates: ## mean of x ## -2.8375 We don’t have to set the data argument to carry out a one-sample t-test on the differences. We just passed along the numeric vector of differences extracted from glycolipid_diffs (using the $ operator). What happened to the mu argument used to set up the null hypothesis? Remember, the null hypothesis is that the population mean is zero. R assumes that this is 0 if we don’t supply it, so no need to set it here. The output is quite familiar… The first line reminds us what kind of test we did, and the second line reminds us what data we used to carry out the test. The third line is the important one: t = -2.6209, df = 7, *p*-value = 0.03436. This gives the t-statistic, the degrees of freedom, and the all-important p-value associated with the test. The p-value tells us that the mean within-individual difference is significant at the p &lt; 0.05 level. We need to express these results in a clear sentence incorporating the relevant statistical information: Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A (t = 2.62, d.f. = 7, p &lt; 0.05). There are a couple of things to point out in interpreting the result of such a test: The sample of differences was used in the test, not the sample of paired observations. This means the degrees of freedom for a paired-sample t test are one less than the number of differences (= number of pairs); not one, or two, less than the total number of observations. Since we have used a paired-sample design our conclusion stresses the fact that the use of the Drug B results in a lower glycolipid level in individual patients; it doesn’t say that the use of Drug B resulted in lower glycolipid concentrations for everyone given Drug B than for anyone given Drug A. 24.4.1 Using the paired = TRUE argument R does have a built in procedure for doing paired-sample t-tests. Now that we’ve done it the hard way, let’s try carrying out the test using the built in procedure. This looks very similar to a two-sample t-test, except that we have to set the paired argument of the t.test function to TRUE: t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.277451 5.397549 ## sample estimates: ## mean of the differences ## 2.8375 R takes care of the differencing for us, so now we can work with the original glycolipid data rather than the glycolipid_diffs data frame constructed above. We won’t step through the output because it should make sense by this point. Order matters Be careful when using the built in procedure for doing paired-sample t-tests. The only information R uses to associate pairs of observations is their order in each group. The first observation in the ‘A’ group is paired with the first observation in the ‘B’ group, the second observation in the ‘A’ group is paired with the second observation in the ‘B’ group, and so on. If the items/individuals aren’t ordered the same way in each group, the test will be wrong and we’ll end up with a meaningless p-value! R makes it easy to do paired-sample t-test. It really doesn’t matter which method we use to carry out the test. Just don’t forget that a paired-sample t-test is only a one-sample test on paired differences. This kind of experimental design is called a cross-over study. It can be problematic if, for example, “carry-over” effects occur, e.g., the effect of one drug is altered when the other drug has previously been administered. We won’t worry about these problems here though.↩ "],
["anova-for-randomised-block-designs.html", "Chapter 25 ANOVA for randomised block designs 25.1 Randomized Complete Block Designs 25.2 Designs without replication 25.3 Analysing an RCBD experiment 25.4 Carrying out the analysis with R 25.5 Are there disadvantages to randomised block designs? 25.6 Multiple blocking factors", " Chapter 25 ANOVA for randomised block designs Block what you can; randomize what you cannot. George Box 25.1 Randomized Complete Block Designs We have only considered one type of experimental ANOVA design up until now: the Completely Randomised Design (CRD). The defining feature of a CRD is that treatments are assigned completely at random to experimental units. This is the simplest type of experimental design. The randomisation is a good thing because it prevents systematic biases creeping in. A CRD approach is often ‘good enough’ in many situations. However, it isn’t necessarily the most powerful design—if at all possible, we should use blocking to account for nuisance factors. A nuisance factor is one that has an effect on the response—it creates variation—but is of no interest to the experimenter. To end up with the most powerful experiment possible, the variability induced by nuisance factors should be accounted for at the design stage of an experiment. Let’s consider a hypothetical experiment to remind ourselves how blocking works… Imagine we’re evaluating the effect of three different fertilizer application rates on wheat yields (t/ha). We suspect that the soil type and management histories of our experimental fields are quite different, leading to significant ‘field effects’ on yield. We don’t care about these field effects—they are a nuisance—but we’d like to account for them. There are two factors in play in this setting: the first is the set of treatments that are the subject of the experiment (fertilizer application rate); the second is the source of nuisance variation (field). Fertilizer application rate is the ‘treatment factor’ and field is the ‘blocking factor’. Here is one way to block the experiment. The essence of the design is that a set of fields are chosen, which may differ in various unknown conditions (soil water, aspect, disturbance, etc.) and within each field, three plots are set up. Each plot receives one of the three fertilizer rate treatments at random. If we chose to work with eight fields the resulting data might look like this: Fertilizer Control Absent High Block Field 1 9.3 8.7 10.0 Field 2 8.7 7.1 9.1 Field 3 9.3 8.2 10.4 Field 4 9.5 8.9 10.0 Field 5 9.9 9.1 10.8 Field 6 8.9 8.0 9.0 Field 7 8.3 6.2 8.9 Field 8 9.1 7.0 8.1 Each treatment level is represented in each field (block), but only once. The experiment is ‘blocked by field’. Now consider these two questions: Why is this design useful? Blocking allows us to partition out the environmental variation due to different field conditions. For example, the three treatments in field 5 produced a high yield relative to the yields within each treatment, while the yields in field 7 are consistently below average within each treatment. This among field variation is real, and if we hadn’t blocked the experiment and used a CRD it would manifest itself in the ‘noise’ component of our analysis. But since we blocked the experiment, and every treatment is present in every block, we can ‘remove’ the block variation from the noise. Less noise means more statistical power. Why is each treatment level is represented only once within blocks? This gives us the best chance of generalising our results. If we are interested in the overall effect of fertilizer, we should prefer to put our effort into including a range of possible environmental conditions. If we only did the experiment in one field the results might turn out to be rather unusual. We are not interested in the environmental variation as such, we just want to know for a range of conditions, whatever they might be, whether there are consistent differences between fertilizer application rates. There are many different ways to introduce blocking into an experiment. The most commonly used design—and the one that is easiest to analyse—is called a Randomized Complete Block Design. The defining feature of this design is that each block sees each treatment exactly once. The fertiliser study is an example of a Randomized Complete Block Design (RCBD). The obvious question is: How do we analyse an RCBD? We’ll explore that after a small detour…. 25.2 Designs without replication So far we’ve only used the one-way ANOVA. As we’ll find out in the Introduction to two-way ANOVA chapter we can have more than one treatment in an ANOVA. Usually when we’re interested in the effect of two variables on our response variable we’ll have the following design: two factors, each having two or more levels, with replicate measurements within each combination of levels: Factor A Level 1 Level 2 Level 3 Factor B Level 1 1,2,3 1,5,9 2,6,8 Level 2 3,4,2 … … Level 3 4,7,9 … … Level 4 4,6,5 … … As we have just seen however, it is possible to have a two-way design with only a single measurement within each combination of levels: Factor A Level 1 Level 2 Level 3 Factor B Level 1 1 1 2 Level 2 3 … … Level 3 4 … … Level 4 2 … … What’s this… no replication? Isn’t that a problem? In fact there is replication of a sort for each level of the factors. For example there are 3 values for each level of Factor B, it’s just that each value is at a different level of Factor A. This means we can still compare the means for the different levels of each treatment using an ANOVA (that is, we can analyse the main effects). What we can’t do is determine whether the effect of Factor B varies depending on the value of Factor A. This is called the interaction. We’ll come back to this idea in the Introduction to two-way ANOVA chapter so don’t worry if it seems confusing now. The interaction is derived from differences between the means and variances of the cases in each individual cell in the table (= each combination of the two factor levels). If there is only one value in a cell then clearly means and variances cannot be calculated. So… it is possible to carry out a two-way ANOVA without replication, though we can’t learn anything about the interaction with this sort of design. Is this ever useful? There are two situations where this sort of unreplicated two-way arrangement is used: One of the factors is a blocking factor. We could have guessed this one, given what this chapter is about. If we have a single blocking factor and one treatment factor, and we adopt an RCBD, then this leads to an unreplicated two-way arrangement. The fact that we can’t estimate the interaction is not a problem. In fact, “it’s a feature, not a bug.” We only care about the treatment factor. There might well be an interaction between the blocks and the experimental treatments, but we only need the average effect of treatments across blocks to be able to generalise our results. We are considering two treatment factors (no blocks), but we are only interested in the main effects. We have three choices in this situation: 1) conduct two separate one-way experiments, 2) construct a single two-way experiment without replication, or 3) construct a single two-way experiment with replication. The third option would be best, but sometimes we just don’t have the resources to do it21. What is the next best choice: option 1 or option 2? The two-way experiment without replication is the better option because a two-way design—with or without replication—is always more powerful than a pair of one-way experiments that include an equivalent number of replicates between them. So there is nothing “dodgy” about a two-way design without replication. Indeed, it is the best design to use in some situations. Let’s see how to the analyse a Randomized Complete Block Design experiment. Interactions are not a feature of the experimental design It’s important to realise that just because we can’t test for the interaction, it does not mean there is no interaction effect between two factors. We simply can’t detect it even if it is there. However, as long as we have a balanced, orthogonal design, a significance test of main effects is still meaningful. If the balanced and orthogonal criteria are not met, then we do have to be very careful about how we interpret a significant main effect in the absence of replication, because the presence of an interaction may generate a spuriously significant main effect. 25.3 Analysing an RCBD experiment Let’s consider a new example to really drive home how an RCBD works. We want to assess whether there is a difference in the impact that the predatory larvae of three damselfly species (Enallagma, Lestes and Pyrrhosoma) have on the abundance of midge larvae in a pond. We plan to conduct an experiment in which small (1 m2) nylon mesh cages are set up in the pond. All damselfly larvae will be removed from the cages and each cage will then be stocked with 20 individuals of one of the species. After 3 weeks we will sample the cages and count the density of midge larvae in each. We have 12 cages altogether, so four replicates of each of the three species can be established. On the face of it this looks like a straightforward one-way design, with each species as a treatment. The only problem to resolve is how to distribute the enclosures in the pond. Obviously the pond is unlikely to be uniform in depth, substrate, temperature, shade, etc… Some of the variation will be obvious, some will not. We have two options: 1) use a CRD and distribute the cages at random, or 2) adopt an RCBD by grouping the cages into clusters of three, placing each cluster at a randomly chosen location, and assigning the three treatments to cages at random within each cluster. These are illustrated below (left = CRD, right = RCBD): What are the consequences of the two alternatives? If the cages are distributed at random (CRD) then they will cover a wide range of variation in these various factors. These sources of variation will almost certainly cause the density of midge larvae to vary around the pond in an unpredictable way, increasing the noise in the data. If we group sets of treatments into clusters we are creating ‘spatial blocks’. There may be considerable differences between blocks, but these won’t obscure differences between the treatments because all three treatments are present in every block. 25.4 Carrying out the analysis with R Walk through example You should work through the example from here. The data from the damselfly experiment are in a file called DAMSELS.CSV. damsels &lt;- read.csv(&quot;./DAMSELS.CSV&quot;) glimpse(damsels) ## Observations: 12 ## Variables: 3 ## $ Midge &lt;int&gt; 304, 464, 320, 578, 509, 458, 680, 740, 630, 356, 390,... ## $ Species &lt;fctr&gt; Enallagma, Lestes, Pyrrhosoma, Enallagma, Lestes, Pyr... ## $ Block &lt;fctr&gt; A, A, A, B, B, B, C, C, C, D, D, D The density of midge larvae in each enclosure, after running the experiment for 3 weeks, are in the Midge variable (number m\\(^{-2}\\)); codes for species in the Species variable (levels: Enallagma, Lestes and Pyrrhosoma), and the block identities (A, B, C, D) in the third column. The process of analysing an RCBD is essentially the same as any other type of ANOVA. First we fit the model using the lm function and then we use anova to calculate F-statistics, degrees of freedom, and p-values: damsels.model &lt;- lm(Midge ~ Species + Block, data = damsels) anova(damsels.model) We suppressed the output for now. Notice that we have put two factors on the right hand side of the ~ symbol. This tells R that we want to fit a model that accounts for the main effects of Species and Block. We put a + between terms to delineate them. Notice that we only specified the two main effects (species and blocks)—we cannot test for the interaction. In general an interaction can be included using a colon (the : symbol) with the two interacting variables either side of it as follows: Midge ~ Species + Block + Species:Block If we put an interaction term in this model however, lm will fit it but the results from anova will be useless: ## Warning in anova.lm(wrong.model): ANOVA F-tests on an essentially perfect ## fit are unreliable ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452 ## Block 3 208425 69475 ## Species:Block 6 14878 2480 ## Residuals 0 0 Look at the residual degrees of freedom (the error degrees of freedom). The model we just fitted is called a saturated model—there are zero degrees of freedom left over after fitting the three terms. We can’t calculate an error sum of squares, which means we can’t calculate mean squares or F-ratios. This demonstrates that there really is no way to estimate an interaction in ANOVA when there is no replication at the level of each combination of factor levels. Here are the results of the global significance tests using the correct ANOVA model for our randomised block experiment: anova(damsels.model) ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452 3.0053 0.1246687 ## Block 3 208425 69475 28.0182 0.0006306 *** ## Residuals 6 14878 2480 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does all this mean? We interpret each line of the ANOVA table in exactly the same way as we do for a one-way ANOVA. The first part tells us what kind of output we are looking at: ## Analysis of Variance Table ## ## Response: Midge This reminds us that we are looking at an ANOVA table where our response variable was called Midge. The table contains the key information: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452 3.0053 0.1246687 ## Block 3 208425 69475 28.0182 0.0006306 *** ## Residuals 6 14878 2480 This ANOVA table is similar to the ones we have already seen, except that now we have to consider two lines—one for each term in the model. The first is for the main effect of Species and the second for the main effect of Block. The F-ratio is the test statistic for each term. These provides a measure of how large and consistent the effects associated with each term are. Each F-ratio has a pair of degrees of freedom associated with it: one belonging to the term itself, the other due to the error (residual). Together, the F-ratio and its degrees of freedom determines the p-value. The p-value gives the probability that the differences between the set of means for each term in the model, or a more extreme difference, could have arisen through sampling variation under the null hypothesis of no difference. We take p &lt; 0.05 as evidence that at least one of the treatments is having an effect. Here, there is a significant effect of block (p &lt; 0.001), which says that the density of midge larvae varies across the lake. It looks like blocking was a good idea—there is a lot of spatial (nuisance) variation in midge larvae density. Of course what we actually care about is the damselfly species effect. This main effect term is not significant (p &gt; 0.05), so we conclude that there is no difference in the impact of the predatory larvae of three damselfly species. It is worth looking at what happens if we analyse the damselfly data as though they are from a one-way design. We do this by including only the experimental treatment term (Species) in the model: damsels.oneway &lt;- lm(Midge ~ Species, data = damsels) anova(damsels.oneway) ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452.1 0.3003 0.7477 ## Residuals 9 223303 24811.4 Look at the degrees of freedom and the sums of squares of the residual (error). How do these compare to the previous model that accounted for the block effect? The degrees of freedom is higher. In principle this is a good thing because it means we have more power to detect a significant difference among the treatment means. However, the error sum of squares is also much higher when we ignore the block effect. We have accounted for much less noise by ignoring the block effect. As a result, the error mean square is much lower, and so the F-statistic associated with the treatment effect is also much lower. The take home message is that designing a blocked experiment, and properly accounting for the blocked structure, will (usually) result in a more powerful analysis. 25.4.1 Multiple comparisons anyone? In a randomised block analysis we are not usually interested in investigating significant block effects—the primary role of the blocking is to remove unwanted variation that might obscure the differences between treatments. R automatically gives us a test of the block effect, and if it is significant it tells us that using the block layout has removed a considerable amount of variation (though even if the result isn’t quite what would conventionally be regarded as significant, i.e. if is not as low as 0.05, then the blocking may still have been helpful). For this, and other, technical reasons we never carry out multiple comparisons between the block means. If the treatment effect is significant, multiple comparisons can be done between the treatment means using the Tukey test. 25.5 Are there disadvantages to randomised block designs? The short answer is no, not really. There are instances when a randomised block design might appear to be disadvantageous at first glance, but these don’t really stand up to criticism: What if we were interested in knowing whether there is an interaction between the levels of the block and the treatments? For example, in the damselfly experiment we might be interested to know whether, if the damselfly species have differing effects on the midge densities, these effects are consistent in all habitat areas (e.g. some species may forage more effectively in muddy areas, others where there are more leaves). If this is the question we are trying to answer, then we should really have designed a different experiment. For example, a two-way design (which might also include blocking) in which we consider treatment combinations of different midge species and habitat characteristics might be appropriate. Fundamentally, the goal of blocking is to account for uncontrolled variation. Designing a blocked experiment and then lamenting the fact that we can’t fully evaluate differences among blocks is a good example of trying to “have our cake and eat it too”. If the blocking term is having no effect in accounting for some of the variation, then the analysis may be slightly less powerful than just using a one-way ANOVA. This is because there are fewer error degrees of freedom associated with the blocked analysis (we lose a few to the block effects). This argument only works if the block effect accounts for very little variation. We can never know before we start an experiment whether or not blocking is needed, but we do know that biological systems are inherently noisy, with many sources of uncontrolled variation coming into play. In the majority of experimental settings (in biology at least) we can be fairly sure that blocking will ‘work’. If we choose not to block an experiment there is no way to account for uncontrolled variation and we will almost certainly end up losing statistical power as a result. The advice contained in the quote at the beginning of this chapter is probably the best experimental design advice ever dished out: “Block what you can; randomize what you cannot.” 25.6 Multiple blocking factors You will not be assessed on the material in this section. It is here to demonstrate that there are options beyond the Randomised Complete Block Design, but that they aren’t easy to employ. It is common to find ourselves in a situation whereby we need to account for more than one blocking factor. The simplest option is to combine the nuisance factors into a single factor. However, this isn’t always possible, or even desirable. Consider an instance where there is a single factor of primary interest (the treatment factor) and two nuisance factors. For example, imagine that we want to test three drugs A, B, C for their effect in alleviating the symptoms of a disease. Three patients are available for a trial, and each will be available for three weeks. Testing a single drug requires a week, meaning an experimental unit is a ‘patient-week’. The obvious question is, how should we randomise treatments across ‘patient-weeks’? We have to design an experiment like this with great care, or there is a risk that we will not be able to statistically separate the effects of the treatment (drug) and block effects (week &amp; patient). The most appropriate design for this kind of experiment has the following structure: Week Patient Drug 1 1 A 1 2 B 1 3 C 2 1 C 2 2 A 2 3 B 3 1 B 3 2 C 3 3 A This kind of experimental design is called a Latin square design. It gets its name form the fact that if we organise the treatments into the rows and columns of a grid according to week and patient number, we arrive at something like this22: A B C C A B B C A Notice that each letter appears once in each column and row. Mathematicians call this a Latin square arrangement. Latin square designs (and their more exotic friends, e.g. ‘Hyper-Graeco-Latin square designs!’) have a very useful property: they allow us to unambiguously separate treatment and block effects. The reasoning behind this conclusion is quite technical, so we won’t try to explain it. We just want to demonstrate that it is perfectly possible to block an experiment by more than one factor, but this needs to be done with care (this is a ‘seek advice’ situation). Having replication at each combination of factors will maximise statistical power, but it also makes for a more robust experiment—if we have only one replicate for a combination and we lose it then we have no information at all from that combination! If we lose just one of a number of replicates in a particular combination it may be a nuisance, but unlikely to ruin everything.↩ This example probably isn’t a very good experimental design, for the simple reason that it lacks statistical power. However, we could design a better version of this experiment using the same basic principles if we needed to.↩ "],
["introduction-to-two-way-anova.html", "Chapter 26 Introduction to two-way ANOVA 26.1 Introduction 26.2 Degrees of freedom, mean squares, and F-statistics 26.3 Multiple comparison tests 26.4 Beyond two-way ANOVA", " Chapter 26 Introduction to two-way ANOVA The experimenter who believes that only one factor at a time should be varied is amply provided for by using a factorial experiment. Box et al. (2005) 26.1 Introduction One-way ANOVA allows us to determine whether there are significant differences between the effects of two or more treatments. The treatments we are interested in comparing are the different levels of a factor. These levels may represent quantitative variations of a general treatment (e.g. the effect of different concentrations of slug poison on slugs), or qualitatively different varieties of a class of treatments (e.g. the effect of different diets on weight loss). Fairly obviously, we are less likely to be interested in questions which involve comparing completely different sorts of treatment. For example, it is hard to see the value of an experiment comparing the movement of slugs where the three different treatments are: (i) half-strength Slugit (ii) moist wood substrate (iii) darkness. None of the treatments are comparable and so it will be very difficult to interpret differences between them. Although the above experiment would not be very useful, we might well be interested in whether the moisture level of the substrate has an effect on movement rate, and similarly, whether movement is affected by the slug being in the light or dark. To address these questions we could obviously design two separate experiments—one where the treatments are wooden boards with two or more different moisture levels, and another where the treatments are ‘light’ or ‘dark’. Although this is a perfectly valid procedure, it still leaves us lacking some information. With the moisture experiment, we have to decide whether to run it in the light or the dark, and with the light and dark experiment we have to decide how moist the wooden boards we use should be. If we decide to run our moisture level experiment in the dark, then we end up (we hope) knowing something about the effect of moisture on slug movement, but only in dark conditions—we can’t say whether the effect of moisture would have been different had the slugs been in the light (and obviously we can’t say anything about the effect of light and dark more generally since that is the subject of our other experiment). One obvious solution might be to run two moisture experiments… one in the dark and one in the light, and similarly three light/dark experiments, one at each different moisture level. This is indeed what we want to do, but instead of running each combination separately, it is more powerful, and experimentally less problematic, to run all the combinations together. If we used four slugs in each combination, we would require: 4 replicates \\(\\times\\) 3 moisture levels \\(\\times\\) 2 light levels \\(=\\) 24 slugs. We would end up with 4 measurements of movement rate in each combination of treatments (figures are cm per min), e.g. Moisture level Light level &lt;5% 50% 100% Dark 2, 3, 5, 0 3, 9, 5, 10 15, 8, 11, 12 Light 4, 2, 7, 1 10, 7, 4, 13 13, 17, 12, 9 An experiment of this sort, where measurements are made under each combination of several levels of two or more different kinds of experimental treatments, is called a fully factorial experiment. This type of experimental design gets its name from the fact it involves every combination of treatments among two or more factors. The example here is a two-factor experiment because it has two different kinds of treatment (illumination and moisture). It should be straightforward to see that different factors can be combined in a single experiment, and that this seems to yield the maximum amount of information, but to get at that information we need to be able to analyse the data. Fortunately the principles of ANOVA that you have seen already can be extended to provide a powerful and elegant way of analysing data from factorial designs. With two different sets of treatment (as here) this approach is referred to as two-way ANOVA (also known as two-factor ANOVA). A two-way ANOVA on data from the slug experiment would tell us whether slug movement was affected by (1) moisture and (2) illumination, and (3) whether the effect of illumination depends on moisture levels (and vice versa). So instead of just one result (as we get from a one-way ANOVA) there are now three to consider. The effect of moisture and of illumination are termed main effects and the effect of each moisture level / illumination combination is termed the interaction. What are these? The main effects are fairly obvious: The moisture effect …tells you whether there is a significant difference between the mean movement of slugs among the three moisture levels (i.e., the means of the data in each of the three columns in the table above, across both light levels). The illumination effect …tells you whether there is any difference between slug movement in the light and dark (i.e., the means of the data in each of the two rows in the table above, across all moisture levels). The interaction is a bit more tricky: The interaction between moisture and illumination …tells you whether there are differences between slug movement rates which are due to specific combinations of different moisture and illumination levels, which cannot be accounted for just by combining the mean effects of moisture level and of illumination level (i.e. are there differences between the means of the data from each cell in the table, having taken account of the overall effects of moisture and illumination?). Another way of looking at the interaction is that it indicates whether slug movement responds differently to moisture depending on whether it is in the light or dark. All this will probably make more sense when we have an example to work with, so we’ll carry out a two-way ANOVA and then come back to how the results should be interpreted. Treatments When writing about factorial experiments, the word ‘treatment’ tends to be used in two subtly different ways: Some people enumerate different treatments at the level of combinations of factor levels. For example, if we were carrying out an experiment with two factors, each of which has two levels (‘A’ vs. ‘B’ and ‘X’ vs. ‘Y’), we would say that the experiment has four treatments. Others delineate treatments at the level of individual factors, and then refer to ‘treatment combinations’ to distinguish unique experimental conditions. In our example we would say that each factor involves two treatments and overall, the experiment involves four treatment combinations. Notice that ‘factor levels’ and ‘treatments’ are synonymous when using the second naming convention (this suggests the first definition is probably the more useful one). We will adopt this second convention in this course because it is so widely used. 26.2 Degrees of freedom, mean squares, and F-statistics We are not going to step through the logic underpinning the calculations of the degrees of freedom, sum of squares, mean squares, and F-statistics. The logic is no different than that used in one-way ANOVA. It is a bit trickier to explain and visualise though. Ultimately, an F-statistic is calculated for each term, which is the ratio of the term’s mean square and the error mean square. A higher F-statistic is more likely to be significant, and the p-value is calculated by comparing the F-statistic to the theoretical F distribution. 26.3 Multiple comparison tests Having established that there are significant differences, we might wish to go further and specify between which means these differences occur. With one-way ANOVA obviously there was only one set of means to compare with the multiple comparison tests. Now, however, there are three possible sets of means: the two main effects and the interaction. 26.4 Beyond two-way ANOVA It is possible to have more complex designs using 3 or more factors (‘multi-way fully factorial’)—for example we could add to our experiment considered earlier by running our existing treatment combinations at each of three different temperatures—but as the experiment becomes more complex, so does the analysis and interpretation (and also the work involved in running it: adding three temperature treatments would mean we needed 72 slugs!). A multi-way factorial design isn’t really a ‘different’ kind of design from the two-way case we have examined. The principle of the 2-way fully factorial design can be directly extended to multi-way fully factorial designs. In a three-way design (factors A, B, C) there are three main effects (A, B, C), three pairwise interactions (A x B, A x C, B x C) and one new kind of interaction: a three-way interaction (A x B x C). The challenge posed by such designs is that the results can be tricky to interpret (what is a three-way interaction?). Analysis of Variance is a large and complex subject—we are only scratching the surface in this book. Most intermediate level biostatistics texts deal with the more involved designs of ANOVA. As with many aspects of statistics and experimental design there is much to be said for doing experiments and analyses you are confident you understand and can interpret, even if more complex forms of analysis are technically possible (providing of course the simpler approach is appropriate!). When contemplating a design that looks like it might require more than two factors, it is a good idea to talk to someone who knows about these things to ensure that is indeed necessary. "],
["two-way-anova-in-r.html", "Chapter 27 Two-way ANOVA in R 27.1 Introduction 27.2 Competition between Calluna and Festuca 27.3 Visualising the data 27.4 Fitting the ANOVA model 27.5 Diagnostics 27.6 Interpreting the results 27.7 Multiple comparison tests 27.8 Drawing conclusions and presenting results 27.9 Balanced or orthogonal designs", " Chapter 27 Two-way ANOVA in R 27.1 Introduction Our goal in this chapter is to learn how to work with two-way ANOVA models in R, using an example from a plant competition experiment. The work flow is very similar to one-way ANOVA in R. We’ll start with the problem and the data, and then work through model fitting, evaluating assumptions, significance testing, and finally, presenting the results. 27.2 Competition between Calluna and Festuca Plants have an optimal soil pH for growth, and this varies between species. Consequently we would expect that if we grow two plants in competition with each other at different pH values the effect of competition might vary according to the soil pH. In a recent study the growth of the grass Festuca ovina (Sheep’s Fescue) in competition with the heather Calluna vulgaris (Ling) was investigated in soils with different pH. Calluna is well adapted to grow on very acidic soils such as on the Millstone grit and blanket bogs around Sheffield. Festuca grows on soils with a much wider range of pH. We might hypothesise that Calluna will be a better competitor of Festuca in very acid soils than in moderately acid soils. To test this hypothesis an experiment was designed in which Festuca seedlings were grown in pots at all combinations of two levels of two different kinds of treatment: Factor 1: Soil pH at 3.5 or 5.5 Factor 2: Presence or absence of Calluna. This is a fully factorial, two-way design. The total number of treatments was thus \\(2 \\times 2 = 4\\). For each of the treatments there were 5 replicate pots, giving a total of \\(2 \\times 2 \\times 5 = 20\\) pots. The following data are the yields of Festuca from each treatment (dry weight in g) from the two pH levels and in the presence or absence of Calluna. pH 3.5 pH 5.5 Calluna Present 2.76, 2.39, 3.54, 3.71, 2.49 3.21, 4.10, 3.04, 4.13, 5.21 Calluna Absent 4.10, 2.72, 2.28, 4.43, 3.31 5.92, 7.31, 6.10, 5.25, 7.45 Walk through You should begin working through the Festuca example from this point. The data are in a CSV file called FESTUCA.CSV. We’ll read the data into an R data frame, giving it the name festuca, and then print the whole data frame to the Console (View is another option here): festuca &lt;- read.csv(file = &quot;FESTUCA.CSV&quot;) ## Weight pH Calluna ## 1 2.76 pH3.5 Present ## 2 2.39 pH3.5 Present ## 3 3.54 pH3.5 Present ## 4 3.71 pH3.5 Present ## 5 2.49 pH3.5 Present ## 6 4.10 pH3.5 Absent ## 7 2.72 pH3.5 Absent ## 8 2.28 pH3.5 Absent ## 9 4.43 pH3.5 Absent ## 10 3.31 pH3.5 Absent ## 11 3.21 pH5.5 Present ## 12 4.10 pH5.5 Present ## 13 3.04 pH5.5 Present ## 14 4.13 pH5.5 Present ## 15 5.21 pH5.5 Present ## 16 5.92 pH5.5 Absent ## 17 7.31 pH5.5 Absent ## 18 6.10 pH5.5 Absent ## 19 5.25 pH5.5 Absent ## 20 7.45 pH5.5 Absent Notice that the data for a two-factor experiment can be laid out in a very similar manner to those from a one factor experiment, with the response variable in one column, but now there are two additional columns containing the codes for the treatments, one for pH and one for Calluna. The first column (Weight) contains the Festuca dry weights, the second column (pH) contains the codes for the pH treatment (levels: pH3.5, pH5.5), the third column (Calluna) contains the codes for the presence or absence of Calluna (levels: Present, Absent). A couple of points are worth noting at this point: As is always the case in this book, the data are ‘tidy’. Each experimental factor is in one column and each observation is in a single row. Last year we pointed out that data need to be in this format to be used effectively with dplyr. The same applies to the majority of R’s statistical modelling tools—they expect data to be supplied in this format. We avoided using numbers to encode the levels of the pH treatment. This is important, as it ensures that the pH variable will be converted into a factor rather than a number when we read the data into R. We have said it before, but it is worth saying one more time: fewer mistakes will occur if we use words to encode the levels of a factor. 27.3 Visualising the data We should take a look at the data before doing anything with it. We only have five replicates per treatment combination, so any figure we produce is going to provide only limited information. Five replicates is just about sufficient for a box plot: ggplot(data = festuca, aes(x = Calluna, y = Weight, colour = pH)) + geom_boxplot() The main purpose of a plot like this is to help us understand what the treatments are doing. We want to quickly assess things like: How big are the main effects? What direction do they work in? Is there likely to be an interaction? It looks like the higher pH conditions tends to increase Festuca growth, and the presence of Calluna tends to reduce it (yes, plants compete!). The more interesting observation is that the effect of Calluna seems to be greater at higher pH. It looks like we might have an interaction. The next thing we should do is check whether any of the assumptions have been violated. We can do this using the regression diagnostics that we learnt about in the Diagnostics chapter. To do this we need to fit the model first. 27.4 Fitting the ANOVA model Carrying out a two-way ANOVA in R is really no different from one-way ANOVA. It still involves two steps. First we have to fit the model using the lm function, remembering to store the fitted model object. This is the step where R calculates the relevant means, along with the additional information needed to generate the results in step two. The second step uses the anova function to calculate F-statistics, degrees of freedom, and p-values. Here is the R code needed to carry out the model-fitting step with lm: festuca_model &lt;- lm(Weight ~ pH + Calluna + pH : Calluna, data = festuca) This is very similar to the R code used to fit a one-way ANOVA model. The first argument is a formula (notice the ‘tilde’ symbol: ~) and the second argument is the name of the data frame that contains all the variables listed in the formula. That’s all we need. The specific model fitted by lm is a result of 1) the type of variables referenced in the formula, and 2) the symbols used to define the terms in the formula. To ensure that we have fitted an ANOVA model, the variables which appear to the right of the ~ must be factors or character vectors—an ANOVA only involves factors. The variable name to the left of the ~ is the numeric response variable we are analysing. We know that Calluna and pH are factors, so we can be certain that lm has fitted some kind of ANOVA model. What kind of ANOVA have we fitted, i.e., what are the terms on the right hand side of the formula doing? Here is the formula we used: Weight ~ pH + Calluna + pH:Calluna There are three terms, each separated by a + symbol: pH, Calluna and pH:Calluna. This tells R that we want to fit a model that accounts for the main effects of pH and Calluna, and that we also wish to include the interaction between these two factors. The specification of the main effects is fairly self-explanatory—we just include the name of each factor variable in the formula. The interaction term is less obvious. It is specified by a colon (the : symbol) with the two interacting variables either side of it. In summary… 1) the ~ symbol specifies a formula in R, where the name on the left hand side is the response variable we are analysing, and the names on the right denote the terms in the model; 2) we place a + between terms to delineate them (we are not adding anything up when the + is used in a formula); 3) each main effect is specified by the corresponding factor name; and 4) an interaction between factors is specified by the : symbol. Notice that we assigned the result a name (festuca_model) which now refers to the model object produced by lm. Just as with a one-way ANOVA, we can’t extract p-values by printing this object to the console, because all this gives us is a limited summary of the fitted model’s coefficients: festuca_model ## ## Call: ## lm(formula = Weight ~ pH + Calluna + pH:Calluna, data = festuca) ## ## Coefficients: ## (Intercept) pHpH5.5 CallunaPresent ## 3.368 3.038 -0.390 ## pHpH5.5:CallunaPresent ## -2.078 We’re not going to worry about what those mean for a two-way ANOVA. We’re going to use anova to calculate things like degrees of freedom, sum of squares, mean squares, F-statistics, and finally, the p-values. But… before we do that, let’s check our assumptions now that we have a fitted model object. 27.5 Diagnostics We’re going to produce two diagnostic plots: a normal probability plot to evaluate the normality assumption, and a scale-location plot allows us to evaluate the constant variance assumption. Here’s the normal probability plot: plot(festuca_model, which = 2, add.smooth = FALSE) This plot allows us to check whether the deviations from the group means (the residuals) are likely to have been drawn from a normal distribution. This looks… not so great. The points deviate from the line in a systematic way so it looks like the normality assumption may not be satisfied. The left tail is above the line and the right tail is below it. This tells us that the tails of the residual distribution do not extend out as far as they should—the distribution is ‘squashed’ toward its middle. The scale-location plot allows us to see whether or not the variability of the residuals is roughly constant within each group. Here’s the plot: plot(festuca_model, which = 3, add.smooth = FALSE) We’re on the lookout for a systematic pattern in the size of the residuals and the fitted values—does the variability go up or down with the fitted values? There is no such pattern so it looks like the constant variance assumption is at least satisfied here. We’ve identified one potential problem. We’ll ignore it for now and press on. The goal here is to learn the work flow for two-way ANOVA in R. However, keep in mind that if we were serious about the analysis we should find a way to ‘fix’ it, for example using the methods we discussed in the Data transformations chapter. 27.6 Interpreting the results Now we’re ready to calculate the degrees of freedom, sums of squares, mean squares, the F-ratio, and p-values for the main effects and the interaction terms: anova(festuca_model) ## Analysis of Variance Table ## ## Response: Weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.9800 19.9800 28.1792 7.065e-05 *** ## Calluna 1 10.2102 10.2102 14.4001 0.00159 ** ## pH:Calluna 1 5.3976 5.3976 7.6126 0.01397 * ## Residuals 16 11.3446 0.7090 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does all this mean? We interpret each line of the ANOVA table in exactly the same way as we do for a one-way ANOVA. The first part tells us what kind of output we are looking at: ## Analysis of Variance Table ## ## Response: Weight This reminds us that we are looking at an ANOVA table where our response variable was called Weight. The table contains the key information: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.9800 19.9800 28.1792 7.065e-05 *** ## Calluna 1 10.2102 10.2102 14.4001 0.00159 ** ## pH:Calluna 1 5.3976 5.3976 7.6126 0.01397 * ## Residuals 16 11.3446 0.7090 This ANOVA table is similar to the ones we have already seen, except that now we have to consider three lines—one for each term in the model. The first is for the main effect of pH, the second for the main effect of Calluna, the third for the interaction between pH and Calluna. The F-ratio is the test statistic for each term. These provide a measure of how large and consistent the effects associated with each term are. Each F-ratio has a pair of degrees of freedom associated with it: one belonging to the term itself, the other due to the error (residual). Together, the F-ratio and its degrees of freedom determines the p-value. The p-value gives the probability that the differences between the set of means for each term in the model, or a more extreme difference, could have arisen through sampling variation under the null hypothesis of no difference. We take p &lt; 0.05 as evidence that at least one of the treatments is having an effect. Here, p &lt; 0.05 for three effects, so we conclude both main effects and the interaction are significant (though at different significance levels). The ANOVA table tells us nothing about the direction of the effects. We have to delve a little further into the fitted model or plot the data to be able to do this. The presence of an interaction between treatments indicates that the impact of one factor depends on the levels of the other factor. This means that if there is a significant interaction in a two-way ANOVA, then the main effects should be interpreted with care. An ‘interaction diagram’ provides a good way to think about these issues… 27.6.1 Understanding the model graphically How should we go about interpreting the significant effects? To reiterate, the interaction tells us that the magnitude, or even direction, of the effect of one factor is dependent upon the levels of the other factor. In other words the treatment effects are contingent on one another. This contingency can arise in a number of ways, giving rise to different mixtures of main effects and interactions. This is illustrated most easily by considering some hypothetical results from a pH/Calluna experiment of this sort, in schematic form (the Calluna bars are linked by a dotted line): Diagrams such as these are sometimes called ‘interaction diagrams’ and they are often the best way of looking at the results from this sort of experiment to try and interpret what is happening. You will notice that the lines linking the treatments are parallel when there is no interaction, but become non-parallel when an interaction is present. An interaction may just mildly change the way the main effects work (4th plot) or it might completely reverse the effects (5th plot). We can use R to produce an interaction diagram for a two-way design. We’ll use dplyr and ggplot2 to construct this for the example. First we have to calculate the mean weight of Festuca in each treatment combination: # step 1. calculate means for each treatment combination festuca_means &lt;- festuca %&gt;% group_by(Calluna, pH) %&gt;% # &lt;- remember to group by *both* factors summarise(Means = mean(Weight)) festuca_means ## # A tibble: 4 x 3 ## # Groups: Calluna [?] ## Calluna pH Means ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 Absent pH3.5 3.368 ## 2 Absent pH5.5 6.406 ## 3 Present pH3.5 2.978 ## 4 Present pH5.5 3.938 The key to making the plot is to specify four aesthetic mappings, and then add two layers, one showing points and the other showing lines: # step 2. plot these as an interaction plot ggplot(festuca_means, aes(x = Calluna, y = Means, colour = pH, group = pH)) + geom_point(size = 4) + geom_line() Notice that we mapped pH to two aesthetics: colour and group. This ‘trick’ makes ggplot2 link the levels of pH with lines, each of which gets its own colour. This clearly reveals how the different effects are working. Our interaction plot resembles the 4th hypothetical outcome. It is possible to make some interpretation of the main effects—namely that increase in pH, and removal of Calluna increase Festuca yield. However, the magnitude of these effects is dependent on the other (interaction)—the effect of Calluna is increased at higher pH. 27.7 Multiple comparison tests Obviously, since the main treatments only have two levels there is no need for any multiple comparison tests on the main effects — if there is a difference it must be between the two levels. However, the interaction is significant, so it may be desirable to know which particular treatment combinations differ. Predictably, the work flow is very similar to that applied to a one-way ANOVA model. We could use the TukeyHSD function to do this. We start by converting the model object produced by lm into an aov object… festuca_aov &lt;- aov(festuca_model) …and then we perform a Tukey HSD test: TukeyHSD(festuca_aov, which = &#39;pH:Calluna&#39;) We have suppressed the output for now. The only new tweak that we have to learn is the which argument. Assigning this the value 'pH:Calluna' makes the TukeyHSD function carry out all pairwise comparisons among the means of each treatment combination, i.e., we are considering the full set of interactions. Here is the output: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = festuca_model) ## ## $`pH:Calluna` ## diff lwr upr p adj ## pH5.5:Absent-pH3.5:Absent 3.038 1.5143518 4.5616482 0.0001731 ## pH3.5:Present-pH3.5:Absent -0.390 -1.9136482 1.1336482 0.8826936 ## pH5.5:Present-pH3.5:Absent 0.570 -0.9536482 2.0936482 0.7117913 ## pH3.5:Present-pH5.5:Absent -3.428 -4.9516482 -1.9043518 0.0000443 ## pH5.5:Present-pH5.5:Absent -2.468 -3.9916482 -0.9443518 0.0014155 ## pH5.5:Present-pH3.5:Present 0.960 -0.5636482 2.4836482 0.3079685 You extract information from this table just as you did before. The table present a series of pair-wise comparisons between mean values tested by the Tukey procedure. For example, the first 3 lines show the significance of differences between the mean of the treatment combination pH 3.5 without Calluna and the 3 other mean values. All we need from this table is to note the codes for the treatment means which are being compared (listed in the first column), and the p-value in each case listed in the final column. If we list the mean values in sequence from the lowest to the highest we can then use the results presented in these tables of pair-wise comparisons to derive letter codes to indicate which means differ significantly (at p&lt;0.05) in exactly the same way as you did last week. There are three significant differences, all of which involve the treatment combinations pH 5.5 with Calluna absent. This implies that there are two ‘not significantly different’ groups: one defined by pH 5.5 with Calluna absent, and then everything else. As you might expect, we don’t have to step through the results of the TukeyHSD function to define the ‘not significantly different’ groups. We used the agricolae package to do this for a one-way ANOVA. We can use this again here. We need to load and attach the package first (it may also need to be installed if you are on a university computer): library(agricolae) Once the package is ready for use, we can carry out the Tukey HSD test to find the ‘not significantly different’ groups using the HSD.test function: HSD.test(festuca_aov, trt = c(&quot;pH&quot;, &quot;Calluna&quot;), console = TRUE) ## ## Study: festuca_aov ~ c(&quot;pH&quot;, &quot;Calluna&quot;) ## ## HSD Test for Weight ## ## Mean Square Error: 0.709035 ## ## pH:Calluna, means ## ## Weight std r Min Max ## pH3.5:Absent 3.368 0.9042511 5 2.28 4.43 ## pH3.5:Present 2.978 0.6089089 5 2.39 3.71 ## pH5.5:Absent 6.406 0.9451614 5 5.25 7.45 ## pH5.5:Present 3.938 0.8685448 5 3.04 5.21 ## ## Alpha: 0.05 ; DF Error: 16 ## Critical Value of Studentized Range: 4.046093 ## ## Minimun Significant Difference: 1.523648 ## ## Treatments with the same letter are not significantly different. ## ## Weight groups ## pH5.5:Absent 6.406 a ## pH5.5:Present 3.938 b ## pH3.5:Absent 3.368 b ## pH3.5:Present 2.978 b Setting the trt argument to c(&quot;pH&quot;, &quot;Calluna&quot;) makes the function carry out all pair-wise comparisons among the mean values defined by each treatment combination. The output that matters is the table at the very end, which shows the group identities as letters, the treatment names, and the treatment means. This just reiterates what we already knew—there are two ‘not significantly different’ groups, defined by pH 5.5 with Calluna absent, and ‘everything else’. Multiple comparison tests for main effects As mentioned above, in this experiment there is no point in trying to make further comparisons between the means from the main treatments (pH 3.5 and 5.5, or with and without Calluna) since (a) there is a significant interaction, so detailed comparisons of the main effects are hard to interpret, and (b) even if that was not the case there are only two levels in each treatment so any difference must be between those two levels! However, it is quite common to have experiments with more than two levels in one or both factors. If the ANOVA indicates that there is a significant effect of one, or both, of the associated effects, and there is no interaction to worry about (don’t forget this caveat), then you may wish to carry out multiple comparisons for the means associated with the main effects. This can be done using a Tukey test just as we did for the interaction in this example. The only difference is that we have to specify the name of the main effect you are interested in. For example, if we wanted to use TukeyHSD function to evaluate the significance of the pH main effects, we would use: TukeyHSD(festuca_aov, which = ‘pH’) 27.8 Drawing conclusions and presenting results In the results section of the report we will need to provide a succinct factual summary of the analysis: There were significant effects of soil pH (ANOVA: F=28.18, df=1,16, p&lt;0.001), competition with Calluna (F=14.4, df=1,16, p=0.002) and the interaction between these treatments (F=7.61, df=1,16, p=0.014) on the dry weight yield of Festuca. Festuca grew much better in the absence of Calluna at high pH than in any other treatment combination (Tukey multiple comparison test p&lt;0.05) (Figure 1). For presentation we could tabulate the results, or better still present them as a figure rather like the interaction diagrams we saw earlier. We can of course produce such a figure, though we should include the standard errors of each mean to use it in a report or presentation. We’ll round off this section by looking at how to produce these publication-ready figures. You won’t be assessed on your ability to produce summary plots such as those below. But yes, you should learn how to make them because you will need to produce these kinds of figures in your own projects. The good news is that the figures below are as complicated as things will get in this book. We want to plot some sample statistics (means and standard errors) so we first have to calculate these using dplyr: # step 1. calculate means for each treatment combination festuca_stats &lt;- festuca %&gt;% group_by(Calluna, pH) %&gt;% # &lt;- remember to group by the two factors summarise(Means = mean(Weight), SEs = sd(Weight)/sqrt(n())) festuca_stats ## # A tibble: 4 x 4 ## # Groups: Calluna [?] ## Calluna pH Means SEs ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Absent pH3.5 3.368 0.4043934 ## 2 Absent pH5.5 6.406 0.4226890 ## 3 Present pH3.5 2.978 0.2723123 ## 4 Present pH5.5 3.938 0.3884250 Once we’ve constructed a data frame containing the descriptive statistics we can make a plot: # step 2. plot these as an interaction plot ggplot(festuca_stats, aes(x = Calluna, y = Means, colour = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean geom_point(size = 3) + # this adds the error bars geom_errorbar(width = 0.1) + # controlling the appearance scale_y_continuous(limits = c(2, 7)) + xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) + # use a more professional theme theme_bw() This is very similar to the ggplot2 code used to make the summary figure in the one-way ANOVA example. We set the data argument in ggplot to be the data frame containing the statistics (not the original raw data), and this time, we set up five aesthetic mappings: x, y, colour, ymin and ymax. We use the colour aesthetic to delineate the levels of pH. We added two layers: one layer is added with geom_point to include the individual points based on the x and y mappings; the second layer is added with geom_errorbar to include the error bars based on the x, ymin and ymax mappings. 27.8.1 A little more customisation It is not uncommon to find that two or more means are quite close to one another, and as a result, plotted points and/or standard errors overlap. We can tweak a figure to avoid this by moving the plotted points a little to one side. The trick is to use the position_dodge function to define a ‘position adjustment’ object, and then associated this with position arguments in geom_errorbar and geom_point: # define a position adjustment pos &lt;- position_dodge(0.15) # make the plot ggplot(festuca_stats, aes(x = Calluna, y = Means, colour = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean (shift positions with &#39;position =&#39;) geom_point(size = 3, position = pos) + # this adds the error bars (shift positions with &#39;position =&#39;) geom_errorbar(width = 0.1, position = pos) + # controlling the appearance scale_y_continuous(limits = c(2, 7)) + xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) + # use a more professional theme theme_bw() Look at the new positions of the points and error bars at each level of Calluna—they have shifted very slightly to the left and right. We don’t need to do this in the Calluna example of course, because there is no overlap to deal with. If the points/SEs had overlapped, this would now be avoided because the data would be plotted side-by-side. We could also use a bar chart with error bars to summarise the data. You only have to change one thing about the last chunk of ggplot2 code to make this. You can probably guess how to do this—instead of using a geom_point, we use geom_col… ggplot(festuca_stats, aes(x = Calluna, y = Means, fill = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean geom_col(position = position_dodge()) + # this adds the error bars geom_errorbar(position = position_dodge(0.9), width=.2) + # controlling the appearance xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) The only other other trick we need to apply is to set the position arguments of geom_errorbar and geom_bar using position_dodge. If we don’t do this, ggplot2 will produce a stacked bar chart and the error bars will end up in the wrong place (the value of 0.9 used in position_dodge(0.9) ensures the error bars appear in the centre of each bar). 27.9 Balanced or orthogonal designs We’re going to finish this chapter with a small warning. In an ideal world, for ANOVA with two or more factors the experiment should be designed such that we have: 1) every possible combination of treatments represented, e.g. pH and Calluna factors each have two levels, so the experiment should have four treatment combinations; 2) equal numbers of replicates in each combination of treatments, e.g. all the cells in the data table for pH and Calluna treatments have equal numbers of data values. This leads to what is called a balanced, orthogonal experiment. The word balanced refers to the ‘equal numbers of replicates in each combination’ aspect of the experiment. The word orthogonal refers to the ‘every possible combination of treatments’ aspect of the experiment. The analysis workflow that we learn in this book assumes a balanced, orthogonal experimental design. We put that in bold because it is a really important point to remember. If at all possible, aim for a balanced, orthogonal experimental design. This makes life much easier when the time comes to analyse the data. The workflow we’re learning is only appropriate when using balanced, orthogonal design. If our experimental design does not meet these conditions, it is not necessarily a problem, but we need to consider its limitations. What are these? Here’s the key one: we can’t just take the fitted model object and pass it to the anova function to carry out the significance tests. We have to be a lot more careful than that. "],
["introduction-to-ancova.html", "Chapter 28 Introduction to ANCOVA 28.1 Introduction 28.2 Why do we need ANCOVA models? 28.3 How does ANCOVA work? 28.4 Degrees of freedom, mean squares and F-statistics 28.5 Assumptions of ANCOVA", " Chapter 28 Introduction to ANCOVA 28.1 Introduction One-way ANOVA allows us to determine whether there are significant differences between the effects of two or more treatments, while two-way ANOVA allows us to combine two factors in a single experiment to explore the main effects of each treatment and their interaction. We can think of ANOVA as a type of statistical model in which the predictor variables are categorical. We have also seen that simple linear regression is used to describe the relationship between two numeric variables; it tells us how the response variable changes in response to the predictor variable. When described like this, ANOVA and regression do not seem all that different. The only thing that distinguishes them is the type of predictor variable they accommodate. An obvious question is then, what happens if we have a mixture of categorical and numeric predictor variables? In this situation we use Analysis of Co-variance (ANCOVA) to analyse our data. Two-way ANCOVA is used when there are two predictor variables: one categorical variable and one numeric variable. 28.2 Why do we need ANCOVA models? Let’s consider an example of when an ANCOVA might be useful. Following a period of recovery from overhunting, sea otter (Enhydra lutris) populations in western Alaska began declining in 1990s. A team of marine scientists hypothesised that rising killer whale abundances—leading to greater rates of predation—was driving this change. They set out to evaluate the impact of killer whales on sea otter populations at Plah Island, Alaska, by contrasting sea otter trends between Holdi Lagoon, an area inaccessible to killer whales, and the adjacent Handae Bay, an open coastal environment. Location Year 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 Holdi Lagoon 178 176 163 174 161 173 167 177 170 154 148 167 Handae Bay 237 220 212 225 235 198 219 211 198 202 201 185 Notice that these data resemble a fully factorial, two-way ANOVA design (but without replication), where the predictors are year and location. Two-way ANOVA and ANCOVA are quite similar in many respects. We could plot these data by creating two different subsets (e.g. using the dplyr function filter)—one for each location—and plotting each in turn as a separate scatter plot. However, unless the abundances of sea otters are very different, it is better to plot everything on the same figure so the abundances at each location can be compared. We need to ensure the data from each location are clearly delineated. We can do this is a number of ways, for example by giving the points different colours as follows. Variables and axes Just as with a regression analysis you should be careful here to make sure the two variables are plotted the right way around with respect to the x and y axes: place the response variable on the y axis and the numeric predictor on the x axis. The groups defined by the categorical variable are given different colours (or shapes, or panels, etc). This indicates that sea otter abundances have declined in both locations, and as predicted, the decline seems to be greater where sea otters were exposed to predation from killer whales (the bay location). How should we analyse these data? It might be tempting to carry out two separate linear regressions of otter abundance against study year. If we do this when we consider just the data from the lagoon population, we do not find a significant effect of year on otter abundance (p=0.062), but we do find a significant effect of year in the bay population (p&lt;0.01). It is tempting to conclude that because there is a significant effect of year on otter abundance in one location but not the other, the population trends are ‘different’. This reasoning is flawed. In fact, if we we want to establish whether or not the trend is different at each location, we have to have to analyse them together. This is where ANCOVA becomes useful. 28.3 How does ANCOVA work? We can think of two-way ANCOVA as a cross between simple regression and two-way ANOVA. Similar to simple regression, a two-way ANCOVA finds straight-line relationships that best describes the response of one variable (the response variable) on another (the numeric predictor variable), but does so for separate groups of data (defined by the categorical predictor variable). Selecting which variable is to be used as the response and which as the predictor variable is usually straightforward. In our example, the investigators set out to understand how otter densities change with respect to time, and so logically, otter abundance depends on year. It is very hard to imagine a situation whereby it make sense to predict study year from otter abundances. An ANCOVA describes, within each group, how the response variable changes with a unit change in the value of the predictor variable. ANCOVA works in essentially the same way as ANOVA or regression. It seeks the set of lines (one for each group) that leaves the smallest total sum of the squared residuals. Remember, residuals are vertical distances between a fitted line and each data point, measured parallel to the \\(y\\)-axis. Just as with regression, the residuals are the ‘bits left over’ after a line has been fitted. The size of the residuals gives an indication of how well the line fits the data. The following illustration shows the best fit model and associated residuals for our example: A two-way ANCOVA on these data will tell us whether otter abundance is related to (1) study year (i.e. the trend) and (2) study site, and (3) whether the effect of year depends on study site. So just like two-way ANOVA, there are three terms to consider: two main effects and one interaction. 28.4 Degrees of freedom, mean squares and F-statistics The statistical significance of the main effects and the interaction are evaluated by examining changes in sums of squares. The statistical significance of each of these terms is evaluated by calculating how much variability in the response variable is ‘explained’ by each term in the model. In practise, this is done by calculating a sum of squares for each term. We can get a sense of how these calculations work by contrasting the residuals associated with different ‘models’ of the data. To understand the location term, we contrast the residuals associated with the overall grand mean (left panel) with those associated with separate means for each location (right panel): It seems clear from this plot that adding a location effect (i.e. allowing for different means) explains a lot of variation, because the residuals (i.e. the lengths of the vertical lines) are much smaller in the right panel. What about the effect of year? To understand the year term, we visualise the residuals associated with separate means for each location (left panel) vs. the residuals when we include a separate mean for each location and a common year slope: The phrase ‘common slope’ refers to the fact that we found the best fit model under a constraint, whereby the year slope was forced to be the same in each location. Adding a common year effect explains some variation, though the change in the size of the residuals is clearly smaller than when we added the location effect. Finally, to understand the interaction term, we can compare the residuals generated when a separate mean for each location and a common year slope is included (left panel), to the case that includes a location-specific mean and year slope (right panel): Adding the interaction only explains a small amount of variation, i.e. the change in the size of the residuals is even smaller than when we added the year term. We could take this analysis further by actually calculating the change in sum of squares that occurs as we add a term. However, at this point, we really just want to get a sense of how ANCOVA works using a visual assessment of the change in the residuals. In terms of the variability they explain, it looks like the order is… main effect of location &gt;&gt; main effect of year &gt;&gt; interaction (N.B. To do this analysis ‘properly’ we should also account for the degrees of freedom associated with each term. R will do this for us though, so we won’t worry about the degrees of freedom now.) 28.5 Assumptions of ANCOVA The assumptions underlying analysis of co-variance are no different from regression and ANOVA: Independence. The residuals must be independent. Measurement scale. The response (\\(y\\)) and the numeric predictor variable (\\(x\\)) variable are measured on interval or ratio scales. Linearity The relationship between the numeric predictor variable (\\(x\\)) and the response (\\(y\\)) variable is linear within each group. Normality. The residuals are drawn from a normal distribution. Constant variance. The variance of the residuals is constant. Measurement error. The values of the numeric predictor variable (\\(x\\)) are determined with negligible error. We will not go through each of these in detail because their meaning should be obvious by this point in the course. Unbalanced or non-orthogonal designs Similar to ANOVA, in an ideal world the data used for ANCOVA should be balanced and orthogonal, i.e., every possible combination of values of the independent variables is represented (orthogonal), with equal numbers of observations in each combination (balanced). The analysis workflow that we teach you in this course assumes a balanced, orthogonal design. ANCOVA crops up a lot in observational studies, where it is frequently difficult (or impossible) to ensure these conditions are met. If your experimental design or data collection protocol is not balanced and orthogonal, you should speak to someone who has a good knowledge of statistics before you collect any data to obtain guidance about how to analyse the resulting data. "],
["two-way-ancova-in-r.html", "Chapter 29 Two-way ANCOVA in R 29.1 Introduction 29.2 Visualising the data 29.3 Fitting an ANCOVA 29.4 Diagnostics 29.5 Interpreting the results 29.6 Presenting the results", " Chapter 29 Two-way ANCOVA in R 29.1 Introduction We’ll use the sea otter predation example from the previous chapter to walk through how to carry out an ANCOVA in R. Walk through example You should begin working through the example from this point. You need to download the OTTERS.CSV file from MOLE and place it in your working directory. Read the data into an R data frame, giving it the name seaotters. Make sure you have a look at the data before you proceed. The data are laid out with the response variable in one column and two additional columns for the predictor variables: one contains the codes for the categorical variable, the other contains the values of the numeric variable. Thus, the first column (Otters) contains the sea otter abundances, the second column (Location) contains the codes for the study population (levels: ‘Lagoon’ and ‘Bay’), and the third column (Year) contains the observation year (1993-2004). 29.2 Visualising the data As always we should start by visualising our data. ggplot(seaotters, aes(x = Year, y = Otters, colour = Location)) + geom_point() As we noted in the previous chapter the figure suggestes that sea otter abundances have declined in both locations, with a greater decline where sea otters were exposed to predation from killer whales (the bay location). We should also consider the assumptions of the ANCOVA. The scatter plot suggests that, within each location, the relationship between \\(x\\) and \\(y\\) is linear. The numeric predictor variable (study year) is measured on an interval scale and the response variable (otter abundance) is measured on ratio scale. Year is obviously measured without error. What about the independence assumption? Independence Can you think of any reasons why the independence assumption may be problematic in this example? Think about how the data have been collected—they are a time series of abundances in a pair of adjacent populations. We’ll assume the independence assumption has been met in these data. As with regression, the remaining assumptions are probably best measured using regression diagnostics after we’ve fitted the model. 29.3 Fitting an ANCOVA As you should expect by this point, carrying out ANCOVA in R is a two step process. The first step is the model fitting step. This is where R calculates the best fit intercepts and slopes for each group (i.e. each location in this example), along with additional information needed to carry out the evaluation of significance in step two. We carry out the model fitting step using the lm function: otters.model &lt;- lm(Otters ~ Location + Year + Location:Year, data = seaotters) This is just more of the same old model fitting with lm. We assigned two arguments: The first argument is a formula. The variable name on the left of the ~ must be the response variable (Otters) and the terms on the right must only include the two predictor variables (Location and Year). The second argument is the name of the data frame that contains the variables listed in the formula (seaotters). Let’s unpack the formula we used: Otters ~ Location + Year + Location:Year There are three terms, each separated by a + symbol: the two main effects (Location and Year) and their interaction (Location:Year). This tells R that we want to fit a model accounting for the main effects of study location and year, but that we also wish to include the interaction between these two variables. The Location term allows each line to cross the y-axis at a different point, the Year term allows the effect of year (the slope) to be non-zero, and the interaction term allows this slope to be different in each location. How does R knows we want to carry out ANCOVA? Notice how similar fitting this ANCOVA was to fitting a two-way ANOVA. How does R know we want to use ANCOVA? You should be able to answer this question. R looks at what type of variables are on the right had side of the ~ in the formula. Since Location is a factor and Year is numeric, R fits an ANCOVA model. If both variables had been factors we fit a two-way ANOVA, and if both variables were numeric we would fit something called a multiple regression model. 29.4 Diagnostics Before we go on to look at the p-values we should check the remaining assumptions using the diagnostics. We’ll make the same plots as if we’d fitted a linear regression. First we’ll evaluate the linearity assumption by constructing a residuals vs. fitted values plot. plot(otters.model, add.smooth = FALSE, which = 1) There’s no evidence of a systematic trend here so the linearity assumption is fine. We’ll move on to the normality assumption next, by making a normal probability plot. plot(otters.model, which = 2) This doesn’t look great, very few of the points are on the dashed line and there appears to be a systematic trend away from the line. We’ll carry on for now as we’re just using this as an example of how to carry out an ANCOVA. If we were really interested in the results of this analysis we should consider transforming our response variable. Normality assumption Can you think of any reason that we might expect the residuals in these data not to be normally distributed? What kind of transformation might help? Finally, we’ll consider the constant variance assumption using the scale location plot. plot(otters.model, add.smooth = FALSE, which = 3) Here, we’re on the lookout for a systematic pattern in the size of the residuals and the fitted values—does the variability go up or down with the fitted values? There doesn’t appear to be a strong pattern here. 29.5 Interpreting the results Next, we use the anova function to determine whether the main effects and the interaction are significant, by passing it the name of the fitted regression model object (otters.model): anova(otters.model) ## Analysis of Variance Table ## ## Response: Otters ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Location 1 11926.0 11926.0 139.3894 1.811e-10 *** ## Year 1 1745.3 1745.3 20.3982 0.0002106 *** ## Location:Year 1 299.1 299.1 3.4964 0.0762142 . ## Residuals 20 1711.2 85.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first line reminds us that we are looking at an ANOVA table. Remember, this doesn’t necessarily mean we are dealing with an ANOVA model—we are definitely examining an ANCOVA here. The second line reminds us what variable we analysed (i.e., the response variable). The critical part of the output is the table at the end: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Location 1 11926.0 11926.0 139.3894 1.811e-10 *** ## Year 1 1745.3 1745.3 20.3982 0.0002106 *** ## Location:Year 1 299.1 299.1 3.4964 0.0762142 . ## Residuals 20 1711.2 85.6 This summarises the parts of the analysis of variance calculations, as they apply to ANCOVA. These are: Df – degrees of freedom, Sum Sq – the sum of squares, Mean Sq – the mean square, F value – the F-statistic (i.e. variance ratio), Pr(&gt;F) – the p-value). The F-statistics (variance ratios) are the key terms. When working with an ANCOVA, these relate to how much variability in the data is explained when we include each term in the model, taking into account the degrees of freedom it ‘uses up’. Larger values indicate a stronger effect. The p-value gives the probability that the relationship could have arisen through sampling variation, if in fact there were no real association: a p-value of less than 0.05 indicates a less than 1 in 20 chance of the result being due to chance, and we take this as evidence that the relationship is real. We need to interpret these p-values. The two main effects are significant (p&lt;0.001), but the interaction is not (p=0.076). An ANOVA table tells us nothing about the direction of the effects—we have to plot the data to be able to do this. If we look back at the scatter plot, it is apparent that the significant main effects are supporting the observation that otter abundances are higher in the bay area, and that in general, otter abundances have declined over the course of the study. The interaction term is non-significant—though we only just missed the conventional p&lt;0.05 cut off. We are forced to conclude that the data do not support the hypothesis that the population abundances in each location have declined by different amounts. 29.6 Presenting the results We will need to provide a succinct factual summary of the analysis in the results section of the report: There were significant effects of location (ANCOVA: F=139.4, df=1,20, p&lt;0.001) and year (F=20.4; df=1,20; p&lt;0.001) on sea otter abundance. The interaction between location and year was not significant (F=3.5, df=1,20, p=0.076). Sea otter abundances were generally higher in Handae Bay, but declined by a similar amount in both locations during the study (Figure 1). Notice that we never referred to ‘treatments’ in this summary. It does not make any sense to describe the variables in this data set as treatments, as we are describing the results from an observational study. Of course, there is nothing to stop us using ANCOVA to analyse experimental data if it is appropriate. For presentation it is best to present the results as a figure. We can produce publication quality figure to summarise ANCOVA in much the same way as we summarise a fitted regression model. We are aiming to produce a figure that shows two pieces of information: a scatter plot and lines of best fit. We also want to differentiates the data and best fit lines for each location. We know how to produce a scatter plot, so the main challenge is to add the lines of best fit. We use the predict function to do this. To use predict, we have to let R know the set of values of the two predictor variables for which we want predictions (Location and Year). Sea otter abundances were evaluated from 1992 to 2003, so it makes sense to predict their abundances over this range. We also need to make distinct predictions for each location (‘Lagoon’ and ‘Bay’). Therefore, the first step in making predictions is to generate a sequence of values for Year from 1992 to 2003 for each location, placing these alongside a location indicator inside a data frame. We have to use a new function, expand.grid, to do this: pred.data &lt;- expand.grid(Year = 1992:2003, Location = c(&quot;Lagoon&quot;, &quot;Bay&quot;)) Remember that 1992:2003 produces a numeric vector in which the elements are a sequence of integers (‘whole numbers’) running from 1992 to 2003. That probably looks very cryptic at the moment. Look at pred.data: pred.data ## Year Location ## 1 1992 Lagoon ## 2 1993 Lagoon ## 3 1994 Lagoon ## 4 1995 Lagoon ## 5 1996 Lagoon ## 6 1997 Lagoon ## 7 1998 Lagoon ## 8 1999 Lagoon ## 9 2000 Lagoon ## 10 2001 Lagoon ## 11 2002 Lagoon ## 12 2003 Lagoon ## 13 1992 Bay ## 14 1993 Bay ## 15 1994 Bay ## 16 1995 Bay ## 17 1996 Bay ## 18 1997 Bay ## 19 1998 Bay ## 20 1999 Bay ## 21 2000 Bay ## 22 2001 Bay ## 23 2002 Bay ## 24 2003 Bay The expand.grid produced a data frame with two variables called Year and Location. The rows of the data frame represent every pairwise combination of the two sets of values (i.e. the vectors) we passed to expand.grid. Notice that we gave the two arguments of expand.grid the exact same names as the predictor variables in the ANCOVA. This is important: the names in pred.data have to match the names of the predictor variables used in model, and every variable in the model has to be represented in pred.data. The next step is the same as the regression example. Once we have set up a data frame to predict from (pred.data) we are ready to use the predict function. We need to capture the predictions inside a data frame using mutate: pred.data &lt;- mutate(pred.data, Otters = predict(otters.model, pred.data)) Look at the resulting data frame: pred.data ## Year Location Otters ## 1 1992 Lagoon 175.2949 ## 2 1993 Lagoon 173.8473 ## 3 1994 Lagoon 172.3998 ## 4 1995 Lagoon 170.9522 ## 5 1996 Lagoon 169.5047 ## 6 1997 Lagoon 168.0571 ## 7 1998 Lagoon 166.6096 ## 8 1999 Lagoon 165.1620 ## 9 2000 Lagoon 163.7145 ## 10 2001 Lagoon 162.2669 ## 11 2002 Lagoon 160.8193 ## 12 2003 Lagoon 159.3718 ## 13 1992 Bay 231.1282 ## 14 1993 Bay 227.6352 ## 15 1994 Bay 224.1422 ## 16 1995 Bay 220.6492 ## 17 1996 Bay 217.1562 ## 18 1997 Bay 213.6632 ## 19 1998 Bay 210.1702 ## 20 1999 Bay 206.6772 ## 21 2000 Bay 203.1841 ## 22 2001 Bay 199.6911 ## 23 2002 Bay 196.1981 ## 24 2003 Bay 192.7051 Notice that we gave the predictions the same name as the response variable in our ANCOVA model. This is convenient, because it means we don’t have to set up any new aesthetic mappings when we use ggplot2 in a moment. Notice that pred.data is set out just like the data frame containing the study data. It has three columns called Otters, Location and Year, but instead of raw data, it contains predictions from the model. Plotting these predictions along with the data is now very easy: ggplot(pred.data, aes(x = Year, y = Otters, colour = Location)) + geom_line() + geom_point(data = seaotters) + xlab(&quot;Year&quot;) + ylab(&quot;Sea Otter Abundance&quot;) Don’t forget: we have to make ggplot2 use the seaotters data (i.e. the raw data) when adding the points. Let’s summarise what we did: 1) using expand.grid, we made a data frame with two columns containing the values of the predictor variables we want to make predictions at; 2) we then used predict to generate these predictions, adding them to the prediction data with mutate; 3) finally, we used ggplot2 to plot the predicted values of the response variable against the numeric predictor variable, colouring the lines and points differently for each location. This workflow is almost identical to that used to summarise a regression model—the only new trick here was the use of expand.grid to deal with the fact that now we have to manage two predictor variables. "],
["working-with-frequencies.html", "Chapter 30 Working with frequencies 30.1 Introduction 30.2 A new kind of distribution 30.3 Types of test", " Chapter 30 Working with frequencies 30.1 Introduction Much of the time in biology we are dealing with whole objects (plants, animals, cells, eggs, islands, etc.) or discrete events (attacks, matings, nesting attempts, etc.). We are often interested in making measurements of numeric variables (length, weight, number, etc.) and then either comparing means from samples (e.g. mean leaf size of plants from two habitat types), or investigating the association between different measurements (e.g. mean leaf size and herbivore damage). However, we sometimes find a situation in which the ‘measurement’ we are interested in is not a quantitative measure, but is categorical. Categorical data are things like sex, colour or species. Such variables cannot be treated in the same way as numeric variables. Although we can ‘measure’ each object (e.g. record if an animal is male or female), we can’t calculate numeric quantities such as the ‘mean colour morph’, ‘mean species’ or ‘median sex’ of animals in a sample. Instead, we work with the observed frequencies, in the form of counts, of different categories, or combinations of categories. 30.2 A new kind of distribution There are quite a few options for dealing with categorical data23. We’re just going to look at one option in this book: \\(\\chi^2\\) tests. This is pronounced, and sometimes written, ‘chi-square’. The ‘ch’ is a hard ‘ch’, as in ‘character’. This isn’t necessarily the best approach for every problem, but \\(\\chi^2\\) tests are widely used in biology so they are a good place to start. It is not critical that you understand everything in this section. This material is here to help those who like to have a sense of how statistical tests work. You won’t be assessed on it. The \\(\\chi^2\\) tests that we’re going to study borrow their name from a particular theoretical distribution, called… the \\(\\chi^2\\) distribution. We don’t need to study this in much detail. However, just as with the normal distribution and the t-distribution, it can be helpful to know a little bit about it. The \\(\\chi^2\\) distribution pops up a lot in statistics. However, in contrast to the normal distribution, it isn’t often used to model the distribution of a variable we’ve sampled (i.e. ‘the data’). Instead, the \\(\\chi^2\\) distribution is often associated with a test statistic of some kind. The standard \\(\\chi^2\\) distribution is completely described by only one parameter, called the degrees of freedom. This is closely related to the degrees of freedom idea introduced in the chapters on t-tests. The \\(\\chi^2\\) distribution is appropriate for positive-valued numeric variables. Negative values can’t be accommodated. This is because the distribution arises whenever we take one or more normally distributed variables, square these, and then add them up. Let’s take a look at the \\(\\chi^2\\) distribution with one degree of freedom: Figure 30.1: Distribution of a large sample of chi-square distributed variable with one degree of freedom As we just noted, only positive values occur and most of these values lie between about 0 and 10. We can also see that the distribution is asymmetric. It is skewed to the right. So why is any of this useful? Let’s look at the plant morph example again. Imagine that we are able to take repeated samples from a population when the purple morph frequency is 25% . Let’s take repeated samples of 1000 plants each time. If the true frequency is 25% we expect to sample 250 purple plants each time. We’ll call this number the ‘expected value’. We won’t actually end up with 250 plants in each sample because of sampling error. We’ll call this latter number the ‘observed value’. So far we’re not doing anything we haven’t seen before. We’re just trying to see what happens under repeated sampling from a population. Here’s the new bit… Imagine that every time we sample the 1000 plants, we calculate the following test statistic… \\[2*\\frac{(O-E)^{2}}{E}\\] …where \\(O\\) is the observed value and \\(E\\) is the expected value defined above. What does the distribution of this test statistic look like? We can find out by simulating the scenario in R and plotting the results: Figure 30.2: Distribution of the test statistic That looks a lot like the theoretical \\(\\chi^2\\) distribution we plotted above. It turns out that observed frequencies (‘counts’) that have been standardised with respect to their expected values—via the \\(\\frac{(O-E)^{2}}{E}\\) statistic—have a \\(\\chi^2\\) sampling distribution (at least approximately). This result is the basis for using the \\(\\chi^2\\) distribution in various statistical tests involving categorical variables and frequencies. 30.3 Types of test We’re going to learn about two different types of \\(\\chi^2\\) test. Although the two tests work on the same general principle, it is still important to distinguish between them according to where they are used. 30.3.1 \\(\\chi^{2}\\) goodness of fit test A goodness-of-fit test is applicable in a situation where we have a single categorical variable and some hypothesis from which we can predict the expected proportions of observations falling in each category. For example, we might want to know if there is any evidence for sex-related bias in the decision to study biology at Sheffield. We could tackle this questions by recording the numbers of males and females in a cohort. This would produce a sample containing one nominal variable (Sex) with only two categories (Male and Female). Based on information about human populations, we know that the sex ratio among 18 year olds is fairly close to 1:124. We are thus able to compare the goodness of fit of the number of males and females in a sample of students with the expected value predicted by the 1:1 ratio. If we had a total of 164 students we might get this sort of table: Male Female Observed 64 100 With a 1:1 sex ratio, if there is no sex-bias in the decision to study biology, we would expect 82 of each sex. In this case it looks as though there may be some discrepancy between the expected values and those actually found. This discrepancy could be entirely consistent with sampling variation—perhaps females are no more likely to choose biology and we ended up with a higher proportion by chance. The \\(\\chi^{2}\\) goodness of fit test allows us to test how likely it is that such a discrepancy has arisen through sampling variation. 30.3.2 \\(\\chi^{2}\\) contingency table test A contingency table test is applicable in situations where each object is classified according to more than one categorical variable. Contingency table tests are usually used to test whether there is an association between the variables. Consider biology students again. We might be interested in whether eye colour was in any way related to sex. It would be simple to record eye colour (e.g. brown vs. blue) along with the sex of each student in a sample. Now we would end up with a table that had two classifications: Blue eyes Brown eyes Male 44 20 Female 58 42 Now it is possible to compare the proportions of brown and blue eyes among males and females… The total number of males and females are 64 and 100, respectively. The proportion of males with brown eyes is 20/64 = 0.31, and that for females 42/100 = 0.42. It appears that brown eyes are somewhat less prevalent among males. A contingency table test will tell us if the difference in eye colour frequencies is likely to have arisen through sampling variation. Notice that we are not interested in judging whether the proportion of males, or the proportion of blue-eyed students, are different from some expectation. That’s the job of a goodness of fit test. We want to know if there is an association between eye colour and sex. That’s the job of a contingency table test. 30.3.3 The assumptions and requirements of \\(\\chi^{2}\\) tests It’s important to realise that in terms of their assumptions, contingency tables and goodness-of-fit tests aren’t fundamentally different from one another. The difference between the two types lies in the type of hypothesis evaluated. When we carry out a goodness-of-fit test we have to supply the expected values, whereas the calculation of expected values is embedded in the formula used to carry out a contingency table test. That will make more sense once we’ve seen the two tests in action. \\(\\chi^{2}\\) tests are often characterised as non-parametric tests because they do not assume any particular form for the distribution of the data. In fact, as with any statistical test, there are some assumptions in play, but these are relatively mild: The data are independent counts of objects or events which can be classified into mutually exclusive categories. The expected counts are not very low. The general rule of thumb is that the expected values should be greater than 5. The most important thing to remember about \\(\\chi^{2}\\) tests is that they must always be carried out on the actual counts. Although the \\(\\chi^{2}\\) is really telling us how the proportions of objects in categories vary, the analysis should never be carried out on the percentages or proportions, only on the original count data, nor can a \\(\\chi^{2}\\) test be used with means. e.g. the ‘log-linear model’, ‘Fisher’s exact test’, and the ‘G-test’.↩ Human sex-ratio is actually slightly biased toward males at birth, but since males experience a higher mortality rate in their teens, the sex ratio among 18 year olds is closer to 1:1.↩ "],
["goodness-of-fit-tests.html", "Chapter 31 Goodness of fit tests 31.1 When do we use a chi-square goodness of fit test? 31.2 How does the chi-square goodness of fit test work? 31.3 Carrying out a chi-square goodness of fit test in R 31.4 Determining appropriate expected values", " Chapter 31 Goodness of fit tests 31.1 When do we use a chi-square goodness of fit test? A \\(\\chi^{2}\\) goodness of fit test is appropriate in situations where we are studying a categorical variable and we want to to compare the frequencies of each category to pre-specified, expected values. Here are a couple of examples: We’ve already seen one situation where a goodness of fit test might be useful: the analysis of the sex ratio among biology undergraduates. We have a prior prediction about what the sex ratio should be in the absence of bias (1:1), and we wanted to know if there was any evidence for sex-related bias in the decision to study biology. We can use a goodness of fit test to compare of the number of males and females in a sample of students with the predicted values to determine whether the data are consistent with the equal sex ratio prediction. Red campion (Silene dioica) has separate male (stamen bearing) and female (ovary and stigma bearing) plants. Both sexes can be infected by the anther smut Ustillago violacea. This smut produces spores in the anthers of the plant which are then transported to other host plants by insect vectors. On infecting the female flowers, Ustillago causes their sex to change, triggering stamen development in the genetically female flowers. In populations of Silene in which there is no infection by Ustillago the ratio of male to female flowers is 1:1. Significant amounts of infection by the fungus may be indicated if there is an increase in the proportion of apparently male flowers relative to the expected 1:1 ratio. The two examples considered above are about as simple as things get: there are only two categories (Males and Females) and we expect a 1:1 ratio. However, the \\(\\chi^{2}\\) goodness of fit test can be employed in more complicated situations… The test can be applied to any number of categories. For example, we might have a diet choice experiment where squirrels are offered four different food types in equal proportions, and the food chosen by each squirrel is recorded. The study variable would then have four categories: one for each food type. The expected ratio need not be 1:1. For example, the principles of Mendelian genetics predict that the offspring of two plants which are heterozygous for flower colour (white recessive, pink dominant) will be either pink or white flowered, in the ratio 3:1. Plants from a breeding experiment could be tested against this expected ratio. 31.2 How does the chi-square goodness of fit test work? The \\(\\chi^{2}\\) goodness of fit test uses raw counts to address questions about expected proportions, or probabilities of events25. As always, we start by setting up the appropriate null hypothesis. This will be question specific, but it must always be framed in terms of ‘no effect’ or ‘no difference’. We then work out what a sampling distribution of some kind looks like under this null hypothesis, and use this to assess how likely the observed result is (i.e. calculate a p-value). We don’t need to work directly with the sampling distributions of counts in each category. Instead, we calculate an appropriate \\(\\chi^{2}\\) test statistic. The way to think about this is that the \\(\\chi^{2}\\) statistic reduces the information in the separate category counts down to a single number. Let’s see how the \\(\\chi^{2}\\) goodness of fit test works using the Silene example discussed above. Imagine that we collected data on the frequency of plants bearing male and female flowers in a population of Silene dioica: Male Female Observed 105 87 We want to test whether the ratio of male to female flowers differs significantly from that expected in an uninfected population. The ‘expected in an uninfected population’ situation is the null hypothesis for the test. Step 1. Calculate the counts expected when the null hypothesis is correct. This is the critical step. In the Silene example, we need to work out how many male and female plants we expected to sample if the sex ratio really were 1:1. These numbers are: (105 + 87)/2 = 192/2 = 96 of each sex. Step 2. Calculate the \\(\\chi^{2}\\) test statistic from the observed and expected counts. We will show you how to do this later. However, this calculation isn’t all that important, in the sense that we don’t learn much by doing it. The resulting \\(\\chi^{2}\\) statistic summarises—across all the categories—how likely the observed data are under the null hypothesis. Step 3. Compare the \\(\\chi^{2}\\) statistic to the theoretical predictions of the \\(\\chi^{2}\\) distribution to assess the statistical significance of the difference between observed and expected counts. The interpretation of this p-value in this test is the same as for any other kind of statistical test: it is the probability we would see the observed frequencies, or more extreme values, under the null hypothesis. 31.2.1 Assumptions of the chi-square goodness of fit test Let’s remind ourselves about the assumptions of the \\(\\chi^{2}\\) goodness of fit test: The data are independent counts of objects or events which can be classified into mutually exclusive categories. We shouldn’t aggregate Silene sex data from different surveys unless we were absolutely certain each survey had sampled different populations. The expected counts are not too low. The rule of thumb is that the expected values (not the observed counts!) should be greater than 5. If any of the expected values are below 5 the p-values generated by the test start to become less reliable. 31.3 Carrying out a chi-square goodness of fit test in R Work through the example in this section. Let’s see how to use R to carry out a \\(\\chi^{2}\\) goodness of fit test with the Silene sex data. There is no need to download any data for this example. The data used in a \\(\\chi^{2}\\) goodness of fit test are so simple that we often just place it into an R script, though there is nothing stopping us from putting the data into a CSV file and reading it into R26. The first step is to construct a numeric vector containing the observed frequencies for each category. We’ll call this observed_freqs: observed_freqs &lt;- c(105, 87) observed_freqs ## [1] 105 87 Note that observed_freqs is a numeric vector, not a data frame. It doesn’t matter what order the two category counts are supplied in. Next, we need to calculate the expected frequencies of each category. Rather than expressing these as counts, R expects these to be proportions. We need to construct a second numeric vector containing this information. We’ll call this expected_probs: # calculate the number of categories n_cat &lt;- length(observed_freqs) # calculate the number in each category (equal frequencies in this e.g.) expected_probs &lt;- rep(1, n_cat) / n_cat expected_probs ## [1] 0.5 0.5 Finally, use the chisq.test function to calculate the \\(\\chi^{2}\\) value, degrees of freedom and p-value for the test. The first argument is the numeric vector of observed counts (the data) and the second is the expected proportions in each category: chisq.test(observed_freqs, p = expected_probs) ## ## Chi-squared test for given probabilities ## ## data: observed_freqs ## X-squared = 1.6875, df = 1, p-value = 0.1939 The vectors containing the data and expected proportions have to be the same length for this to work. R will complain and throw an error otherwise. The output is straightforward to interpret. The first part (Chi-squared test for given probabilities) reminds us what kind of test we did. The phrase ‘for given probabilities’ is R-speak informing us that we have carried out a goodness of fit test. The next line (data: observed_freqs) reminds us what data we used. The final line is the one we care about: X-squared = 1.6875, df = 1, p-value = 0.1939. This output shows us the \\(\\chi^{2}\\) test statistic, the degrees of freedom associated with the test, and the p-value. Since p &gt; 0.05, we conclude that the sex ratio is not significantly different from a 1:1 ratio. Degrees of freedom for a χ2 goodness of fit test We need to calculate the degrees of freedom associated with the test: in a χ2 goodness-of-fit test these are n − 1, where n − 1 is the number of categories. This comes from the fact that we have to calculate one expected frequency per category (= n frequencies), but since the frequencies have to add up to the total number of observations, once we know n − 1 frequencies the last one is fixed. 31.3.1 Summarising the result Having obtained the result we need to write the conclusion. As always, we always go back to the original question to write the conclusion. In this case the appropriate conclusion is: The observed sex ratio of Silene dioica does not differ significantly from the expected 1:1 ratio (\\(\\chi^{2}\\) = 1.69, d.f. = 1, p = 0.19). 31.3.2 A bit more about goodness of fit tests in R There is a useful short cut that we can employ when we expect equal numbers in every category (as above). In this situation there is no need to calculate the expected proportions because R will assume we meant to use the ‘equal frequencies’ null hypothesis. So the following… chisq.test(observed_freqs) ## ## Chi-squared test for given probabilities ## ## data: observed_freqs ## X-squared = 1.6875, df = 1, p-value = 0.1939 …is exactly equivalent to the longer method we used first. We showed you the first method because we do sometimes need to carry out a goodness of fit test assuming unequal expected values. R will warn us if it thinks the data are not suitable for a \\(\\chi^{2}\\) test. It is often safe to ignore the warnings produced by R. However, this is one situation where it is important to pay attention to the warning. We can see what the important warning looks like by using chisq.test with a fake data set: chisq.test(c(2,5,7,5,5)) ## Warning in chisq.test(c(2, 5, 7, 5, 5)): Chi-squared approximation may be ## incorrect ## ## Chi-squared test for given probabilities ## ## data: c(2, 5, 7, 5, 5) ## X-squared = 2.6667, df = 4, p-value = 0.6151 The warning Chi-squared approximation may be incorrect is telling us that there might be a problem with the test. What is the problem? The expected counts are below 5, which means the p-values produced by chisq.test will not be reliable. 31.3.3 Doing it the long way… It’s fine to skip this section if you’re not the kind of person who likes to know how things work. Nobody really does a \\(\\chi^{2}\\) test by hand these days. Nonetheless, just to prove that R isn’t really doing anything too clever, let’s work through the calculations involved in goodness of fit test. The \\(\\chi^{2}\\) test statistic is found by taking the difference of each observed and expected count, squaring these differences, dividing each of these squared differences by the expected frequency, and finally, summing these numbers over the categories. That’s what this formula for the \\(\\chi^{2}\\) test statistic means: \\[\\chi^{2}=\\sum\\frac{(O_i-E_i)^{2}}{E_i}\\] In this formula the \\(O_i\\) are the observed frequencies, the \\(E_i\\) are the expected frequencies, the \\(i\\) in \\(O_i\\) and \\(E_i\\) refer to the different categories, and the \\(\\Sigma\\) means summation (‘add up’). Here’s how to apply this to our example: Work out how many male and female plants we would have sampled on average if the sex ratio really were 1:1. We already did this—the numbers are: (105 + 87)/2 = 192/2 = 96 of each sex. Calculate the \\(\\frac{(O_i-E_i)^2}{E_i}\\) term associated with each category. For the males, we have (105-96)2 / 96 = 0.844, and for females we (87-96)2 / 96 = 0.84427. The \\(\\chi^{2}\\) statistic is the sum of these two values. Here’s some R code that does this for us: Calculate observed and expected frequencies** O_freqs &lt;- c(105, 87) E_freqs &lt;- rep(sum(O_freqs)/2, 2) Calculate \\(\\chi^{2}\\) test statistic Chi_test_stat &lt;- sum((O_freqs-E_freqs)^2 / E_freqs) Chi_test_stat ## [1] 1.6875 Calculate degrees of freedom Chi_df &lt;- length(O_freqs) - 1 Chi_df ## [1] 1 Once we have obtained the \\(\\chi^{2}\\) value and the degrees of freedom we need to calculate the p-value associated with these values. Once upon a time we would have looked this up in a table of some kind. It is much easier to let R handle the calculations for us, using a function called pchisq. This does the ‘probability we would see the observed frequencies, or more extreme values’ calculation mentioned above. pchisq takes three arguments: the first is the \\(\\chi^{2}\\) value, the second is the degrees of freedom, and the third says which tail of the distribution it should use. Here’s how to calculate the required p-value from the \\(\\chi^{2}\\) and d.f. values we just calculated: pchisq(Chi_test_stat, Chi_df, lower.tail = FALSE) ## [1] 0.1939309 No surprises there… The p-value is the same as before. 31.4 Determining appropriate expected values Obviously unless we can find some expected values with which to compare the observed counts, a goodness of fit test will be of little use. Equally obviously, by using inappropriate expected values almost anything can be made significant! This means we always need to have a justifiable basis for choosing the expected values. In many cases the experiment can be designed, or the data collected, in such a way that we would expect to find equal numbers in each category if whatever it is we are interested in is not having an effect. At other times the expectation can be generated by knowledge, or prediction, of a biological process e.g. a 1:1 sex ratio, a 3:1 ratio of phenotypes. At other times the expectation may need a bit more working out. We said it before, but it’s worth saying again. Do not apply the goodness of fit test to proportions or means.↩ In general, it is a good idea to keep data and R code separate, but we sometimes break this rule for simple analyses.↩ These are the same because there are only two categories in this example.↩ "],
["contingency-tables.html", "Chapter 32 Contingency tables 32.1 When do we use a chi-square contingency table test? 32.2 How does the chi-square contingency table test work? 32.3 Carrying out a chi-square contingency table test in R 32.4 Working with larger tables", " Chapter 32 Contingency tables 32.1 When do we use a chi-square contingency table test? A \\(\\chi^{2}\\) contingency table test is appropriate in situations where we are studying two or more categorical variables—each object is classified according to more than one categorical variable—and we want to evaluate whether categories are associated. Here are a couple of examples: Going back to the data on biology students, we suggested that we might want to know if eye colour was related to sex in any way. That is, we want to know whether brown and blue eyes occur in different proportions among males and females. If we recorded the sex and eye colour of male and female students we could use a \\(\\chi^{2}\\) contingency table test to evaluate whether eye colour is associated with sex. The two-spot ladybird (Adalia bipunctata) occurs in two forms: the typical form, which is red with black spots and the dark form, which has much of the elytral surface black, with the two spots red. We want to know whether the relative frequency of the colour morphs is different in industrial and rural habitats. We could address this question by applying a \\(\\chi^{2}\\) contingency table test to an aggregate sample of two-spot ladybirds taken from rural and industrial habitats. Let’s think about what these kinds of data look like. Here are the biology student sex and eye colour data again, organised into a table: Blue eyes Brown eyes Male 22 10 Female 29 21 This is called a two-way contingency table. It is a two-way contingency table because it summarises the frequency distribution of two categorical variables at the same time28. If we had measured three variables we would have ended up with a three-way contingency table (e.g. 2 x 2 x 2). A contingency table takes its name from the fact that it captures the ‘contingencies’ among the categorical variables: it summarises how the frequencies of one categorical variable are associated with the categories of another. The term association is used here to describe the non-independence of categories among categorical variables. Other terms used to refer to the same idea include ‘linkage’, ‘non-independence’, and ‘interaction’. Associations are evident when the proportions of objects in one set of categories (e.g. R1 and R2) depends on a second set of categories (e.g. C1 and C2). Here are two possibilities: Table 32.1: Contingency tables without an association (left table), and with an association (right table), among two categorical variables C1 C2 R1 10 20 R2 40 80 C1 C2 R1 10 80 R2 40 20 In the first table (left) there is no association: the numbers in category R1 are 1/4 of those in category R2, whether the observations are in category C1 or C2. Notice that this reasoning isn’t about the total numbers in each category—there are 100 category C2 cases and only 50 category C1 cases. In the second table (right) this is evidence of an association: the proportion of observations in R1 changes markedly depending on whether we are looking at observations for category C1 or category C2. The R1 cases are less frequent in the C1 column, relative to the C2 column. Again, it is the proportions that matter, not the raw numbers. A variety of different questions can be asked of data in a contingency table, but they are usually used to test for associations between the variables they summarise. That’s the topic of this chapter. There are different ways to carry out such tests of association. We’ll focus on the most widely used tool—the ‘Pearson’s Chi Square’ (\\(\\chi^{2}\\)) contingency table test29. 32.2 How does the chi-square contingency table test work? The \\(\\chi^{2}\\) contingency table test uses data in the form of a contingency table to address questions about the dependence between two or more different kinds of outcomes, or events. We start to tackle this question by setting up the appropriate null hypothesis. The null hypothesis is always the same for the standard contingency table test of association: it proposes that the different kinds of events are independent of one another. This means the occurrence of one kind of event does not depend on the other kind of event, i.e. they are not associated. Once the null hypothesis has been worked out, the remaining calculations are no different from those used in a goodness of fit test. We calculate the frequencies expected in each cell under the null hypothesis, we calculate a \\(\\chi^{2}\\) test statistic to summarise the mismatch between observed and expected values, and then use this to assess how likely the observed result is under the null hypothesis, resulting in the p-value. We’ll continue with the two-spot ladybird (Adalia bipunctata) example from the beginning of this chapter. Here’s a bit more information… The dark (melanic) form is under the control of a single gene. Melanic and red types occur at different frequencies in different areas. Two observations are pertinent to this study: In London melanics comprise about 10%, whereas in rural towns in northern England the frequency is greater (e.g. Harrogate 63%, Hexham 75%). The frequency of melanics has decreased in Birmingham since smoke control legislation was introduced. It was thought that the different forms might be differentially susceptible to some toxic component of smoke, but this doesn’t explain the geographic variation in proportions of melanics. It turns out that the effect is a rather subtle one in which melanic forms do rather better in conditions of lower sunshine than red forms, due to their greater ability to absorb solar radiation. So where the climate is naturally less sunny melanics are favoured, though there will also be smaller scale variations due to local environmental conditions such as smoke, that affect solar radiation. To test whether this effect still occurs in industrial areas, a survey was carried out of Adalia bipunctata in a large urban area and the more rural surrounding areas. The following frequencies of different colour forms were obtained. Black Red Totals Rural 30 70 100 Industrial 115 85 200 Totals 145 155 300 We want to test whether the the proportions of melanics are different between urban and rural areas. In order to make it a bit easier to discuss the calculations involved we’ll refer to each cell in the table by a letter… Black Red Totals Rural \\(a\\) \\(b\\) \\(e\\) Industrial \\(c\\) \\(d\\) \\(f\\) Totals \\(g\\) \\(h\\) \\(k\\) We can now step through the steps involved in a \\(\\chi^{2}\\) contingency table test: Step 1. We need to work out the expected numbers in cells a-d, under the null hypothesis that the two kinds of outcomes (colour and habitat type) are independent of one another. If you happen to have studied a bit of basic probability theory at some point you might be able work these numbers out. We’ll demonstrate the logic of the calculation, though you don’t have to remember it. Let’s see how it works for the Black-Rural cell (‘a’): Calculate the probability that a randomly chosen individual in the sample is from a rural location (\\(p(\\text{Rural})\\)). This is the total ‘Rural’ count (\\(e\\)) divided by the grand total (\\(k\\)): \\[p(\\text{Rural}) = \\frac{e}{k}\\] Calculate the probability that a randomly chosen individual in the sample set is black (\\(p(\\text{Black})\\)). This is the total ‘Black’ count (\\(g\\)) divided by the grand total (\\(k\\)): \\[p(\\text{Black}) = \\frac{g}{k}\\] Calculate the probability that a randomly chosen individual is both ‘Rural’ and ‘Black’, assuming these events are ‘independent’ . This is given by the product of the probabilities from steps 1 and 2: \\[p(\\text{Rural, Black}) = \\frac{e}{k} \\times \\frac{g}{k}\\] Convert this to the expected number of individuals that are ‘Rural’ and ‘Black’, under the independence assumption. This is the probability from step 3 multiplied by the grand total (\\(k\\)): \\[E(\\text{Rural, Black}) = \\frac{e}{k} \\times \\frac{g}{k} \\times {k} = \\frac{g \\times e}{k}\\] In general, the expected value for any particular cell in a contingency table is given by multiplying the associated row and column totals and then dividing by the grand total. This is just a short cut for the full calculation we just worked through. Step 2 Calculate the \\(\\chi^{2}\\) test statistic from the four observed cell counts and their expected values. The resulting \\(\\chi^{2}\\) statistic summarises—across all the cells—how likely the observed frequencies are under the null hypothesis of no association (= independence). Step 3 Compare the \\(\\chi^{2}\\) statistic to the theoretical predictions of the \\(\\chi^{2}\\) distribution in order to assess the statistical significance of the difference between observed and expected counts. This p-value is the probability we would see the observed pattern of cell counts, or a more strongly associated pattern, under the null hypothesis of no association. A low p-value represents evidence against the null hypothesis. Method for hand calculation of 2 x 2 contingency tables For a 2 x 2 table there is a short cut method which is quicker than the general method outlined above. The formula for χ2 for a 2 x 2 table table (using the letter labels as in the table above) is: \\[\\chi^{2}=\\frac{k(bc-ad)^{2}}{efgh}\\] The only problem with this short cut method is that this formula does not show us what the expected values are. If we think it might be a problem, then we should pick the smallest column and row totals and calculate the expected value for the corresponding cell, using the formula above—this is the smallest expected value. 32.2.1 Assumptions of the chi-square contingency table test The assumptions of the \\(\\chi^{2}\\) goodness of fit test are essentially the same as those of the goodness of fit test: The observations are independent counts. For example, it would not be appropriate to apply a \\(\\chi^{2}\\) goodness of fit test where observations are taken before and after an experimental intervention is applied to the same objects. The expected counts are not too low. The rule of thumb is that the expected values (again, not the observed counts) should be greater than 5. If any of the expected values are below 5, the p-values generated by the test start to become less reliable. 32.3 Carrying out a chi-square contingency table test in R Walk through example You should work through the example in this section. Let’s carry on with the ladybird colour and habitat type example. You need to download three data sets to work through this section: LADYBIRDS1.CSV, LADYBIRDS2.CSV, and LADYBIRDS3.CSV. These all contain the same information—it is just organised differently in each case. Carrying out a \\(\\chi^{2}\\) contingency table test in R is very simple: we use the chisq.test function again. The only slight snag is that we have to ensure the data is formatted correctly before it can be used. Whenever we read data into R using read.csv we end up with a data frame. Unfortunately, the chisq.test function is one of the few statistical functions not designed to work with data frames. This means we first have to use a function called xtabs to construct something called a contingency table object.30. The xtabs function does categorical ‘cross tabulation’31, i.e., it sums up the number of occurrences of different combinations of categories among variables. We’ll look at how to use xtabs before running through the actual test… 32.3.1 Step 1. Getting the data into the correct format It is not difficult to use, but the precise usage of xtabs depends upon how the raw data are organised. We’ll examine the three main cases in turn. Case 1… Data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are often represented in a data set with one column per categorical variable, and one row per observation. The LADYBIRDS1.CSV file contains the data in this format. Read it into an R data frame: lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS1.CSV&quot;) We called the data lady_bird_df to emphasise that they are stored in a data frame at this point. We can use glimpse, head and tail to get a sense of how the data are organised: glimpse(lady_bird_df) ## Observations: 300 ## Variables: 2 ## $ Habitat &lt;fctr&gt; Rural, Rural, Rural, Rural, Rural, Rural, Rural, Rura... ## $ Colour &lt;fctr&gt; Black, Black, Black, Black, Black, Black, Black, Blac... head(lady_bird_df, 10) ## Habitat Colour ## 1 Rural Black ## 2 Rural Black ## 3 Rural Black ## 4 Rural Black ## 5 Rural Black ## 6 Rural Black ## 7 Rural Black ## 8 Rural Black ## 9 Rural Black ## 10 Rural Black tail(lady_bird_df, 10) ## Habitat Colour ## 291 Industrial Red ## 292 Industrial Red ## 293 Industrial Red ## 294 Industrial Red ## 295 Industrial Red ## 296 Industrial Red ## 297 Industrial Red ## 298 Industrial Red ## 299 Industrial Red ## 300 Industrial Red We only showed you the first and last 10 values—you should take a full look at the data with the View function. You will see that the data frame contains 300 rows—one for each ladybird—and two variables (Habitat and Colour). The two variables obviously contain the information about the categorisation of each ladybird in the sample. We require a two-way table that contains the total counts in each combination of categories. This is what xtabs does. It takes two arguments: the first is a formula (involving the ~ symbol) that specifies the required contingency table; the second is the name of the data frame containing the raw data. When working with data in the above format—one observation per row—we use a formula that only contains the names of the categorical variables on the right hand side of the ~ (i.e., ~ Habitat + Colour): lady_bird_table &lt;- xtabs(~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 When used like this, xtabs will sum up the number of observations with different combinations of Habitat and Colour. We called the output lady_bird_table to emphasise that the data from xtabs are now stored in a contingency table. When we print this to the console we see that lady_bird_table does indeed refer to something that looks like a 2 x 2 contingency table of counts. Case 2… Sometimes data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are partially summarised into counts. For example, imagine that we had visited five rural sites and five urban sites and recorded the numbers of red and black colour forms found at each site. Data in this format are stored in the LADYBIRDS2.CSV file. Read this into an R data frame and examine this with the View function: lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS2.CSV&quot;) glimpse(lady_bird_df) ## Observations: 20 ## Variables: 4 ## $ Habitat &lt;fctr&gt; Rural, Rural, Rural, Rural, Rural, Rural, Rural, Rura... ## $ Site &lt;fctr&gt; R1, R2, R3, R4, R5, R1, R2, R3, R4, R5, U1, U2, U3, U... ## $ Colour &lt;fctr&gt; Black, Black, Black, Black, Black, Red, Red, Red, Red... ## $ Number &lt;int&gt; 10, 3, 4, 7, 6, 15, 18, 9, 12, 16, 32, 25, 25, 17, 16,... lady_bird_df ## Habitat Site Colour Number ## 1 Rural R1 Black 10 ## 2 Rural R2 Black 3 ## 3 Rural R3 Black 4 ## 4 Rural R4 Black 7 ## 5 Rural R5 Black 6 ## 6 Rural R1 Red 15 ## 7 Rural R2 Red 18 ## 8 Rural R3 Red 9 ## 9 Rural R4 Red 12 ## 10 Rural R5 Red 16 ## 11 Industrial U1 Black 32 ## 12 Industrial U2 Black 25 ## 13 Industrial U3 Black 25 ## 14 Industrial U4 Black 17 ## 15 Industrial U5 Black 16 ## 16 Industrial U1 Red 17 ## 17 Industrial U2 Red 23 ## 18 Industrial U3 Red 21 ## 19 Industrial U4 Red 9 ## 20 Industrial U5 Red 15 This time we printed at the whole dataset (it’s easier to use View, but that won’t render in this book). The counts at each site are in the Number variable, and the site identities are in the Site variable. We need to sum over the sites to get the total number within each combination of Habitat and Colour. We use xtabs again, but this time we have to tell it which variable to sum over: lady_bird_table &lt;- xtabs(Number ~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 When working with data in this format—more than one observation per row—we use a formula where the name of the variable containing the counts is on left hand side of the ~, and the names of the categorical variables to sum over are on the right hand side of the ~ (i.e., Number ~ Habitat + Colour). When used like this xtabs will sum up the counts associated with different combinations of Habitat and Colour. The lady_bird_table object produced by xtabs is no different than before. Good! These are the same data. Case 3… Data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are sometimes already summarised into total counts. Data in this format are stored in the LADYBIRDS3.CSV file. Read this into an R data frame and print it to the console (it’s very small, so there’s no need to use View): lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS3.CSV&quot;) lady_bird_df ## Habitat Colour Number ## 1 Industrial Black 115 ## 2 Industrial Red 85 ## 3 Rural Black 30 ## 4 Rural Red 70 The total counts are already in the Number variable so there is no real need to sum over anything to get the total for each combination of Habitat and Colour. However, we still need to convert the data from a data frame to a contingency table. There are various ways to do this, but it is easiest to use xtabs. The R code is identical to the previous case: lady_bird_table &lt;- xtabs(Number ~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 In this case xtabs doesn’t change the data at all because it’s just ‘summing’ over one value in each combination of categories. We’re just using xtabs to convert it from a data frame to a contingency table. The resulting lady_bird_table object is the same as before. 32.3.2 Step 2. Doing the test Once we have the data in the form of a contingency table the associated \\(\\chi^{2}\\) test of independence between the two categorical variables is easy to carry out with chisq.test: chisq.test(lady_bird_table) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lady_bird_table ## X-squared = 19.103, df = 1, p-value = 1.239e-05 That’s all we have to do. Just pass one argument to chisq.test: the contingency table. This output should make sense in the light of what we saw in the previous chapter. R first prints a reminder of the test employed (Pearson's Chi-squared test with Yates' continuity correction) and the data used (data: lady_bird_table). We’ll come back to the “Yates’ continuity correction” bit in a moment. R then summarises the \\(\\chi^{2}\\) value, the degrees of freedom, and the p-value: X-squared = 19.103, df = 1, p-value = 1.239e-05 The p-value is highly significant (p&lt;0.001) indicating that the colour type frequency varies among the two kinds of habitats.32 Degrees of freedom for a χ2 contingency table test We need to know the degrees of freedom associated with the test: in a two-way χ2 contingency table test these are (nA − 1)×(nB − 1), where nA − 1 is the number of categories in the first variable, and nB − 1 is the number of categories in the second variable. So if we’re working with a 2 x 2 table the d.f. are 1, if we’re working with a 2 x 3 table the d.f. are 2, if we’re working with a 3 x 3 table the d.f. are 4, and so on. What was that “Yates’ continuity correction” all about? The reasoning behind using this correction is a bit beyond this course, but in a nutshell, it generates more reliable p-values under certain circumstances. By default, the chisq.test function applies this correction to all 2 x 2 contingency tables. We can force R to use the standard calculation by setting correct = FALSE if we want to: chisq.test(lady_bird_table, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: lady_bird_table ## X-squared = 20.189, df = 1, p-value = 7.015e-06 Both methods give similar results in this example, though they aren’t exactly the same—the \\(\\chi^{2}\\) value calculated when correct = FALSE is very slightly higher than the value found when using the correction. Don’t use the correct = FALSE option! The default correction is a safer option for 2 x 2 tables. 32.3.3 Summarising the result We have obtained the result so now we need to write the conclusion. As always, we go back to the original question to write the conclusion. In this case the appropriate conclusion is: There is a significant association between the colour of Adalia bipunctata individuals and habitat, such that black individuals are more likley to be found in industrial areas (\\(\\chi^{2}\\) = 19.1, d.f. = 1, p &lt; 0.001). Notice that we summarised the nature of the association alongside the statistical result. This is easy to do in the text when describing the results of a 2 x 2 contingency table test. It’s much harder to summarise the association in written form when working with larger tables. Instead, we often present a table or a bar chart showing the observed counts. 32.4 Working with larger tables Interpreting the results of a contingency table test is fairly straightforward in the case of a 2 x 2 table. We can certainly use contingency table tests with larger two-way tables (e.g. 3 x 2, 3 x 3, 3 x 4, etc), and higher dimensional tables (e.g. 2 x 2 x 2), but the results become progressively harder to understand as the tables increase in size. If you get a significant result, it is often best to compare the observed and expected counts for each cell and look for the highest differences to try and establish what is driving the significant association. Visualising the data with a bar chart can also help with interpretation. There are ways of subdividing large tables to make subsequent \\(\\chi^{2}\\) tests on individual parts of a table, in order to establish specific effects, but these are not detailed here. Unless we plan to make the more detailed comparisons before we started collecting the data, it is hard to justify this kind of post hoc analysis. This is called their ‘joint distribution’, in case you were wondering.↩ This is the same Pearson who invented the correlation coefficient for measuring linear associations by the way.↩ The table objects produced by xtabs are not the same as the dplyr table-like objects: ‘tibbles’. This is one of the reasons that dplyr adopted the name ‘tibble’. The overlap in names is unfortunate, but we’ll have to live with it—there are only so many ways to name things that look like tables.↩ Pivot tables can be used to do the same thing in Excel.↩ We could have summarised the result as: habitat type varies among the two colour types. This way of explaining the result seems odd though. Ladybirds are found within habitats, not the other way around. Just keep in mind that this is a semantic issue. The contingency table test doesn’t make a distinction between directions of effects.↩ "],
["non-parametric-tests.html", "Chapter 33 Non-parametric tests 33.1 What is a non-parametric test? 33.2 How does a non-parametric test work? 33.3 Non-parametric equivalents 33.4 Wilcoxon signed-rank test 33.5 The Mann-Whitney U-test 33.6 The Kruskal-Wallis test 33.7 Spearman’s rank correlation 33.8 Why use non-parametric tests … and when?", " Chapter 33 Non-parametric tests 33.1 What is a non-parametric test? The majority of procedures we have been using to evaluate statistical significance require various assumptions about population distributions to be satisfied. These are referred to as parametric methods because they are underpinned by a mathematical model of the population(s) (we discussed this idea in the Parametric statistics chapter). For this reason, the statistical tests associated with these methods—e.g. global significance tests and multiple comparisons tests in ANOVA—are called parametric tests. Non-parametric tests are a class of statistical tests that make much weaker assumptions. The advantage of non-parametric tests is that they can be employed with a much wider range of forms of data than their parametric cousins. Although non-parametric tests are less restrictive in their assumptions, they are not, as is sometimes stated, assumption-free. The term non-parametric is just a catch-all term that applies to any test which doesn’t assume the data are drawn from a specific distribution. We have already seen a number of examples of non-parametric tests: The bootstrap and permutation test procedures introduced in the first few chapters are non-parametric techniques. The \\(\\chi^{2}\\) goodness of fit and the \\(\\chi^{2}\\) contingency table tests make weak assumptions about the frequency data. In this chapter we are going to look at non-parametric tests which perform analyses equivalent to t-tests, correlation, and the global significance test in one-way ANOVA. 33.2 How does a non-parametric test work? The key thing about the non-parametric tests we’ll consider here is that the calculations are done using the rank order of the data, whatever the type of the original data-–-ratio, interval, or ordinal. The general principle of such a test can be easily illustrated with the following example. Imagine that we want to compare two samples to determine whether they differ in their central tendency. That is, we want to know if the values in one sample tend to be larger or smaller than the values in a second sample. For example… Sample A: 1.3 2.8 4.1 3.2 Sample B: 7.2 3.0 4.2 6.2 If all the data (i.e. from both samples) are put in ascending order… Sample A: 1.3 2.8 3.2 4.1 Sample B: 3.0 4.2 6.2 7.2 Then each number can be given a rank according to its place in the ordering… Sample A: 1 2 4 5 Sample B: 3 6 7 8 It is easy to see now that if the rank values from each sample are added up Sample A will have a lower value (sum = 12) than Sample B (sum = 24). This suggests that sample B has larger values than sample A. If the samples had been completely non-overlapping the totals would have been… Sample A: 1 + 2 + 3 + 4 = 10 Sample B: 5 + 6 + 7 + 8 = 26 On the other hand, if the samples had been largely overlapping the rank totals would have been equal, or close to it… Sample A: 1 + 4 + 6 + 7 = 18 Sample B: 2 + 3 + 5 + 8 = 18 Obviously, the greater the difference in the rank totals the less likely it is that the two samples could have come from the same distribution of values—i.e. the more likely it is that difference between them may be statistically significant. The important thing to notice in such a procedure is that the original data could have been replaced with quite different values which, provided the ordering was the same, would have given exactly the same result. In other words the outcome of the test is reasonably insensitive to the underlying distribution of the data. 33.3 Non-parametric equivalents There are several non-parametric procedures (all available in R) which provide similar types of test to the simple parametric tests we have seen already. The actual calculations for the different tests are a little more involved than the general outline above, but they work from the same basic principle: find the rank order of all the data, and then use the ranks in different groups to assess the statistical significance of the observed differences. As always, we have to specify some kind of null hypothesis (e.g. the medians of two samples are the same) to make the significance calculation work. The four tests discussed here are: Wilcoxon signed-rank test: The test is equivalent to a one-sample and paired-sample t-test. This test also goes by the name of the Wilcoxon one-sample test, the Wilcoxon matched-pairs test, the Wilcoxon paired-sample test. It can be used to… compare a sample to a single value, or test for differences between paired samples. Mann-Whitney U-test: This is equivalent to the two-sample t-test. It tests for differences between two unpaired samples. This test also goes by the name of the Wilcoxon two-sample test, the Mann–Whitney–Wilcoxon, Wilcoxon rank-sum test. Kruskal-Wallis test: tests for differences between several samples. This is equivalent to a one-way analysis of variance. Spearman’s Rank correlation: tests for an association between two variables. This is equivalent to Pearson’s correlation. Notice that we haven’t said anything about what kind of differences among samples these evaluate, i.e. we didn’t specify the null hypothesis. We’ll address this question as we discuss each test. 33.4 Wilcoxon signed-rank test The most widely used non-parametric equivalent to the one-sample t-test is the Wilcoxon signed-rank test. The test can be used for any situation requiring a test to compare the median of a sample against a single value. However, it is almost always used to analyse data collected using a paired-sample design, so we’ll focus on that particular application of the test. The Wilcoxon signed-rank test makes less stringent assumptions than the t-test, but it does make some assumptions: The variable being tested is measured on ordinal, interval, or ratio scales The (population) distribution of the variable is approximately symmetric33. The first assumption is simple enough—the variable can be anything but nominal. The second assumption essentially means that it doesn’t matter what form the distribution of the variable looks like as long as it is symmetric—it would not be satisfied if the distribution were strongly right- or left-skewed. As with the t-test, when applied to paired-sample data the Wilcoxon signed-rank test starts by finding the differences between all the pairs, and then tests whether these differences are significantly different form zero. The distribution under consideration is the distribution of differences between the pairs. Remember, this distribution will often be approximately normal even when the connected samples are not themselves drawn from a normal distribution. This means that even if the samples have odd distributions, we may still find that we can use a paired-sample t-test if differences have a perfectly acceptable distribution. However, if the distribution of differences is not normal then the Wilcoxon signed-rank test provides an alternative. The following is an example of a situation where the Wilcoxon signed-rank test would be appropriate. 33.4.1 Leaf damage, plant defences and feeding by winter moth larvae It has been hypothesised that plants respond to physical damage to their leaves from herbivores such as lepidopteran larvae by increasing production of defensive compounds such as phenolics. In an experiment to test this effect, birch saplings were subjected to artificial leaf damage (hole punching) on half the leaves on selected branches. After 24 h undamaged leaves from both the branches with hole-punched leaves and others distant from the damage site were collected and used in a choice test experiment with winter moth larvae. Twenty trials were carried out with a single caterpillar in each trial, offered one of each type of leaf (i.e. one leaf from close to the damage site and one from an undamaged area). The percentage of each leaf consumed was estimated (to the nearest 5%) after 24h. The data are in the file LEAF_DAMAGE.CSV: leaves &lt;- read.csv(file=&quot;LEAF_DAMAGE.CSV&quot;) glimpse(leaves) ## Observations: 40 ## Variables: 3 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... ## $ LeafType &lt;fctr&gt; Notdamaged, Notdamaged, Notdamaged, Notdamaged, Notd... ## $ Damage &lt;int&gt; 65, 0, 10, 45, 55, 45, 0, 10, 5, 70, 55, 40, 0, 70, 6... There are three variables: ID contains a unique identifier for each pair of leaves in a trial (1-20), Leaf identifies the type of leaf (‘Damaged’ vs. ‘Undamaged’), and Damage contains the percent damage score. As always, it’s a good idea to visualise the data: jitter_leaves &lt;- mutate(leaves, Damage = jitter(Damage, factor = 4)) ggplot(jitter_leaves, aes(x = LeafType, y = Damage, group = ID)) + geom_point() + geom_line() It’s not critical to learn it, but we used a new trick here: using mutate, we ‘jittered’ the Damage values (with the jitter function) to deal with the overplotting. This adds a bit of random noise to the values so that those with the same values end up being plotted in different highlights two features of the data: 1) There is plenty of variation (some larvae just eat more than others), 2) there is a tendency for larvae to prefer the undamaged leaves, though the pattern is not overwhelming. Obviously we need a statistical test—a paired sample t-test or a Wilcoxon signed-rank test are our two options. We should also visualise this distribution of within larvae differences to determine whether or not we need to use a Wilcoxon signed-rank test: # step 1 -- calculate the differences leaves_diff &lt;- leaves %&gt;% group_by(ID) %&gt;% summarise(DiffDamage = diff(Damage)) # step 2 -- make the plot ggplot(leaves_diff, aes(x = DiffDamage)) + geom_dotplot(binwidth = 10) There are large differences in both directions and the distribution does not seem to be normal (it is very ‘flat’). The problem here is that caterpillars often spend time feeding on the first leaf they encounter, which is a more or less random choice; some may change leaves in quickly in response to the quality of the leaf, others may feed for sometime before leaving the leaf. We might still expect those on better defended leaves to feed less, as the leaves should be less palatable, but they may not make the behavioural decision to leave the leaf during the short period of the experiment (the experiment cannot go on for longer because the leaves change chemically the longer they are detached from the plant) The distribution may not be normal, but it is roughly symmetrical. The symmetry assumption of the Wilcoxon signed-rank test seems to be satisfied, so we can use this test to examine whether there is any difference in caterpillar feeding on leaves from damaged areas and undamaged areas. We carry out the test using the wilcox.test function in R34. This is used in exactly the same way as the t.test function for a paired-sample design: wilcox.test(Damage ~ LeafType, paired = TRUE, data = leaves) ## Warning in wilcox.test.default(x = c(5L, 25L, 30L, 40L, 15L, 5L, 35L, ## 45L, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(x = c(5L, 25L, 30L, 40L, 15L, 5L, 35L, ## 45L, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: Damage by LeafType ## V = 50.5, p-value = 0.07617 ## alternative hypothesis: true location shift is not equal to 0 We must remember to set paired = TRUE. If we forget to do this R will carry out the two-sample version of the test (the Mann-Whitney U-test discussed next). R produces a couple of warning here but we can usually ignore these—we can’t do much about them even if we wanted to. The elements of the output should be fairly easy to understand by now. At the top we see a summary of the test and the data: ## Wilcoxon rank sum test with continuity correction ## data: Damage by LeafType Next, we see the results: a test statistic and p-value. Although the test statistic is given as ‘V’ in the output, it is often quoted as ‘W’ when writing it in a report (we’ll adopt this convention). This seems a bit odd, but it stems from the fact that there is some variation in the literature about what we report, and how to report, non-parametric tests. The p-value is close to p = 0.05, but it doesn’t quite fall below the threshold. It looks like there is no significant difference in feeding rates on the two types of leaf. We might report this as: When given a choice, winter moth larvae did not consume larger amounts of the leaves collected from areas of the tree that are undamaged than those from damaged areas (Wilcoxon matched-pairs test: W = 139.5, n = 20, p = 0.076). 33.5 The Mann-Whitney U-test The Mann-Whitney U-test is the non-parametric equivalent to the independent two-sample t-test. The test can be used for any situation requiring a test to compare the median of two samples. The assumptions of the Mann-Whitney U-test are: The variable being tested is measured on ordinal, interval, or ratio scales The observations from both groups are independent of one another. The first two assumptions are straightforward—data can be anything but nominal, and as with a paired-sample t-test, there must not be any dependence between the observations. Though not strictly necessary for the Mann-Whitney U-test to be valid, we usually add a third assumption: The distribution of the variable in each group is similar (apart than the fact that they have a different central tendency) This assumption essentially means that it doesn’t matter what the distributions of the two samples are like, but they should be at least roughly similar—it would not be satisfied if we plan to compare data from a strongly right-skewed distribution with data from a strongly left-skewed distribution. If this assumption is not satisfied the test can still be used, but a significant result is hard to interpret (so don’t bother!). When all three of the above assumptions are satisfied the Mann-Whitney U-test is used as a way of looking for differences between the central tendency of two distributions. A two-sample t-test evaluates the statistical significance of differences between two means. The null hypothesis of the Mann-Whitney U-test (if all three of the above assumptions) is that the two distributions have the same median . A significant p value therefore indicates that the medians are likely to be different. 33.5.1 Ant foraging Let’s use the Red wood ants example from the chapter on transformations to see how to carry out the Mann-Whitney U-test. The data are the file ANTS1.CSV (not ANTS2.CSV!). Recall that this study measured the total biomass of prey being transported—the rate of food collection per ant per half hour—on three different tree species (rowan, sycamore and oak). The Tree variable contains the identities and the Food variable contains the food collection rates. Since we the Mann-Whitney U-test only compares two samples, we will focus on the oak and sycamore here to illustrate the test35. After reading the data into R we need to use the filter function to remove ‘Rowan’ cases: ants &lt;- read.csv(&quot;ANTS1.CSV&quot;) ants &lt;- filter(ants, Tree != &quot;Rowan&quot;) The Tree != &quot;Rowan&quot; inside the filter function specifies that we want cases where Tree is not equal to (!=) ‘Rowan’. Carrying out a Mann-Whitney U-test in R is simple enough, but somewhat confusingly, we have to use the wilcox.test function again. This is because, as we noted above, the Mann-Whitney U-test is also called a two-sample Wilcoxon test. The wilcox.test function for an independent two-sample design works just like the corresponding analysis with the t.test function. The first argument should be a formula, and the second should be the data frame containing the relevant variables: wilcox.test(Food ~ Tree, data = ants) ## Warning in wilcox.test.default(x = c(20.1, 47.4, 85.6, 17.1, 5.7, 7.8, ## 28.8, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: Food by Tree ## W = 453.5, p-value = 0.06955 ## alternative hypothesis: true location shift is not equal to 0 The formula (Food ~ Tree) works in the same way as other statistical function in R: the variable containing the values we wish to compare (Food) is on the left hand side of the ~ and the variable containing the group identities (Tree) belongs to the right of it. The output from a Mann-Whitney U-test is similar from that produced by the t.test function. At the top we see a summary of the test and the data: ## Wilcoxon rank sum test with continuity correction ## data: Food by Tree This version of the Wilcoxon test is the same thing as a Mann-Whitney U-test so there is nothing to worry about here. After this line we see a test statistic (’W’ilcoxon) and the associated p-value. Since p=0.07, we conclude the rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore. In a report the conclusion from the test can be summarised… The rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore (Mann-Whitney U-test: U=453.5, n1=26, n2=27, p=0.07). Notice how the statistics were reported. Because we are describing the test as a Mann-Whitney U-test, it is conventional to quote the test statistic as ‘U’, rather than ‘W’. If we had decided to present the test as a two-sample Wilcoxon test the test can be summarised… The rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore (two-sample Wilcoxon test: W=453.5, n1=26, n2=27, p=0.07). …remembering to change the name (‘two-sample Wilcoxon test’) and label (‘W’) used to describe the test statistic. In either case, we also have to provide the sample sizes associated with each tree group so that a reader can judge how powerful the test was. 33.6 The Kruskal-Wallis test The Kruskal-Wallis is the non-parametric equivalent to one-way ANOVA. The Kruskal-Wallis test allows us to test for differences among more than two samples. Like the other rank-based tests we have encountered it has some assumptions, but these are less restrictive than those for ANOVA. The assumptions are essentially the same as those for the Mann-Whitney U-test: The variable being tested is measured on ordinal, interval, or ratio scales The observations from both groups are independent of one another. The distribution of the variable in each group is similar (apart than the fact that they have a different central tendency) The third assumption is important, particularly with respect to the skewness of the distributions. The test is at least reasonably robust to differences in the dispersion, but the Kruskal-Wallis test should not be used if the skewness of the variable is different among groups is very different. The reason for this is—just as with Mann-Whitney U-test—that a significant result is hard to interpret. When all three of the above assumptions are satisfied the Kruskal-Wallis is used as a way of looking for differences in the central tendency of two or more groups groups. A one-way ANOVA evaluates the statistical significance of differences between means of these groups. The null hypothesis of the Kruskal-Wallis test (if all three of the above assumptions) is that the groups have the same median. A significant p value therefore indicates that the medians are likely to be different. 33.6.1 Learning in cuttlefish In a study of the ability of cuttlefish to learn, an experiment was conducted to determine how the length of exposure to a situation influenced the learning process. Cuttlefish feed on prawns. If they are presented with prawns in a glass tube they strike at them but, obviously, fail to capture the prey. Not surprisingly, after a period of this fruitless activity, they give up striking at the prawns. In the experiment, 50 cuttlefish were divided at random into 5 groups of 10 and cuttlefish from each group were presented with prawns in glass tubes for different lengths of time: 2 min., 4 min., 8 min., 15 min., and 30 min. for the 5 groups respectively. After 24 hours the same cuttlefish were presented with prawns again and the number of strikes they made (over a fixed period) were recorded. The data are in the file CUTTLEFISH.CSV. There are two variables: Strikes contains the number of strikes recorded and Time identifies the groups (period of previous exposure): cuttlefish &lt;- read.csv(file=&quot;CUTTLEFISH.CSV&quot;) glimpse(cuttlefish) ## Observations: 50 ## Variables: 2 ## $ Strikes &lt;int&gt; 5, 12, 11, 4, 1, 6, 8, 3, 0, 5, 0, 13, 0, 3, 0, 7, 3, ... ## $ Time &lt;fctr&gt; t02, t02, t02, t02, t02, t02, t02, t02, t02, t02, t04... Take need to understand the data. Dot plots for each treatment will work here: ggplot(cuttlefish, aes(x = Strikes)) + geom_dotplot(binwidth = 2) + facet_wrap(~ Time) The data are clearly very variable. The combination of the apparent skew and the fact that the data are generally small whole numbers with several equal values in each sample, means that we may not be very successful in using a transformation to beat the data into shape. Let’s use the Kruskal-Wallis test instead. Predictably, the R function to carry out a Kruskal-Wallis test is called kruskal.test, and it is used in exactly the same way as every other statistical modelling function we have looked at: kruskal.test(Strikes ~ Time, data = cuttlefish) ## ## Kruskal-Wallis rank sum test ## ## data: Strikes by Time ## Kruskal-Wallis chi-squared = 11.042, df = 4, p-value = 0.0261 And… one more time… the elements of the output should be easy to work out. These are a statement of the test used and the data, followed by the results: a test statistic (another type of \\(\\chi^2\\) statistic), a degrees of freedom, and the all-important p-value. We report all of these when writing up results of a Kruskal-Wallis test. However, there is some disagreement in the literature how to report a Kruskal-Wallis test—some people report the statistic as a \\(\\chi^2\\), while others refer to it as an ‘H’ statistic. We will follow the common convention of reporting it as an ‘H’ value. The test (as with ANOVA) tells us that there is at least one difference among the groups, but it doesn’t tell where the difference or differences are. The output does not give the medians so we cannot judge how the samples are ordered. We can use dplyr to calculate the group-specific medians though: cuttlefish %&gt;% group_by(Time) %&gt;% summarise(Median = median(Strikes)) ## # A tibble: 5 x 2 ## Time Median ## &lt;fctr&gt; &lt;dbl&gt; ## 1 t02 5.0 ## 2 t04 3.5 ## 3 t08 2.5 ## 4 t15 2.5 ## 5 t30 0.0 In this case it is fairly clear that longer periods of exposure to the protected prawn do seem to result in fewer strikes in the later trial. Once we understand what is driving the significant result we’re in a position to write a summary: The frequency with which the cuttlefish attacked the prawns was significantly affected by the length of time for which they had been exposed to protected prawns 24h earlier (Kruskal-Wallis test: H=11.04, d.f.=4, p&lt;0.05), with longer prior exposure resulting in lower attack rates. If it was important to know exactly which treatments were significantly different, then some sort of multiple comparison test would be useful. There are no non-parametric multiple comparison tests available in base R, but they are implemented in the package called nparcomp. 33.7 Spearman’s rank correlation Spearman’s rank correlation (\\(\\rho\\)) tests for an association between two numeric variables. It is equivalent to Pearson’s correlation. The advantages of using Spearman’s rank correlation are: 1) the two variables do not need to be normally distributed, and 2) ordinal data can be used. This means Spearman’s rank correlation can be used with data having skewed (or other odd) distributions, or with data originally collected on a rank/ordinal scale. The key assumptions of Spearman’s rank correlation are: Both variables are measured on ordinal, interval or ratio scales. There is a monotonic relationship between the two variables. A monotonic relationship occurs when, in general, the variables increase in value together, or when the values of one variable increase, the other variable tends to decrease. What this means in practice is that we should not use Spearman’s rank correlation if a scatter plot of the data forms a clear ‘hill’ or ‘valley’ shape. Spearman’s rank correlation is somewhat less powerful (roughly 91% in some evaluations) than Pearson’s method when the data are suitable for the latter. Otherwise it may even be more powerful. We’ll work through an example to learn about Spearman’s correlation… 33.7.1 Grouse lekking Some bird species, at a particular point in the spring, form ‘leks’—gatherings of birds, with males each defending a small area of ground, displaying, and each mating with such females as he is successful in attracting. In general, in leks, a few birds secure many matings and most birds secure rather few. In a study of lekking in black grouse, a biologist is interested in whether birds that secure many matings in one season also do better the next year. Using a population with many colour-ringed birds he is able to get data for a reasonable number of males from two leks in successive years. The data are in the file GROUSE.CSV. Read these data into a data frame, calling it grouse. grouse &lt;- read.csv(&quot;GROUSE.CSV&quot;) glimpse(grouse) ## Observations: 20 ## Variables: 2 ## $ Year1 &lt;int&gt; 3, 15, 4, 3, 4, 9, 1, 0, 2, 7, 3, 2, 6, 0, 1, 12, 2, 1, ... ## $ Year2 &lt;int&gt; 6, 8, 0, 0, 2, 10, 5, 4, 1, 4, 3, 7, 6, 2, 5, 17, 0, 0, ... Each row of the data is the number of matings for a male in the two successive leks: Year1 (year 1) and Year2 (year 2). The first thing we should do is summarise the distribution of each variable: ggplot(grouse, aes(x = Year1)) + geom_dotplot(binwidth = 2) ggplot(grouse, aes(x = Year2)) + geom_dotplot(binwidth = 2) Notice that the data are integer-valued (i.e. they are counts). These distributions seems to tie in with the biological observation that the distribution of matings is right-skewed: in both years there are only a few males that have high mating success, with most males securing only a handful of matings. Next we need to visualise the association: The data are integers, which means there is a risk of over-plotting (points will lie on top of one another). We made the points semi-transparent alpha = 0.5 to pick this up where it occurs. It seems clear that mating success is positively associated, but we should confirm this with a statistical test. We’ll base this on Spearman’s correlation. How do we know to use a correlation analysis with these data? Although there seems to be an association between the counts, it is not obvious that success in one year ‘causes’ success in another year and neither variable is controlled by the investigator. We’re also not interested in using the success measure in year 1 to predict success in year 2. This indicates that correlation analysis is the appropriate method to evaluate the significance of the association. Why are we using Spearman’s correlation? The relationship appears roughly linear, so in that regard Pearson’s correlation might be appropriate. However, the distribution of each count variable is right-skewed, which means the normality assumption is probably suspect in this instance. We’re left with no choice but to use Spearman’s correlation. Carrying out a correlation analysis using Spearman’s rank correlation in R is simple. Again, we use the cor.test function to do this: cor.test(~ Year1 + Year2, method = &quot;spearman&quot;, data = grouse) ## Warning in cor.test.default(x = c(3L, 15L, 4L, 3L, 4L, 9L, 1L, 0L, 2L, ## 7L, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: Year1 and Year2 ## S = 592.12, p-value = 0.01112 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.5547952 The only other thing we had to change, compared to the Pearson’s correlation example, was to set method = &quot;spearman&quot; to specify the use of Spearman’s rank correlation. Notice the warning message: ‘Cannot compute exact p-value with ties’. This is generally not something we need to worry about for this particular test. The output is very similar to that produced by cor.test when using Pearson’s correlation. Once again, the S = 592.12, p-value = 0.01112 line is the one we care about. The main difference is that instead of a t-statistic, we end up working with a different kind of test statistic (‘S’). We aren’t going to explain where this comes from because it’s quite technical. Next we see the p-value. This is generated under the null hypothesis of zero correlation (\\(\\rho = 0\\)). Since p &lt; 0.05, we conclude that there is a statistically significant correlation between mating success in successive years. Wait, where are the degrees of freedom? Simple—there aren’t any for a Spearman’s correlation test. What is the correlation between mating success? That’s given at the bottom of the test output again: \\(+0.55\\). This says that there is a moderate, positive association between mating success in successive years, which is what we expect from the scatter plot. When using the Spearman method it is fine to report just the value of the correlation coefficient, the sample size, and the p-value (there is no need to report the test statistic). Here’s how to report the results of this analysis: There is a positive association between the number of matings achieved by a particular male in one lek and the number the same male achieves in a subsequent lek (Spearman’s rho=0.55, n=20, p &lt; 0.05). 33.8 Why use non-parametric tests … and when? 33.8.1 What are the advantages? Since most non-parametric tests make relatively weak assumptions about the distribution of the data, they are obviously useful techniques for many situations where the data we have are not well suited to parametric tests. If in doubt, a non-parametric test may be a safe bet. Importantly, since non-parametric tests work with ranks of the original data, they can be used to analyse data originally collected in ordinal, or rank, form. This is extremely useful in many investigations where the data cannot be collected any other way for example… Subjects in a psychology experiment might be asked to rank a series of photographs of people in order of attractiveness A panel of tasters judging the sweetness of wines may be able to score sweetness on a rank scale Encounters between animals might be scored on the basis of the aggressive behaviours shown which can be put in rank order (retreats, stands ground passively, fights when attacked, initiates attack) The apparatus for making actual measurements might be unavailable—but relative comparisons can be made by direct observation, perhaps in the field—e.g. ‘greenness’ of leaves, turbidity of water, crawling speed of caterpillars, order of dung fly species arrival on a new dung pat, etc. 33.8.2 What are the disadvantages? Given their advantages, there has to be a catch otherwise everyone would use non-parametric tests all the time. In fact, they are usually used as a last resort. One problem with non-parametric tests is that if the data are actually appropriate for a parametric test the equivalent non-parametric test will be less powerful (i.e. less likely to find a difference even if there really is one). For some tests the difference is not enormous—for example, if data are suitable for a t-test the Mann-Whitney U-test is about 95% as powerful as the t-test. For this reason, an appropriate transformation followed by a parametric test will yield a more powerful analysis. A second limitation of non-parametric tests is that by their very nature, they are less informative than parametric tests. For example, if we find a significant differences between group medians using a Kruskal-Wallis test, it can be difficult to understand which differences are driving the significant effect (methods are available, but they are not easy to use). On the other hand, if we can determine a suitable transformation and use an ANOVA model we can deploy tools such as multiple comparison tests to better understand the data. Parametric models make stronger distributional assumptions about the data, but in some ways they are much more flexible than non-parametric tests, i.e. there are non-parametric equivalents to some parametric tests, but there are many parametric tests for which there is no readily available non-parametric equivalent (e.g., the more complex designs of ANOVA). There is a non-parametric equivalent to ANOVA for complete randomized block design with one treatment factor, called Friedman’s test (available via the friedman.test function in R), but beyond that the options are very limited unless we are able to use advanced techniques such as the bootstrap. 33.8.3 Parting words Inappropriately distributed data can result in incorrectly high or low p-values. We should choose the statistical model we use on the basis of what the data look like in relation to the assumptions of the model, and reasons in principle, even if not clearly evident in a small sample, why the population from which the data are drawn might be expected to violate the test assumptions. On the principle that biological data rarely, if ever fully comply with the assumptions of parametric tests, it is sometimes advocated that non-parametric tests should always be used. This is not very good advice. There is more to statistics than just calculating p-values, and where possible, we prefer readily intepretable models to more ‘black box’ approaches. Use both approaches as appropriate and be aware of the strengths and weaknesses of each. There is an alternative to the Wilcoxon test which doesn’t make an assumption about the distribution being symmetrical—called the sign test. This can be done in R, but there isn’t a dedicated function for the sign test, and in any case, it is not very powerful with smaller sample sizes.↩ There are two different ways to use the wilcox.test function for a paired design. These are analogous to the two methods used to carry out a paired sample t-test: the first uses the raw data and makes use of the R’s formula system. The second supplies the function with a vector of differences↩ Of course, if we really were interested in the difference between sycamore, oak and rowan we should use a different test.↩ "],
["choosing-models-and-tests.html", "A Choosing models and tests A.1 Introduction A.2 Getting started… A.3 A key to choosing statistical models and tests A.4 Four main types of question A.5 Question 1 –- Comparison of group means or medians A.6 Question 2 – Associations between two variables? A.7 Question 3 -– Frequencies of categorical data A.8 Variables or categories?", " A Choosing models and tests A.1 Introduction One of the more difficult skills in data analysis is deciding which statistical models and tests to use in a particular situation. This book has introduced a range of different approaches, and has demonstrated a variety of biological questions that can be addressed with these tools. Here we draw together the statistical tools we’ve encountered and explain how to match them to particular kinds of question or hypothesis. However, before diving into an analysis, there are a few things to consider… A.1.1 Do we need to carry out a statistical analysis? This may seem like an odd question to ask, having just spent a considerable amount of time learning statistics. But it is an important one. There are many situations in which we don’t need, or can’t use, statistical tools. Here are two common ones: There are no statistical procedures that will allow us to analyse our data correctly36. This happens sometimes. Even with careful planning, things don’t always work out as anticipated and we end up with data that cannot be analysed with a technique we know about. If the data can’t be analysed in a sensible way there is no point doing any old analysis just because we feel we have to produce a p-value. Instead, it’s time to seek some advice. We could quite correctly apply a statistical test to your data, but it would be entirely superfluous. We don’t always need statistics to tell us what is going on. An effect may be exceptionally strong and clear, or it may be that the importance of the result is not something captured by applying a particular statistical model or test37. This caveat is particularly relevant to exploratory studies, where the goal is to use the data to generate new hypotheses, rather than test a priori hypotheses. Also in the category of superfluous statistics are situations where we are using statistics in a technically correct way, but we’re evaluating effects that simply are not interesting or relevant to the question in hand. This often arises from a misplaced worry that unless we have lots of statistics in our study it somehow isn’t ‘scientific’, so we apply them to everything in the hope that a reader will be impressed. Resist the temptation! This strategy will have the opposite effect—a competent reader will just assume you don’t know what you’re doing if they see a load of pointless analyses. A.2 Getting started… If there is a need for statistical analysis then the first thing to do is read the data into R and then… resist the temptation to start generating p-values! There are a number of things to run through before leaping into a statistical analysis: Be sure to carefully review the data after importing it into R (functions like View and glimpse is good for this). There are a number of things to look out for… Understand how the data is organised Most importantly, is the data set ‘tidy’? Each variable should be one column and each row should correspond to one observation. The majority of statistical modelling tools in R expect the data to be organised in this format, as do dplyr and ggplot2. If it isn’t already tidy, a data set will need to be reorganised first. We can always do this by hand, but it is almost always quicker to do it with R (the tidyr package is really good at this). Understand how R has encoded the variables. Examine the data using functions like glimpse or str. Pay close attention to the variable types—is it numeric, a character, or a factor? If a variable is not appropriate for the planned analysis, make any necessary changes. For example, if we plan to treat a variable as a factor because we’re going to carry out ANOVA, but it has been read in as a number, we’d need to convert it to a factor before preceding. Check whether or not there are any missing values. These appear as NA in R. If they are present, were they expected? If not, check the original data source to determine what has happened. If we’re absolutely certain the missing values represent an error in the way the data were coded then it might be sensible to fix the source data. However, if there is any doubt about how they arose, it is better to leave the source data alone and deal with the miscoded NAs in the R script. Ensure there are no miscoded values. In an ideal world we should leave the source data alone and fix these miscoded values in the R script. Why? Because changing the source data is slow and runs the risk of introducing new errors. It’s easy to edit an R script and rerun it. Editing source data is time consuming and error prone. If this is too difficult then fix these in the source data. Either way, there’s no point starting an analysis until the data are error free. Spend some time thinking about the variables in the data set. Which ones are relevant to the question in hand? If appropriate, decide which variable is the dependent variable (the ‘y’ variable) and which variable(s) is (are) the independent variable(s)? What kind of variables are we dealing with—ratio or interval scale numeric variables, ordinal or nominal categorical variables? It is much easier to determine which analysis options are available once these details are straightened out. Make at least one figure to visualise the data. We have done this throughout this book. This wasn’t to fill the time—it is a crucial step in any data analysis exercise. Informative figures allow us spot potential problems with the data and they give us a way to evaluate our question before diving into an analysis. If we can’t see an appropriate way to visualise the data, we probably aren’t ready to start doing the statistics! Steps 1-3 are all critical components of ‘real world’ data analysis. It may be tempting to skip these and just get on with the statistics, especially when pressed for time. Don’t do this! The ‘skip to the stats’ strategy always leads to a lot of time being wasted, either because we fail to spot potential problems or because we end up carrying out an inappropriate analysis. A.3 A key to choosing statistical models and tests The choice of statistical model/test is affected by two things: The kind of question we are asking. The nature of data we have: what type of variables: ratio, interval, ordinal or nominal? are the assumptions of a particular model or test satisfied by the data? The schematic key (below) provides a overview of the statistical models and tests we’ve covered in this book, structured in the form of a key. The different choices in the key are determined by a combination of the type of question being asked, and the nature of the data under consideration. Figure A.1: A key for choosing statistical models and tests. The notes that follow the key expand on some of the issues it summarises and explain some of the trickier elements of deciding what to do. The key is quite large, so it is hard to read easily a web browser (it also doesn’t render very well in Firefox for some reason). Either download a PDF copy of the key or right click on the figure to open it on its own in a new tab and then zoom in. A.4 Four main types of question Question 1: Are there significant differences between the means or medians (‘central tendency’) of a variable in two or more groups, or between a single mean or median and a theoretical value? This first question is relevant when we have measurements of one variable (e.g. plant height) on each experimental unit (e.g. individual plants) and experimental units are in different groups. If there is more than one group, one or more variables (factors) are used to encode group membership (given by the factor levels). Keep in mind that these grouping factors are distinct from the variable being analysed—they essentially describe the study design. This type of question includes anything where a comparison is being made between the variable in one group and… a single theoretical value another group whose values are independent of those in the first group (independent design) more than one other group a second group which has values that form logical pairs with those in the first group (paired design)38 The measurement scale of the variable in these situations may be ratio, interval, or ordinal. The only variable for which the statistical tools described here would not be suitable are categorical variables Question 2: Is there a association between two variables? What is the equation that describes the dependence of one variable on the other? Where Question 1 is concerned with comparing distinct groups, where we have measurements of one variable on each experimental units, Question 2 occurs where we’ve taken two different measurements for each experimental unit (e.g., plant size and number of seeds produced). Here we are interested in asking whether, and perhaps how, the two variables are associated with each other. Here, again, the measurement scale of the variable in these situations may be ratio, interval, or ordinal. Question 3: Are there significant differences between the observed and expected number of objects or events classified by one or more categorical variables? Question 3 is the only question which is focused on the analysis of categorical variables. Here we have situations where the ‘measurements’ on objects can be things like colour, species, sex, etc. In these situations we analyse the data by counting up how many of the objects fall into each category, or combination of categories. The frequency of counts across the categories can then be tested against the against some predicted pattern. A.5 Question 1 –- Comparison of group means or medians A.5.1 Question 1 How many groups? Within the set of situations covered by Question 1, there are some further subdivisions: we need to decide whether: we have one group only (and a theoretical or expected value to compare it against), we have two groups, or we have more than two groups. Usually this should be a fairly straightforward decision. Confusion sometimes arises in situations similar to Festuca experiment, where the observations are classified by two factors (pH and and Calluna). What exactly do we mean by ‘a group’ in this situation? The answer is that if we have more than one factor, then we must think of each group as being the set of observations defined by each combination of factor levels. So in the case of the Festuca experiment, there were four different combinations of the two factors (with and without Calluna, at either pH level). The simplest thing to remember is that if we have more than one factor (regardless of how many levels the factors have) then we must be dealing with more than two groups. A.5.2 [Question 1] Single group When we have a single group (1a) the only thing that remains to be done is check the type and distribution of the variable. If the data are approximately normally distributed then the obvious test is a one-sample t-test. If the data are not suitable for a t-test, even after transformation, then we could use a Wilcoxon test (we studied this in terms of a paired design, but remember, a paired experimental design is ultimately reduced to a one-sample test). A.5.3 [Question 1] Two groups If we have two groups then there is a further choice to be made: whether there is a logical pairing between variable measured in the two groups, or whether the data can be regarded as independent. This sometimes causes problems, particularly where the pairing is not of the obvious sort. One useful rule-of-thumb is to ask whether there is more than one way the data could be ‘paired’. If there is any uncertainty about how the pairing should be done, that is probably an indication that it isn’t a paired situation. The most common problem, however, is failing to recognise pairing when it exists. When faced with paired design the test involves first deriving a new variable from the differences among pairs, and then using this variable in a one-sample test. Either a one-sample t-test or Wilcoxon paired-sample test is required, depending on whether the new variable (the differences) is approximately normally distributed or not. If the data are independent then a two-sample t-test or Mann-Whitney -test will be the best approach. A.5.4 [Question 1] More than two groups The first decision here is about the structure of the data. This sometimes causes problems. There are a variety of different situations in which we may be interested in testing for differences between several means (or perhaps medians). Most of the time these will involve either a one-way comparison in which each observation can be classified as coming from one set of treatments (one factor), or a two-way comparison in which each value comes from a combination of two different sets of treatments (two factors). It is easy to mix these situations up if we’re not paying attention. One way to try and establish the structure of the data is to draw up a table… If the data fit neatly into the sort of table below, where there is one factor (e.g. factor ‘A’) which has two or more levels (e.g. level 1, 2, 3) then we have a one-way design. The treatments designated by the levels of the factor in this situation are typically related in some way (e.g. concentrations of pesticide, temperatures). The only question it makes sense to address with these data is whether there are differences among the means of the three treatments. FACTOR A Treatment 1 1,4,6,2,9 Treatment 2 7,3,8,9,4 Treatment 3 5,3,7,6,4 If there are two different types of treatment factor (factors ‘A’ and ‘B’) and within each factor there are two or more levels then we have a two-way design. The treatments designated by the levels of a particular factor are typically related in some way, but the set of treatments associated with each factors are typically not related to one another. We could not draw up a table like the first one (above) and fit these data into it, because each observation occurs simultaneously in one treatment level from each of the two factors (below). Treatment B1 Treatment B2 Treatment B3 Treatment A1 1,4,6 3,9,1 2,2,7 Treatment A2 7,3,8 2,3,6 9,3,4 Treatment A3 5,3,7 1,8,6 2,2,6 From the data in this design, we can ask more than one question (i.e. there are several different tests associated with this design). It clearly makes sense to ask questions about the main effects and the interaction: main effect of factor A: difference among row means main effect of factor B: difference among column means interaction: differences among means for each different combination of treatments of factors A and B (individual cells of the table). When a data set can be described by the second table, but there is only one value in each cell of the table, then we still have a two-way design, but now without replication. It is possible to analyse this design, but we there is no way to assess the interaction between treatments—we can only test main effects with this kind of design. This design is most often associated with a Randomized Complete Block Design with one treatment factor (the thing we care about) and one blocking factor (a nuisance factor). Having established whether we have a one-way or two-way design we need to determine whether the data are likely to satisfy the assumptions of the model we’re presented with. We can start to make this evaluation by plotting the raw data (e.g. using a series dot plots). Occasionally it will be obvious at this stage that the data are not suitable for ANOVA. However, things are often not so clear so it’s better to fit the ANOVA model and use regression diagnostics to check whether the assumptions are satisfied. In the case of a one-way design we have the option of a non-parametric Kruskal-Wallis test when the data are not suitable for ANOVA. Otherwise we need to find a suitable transformation and then use a one-way ANOVA. Remember, before turning to a Kruskal-Wallis test it is a good idea to see if the data can be made suitable for ANOVA by transformation. In the case of the two-way design, we don’t have the option of a non-parametric test. If the data are suitable (or can be made suitable by transformation), then use a two-way ANOVA, otherwise there is no choice but to break the analysis down into its component parts, which can be analysed as one-way designs using non-parametric methods. This is far from ideal though because we lose all information about the interactions by doing this (which somewhat defeats to purpose of designing a two-way experiment). (N.B. In the special case of a two-way design without replication, then there is a non-parametric test—Friedman’s test—that can be used instead of normal two-way ANOVA.) Finally, we should consider the option of multiple comparison tests. These should only be used if the global significance test from an ANOVA is significant. Additionally, in the two-way ANOVA, there is a further consideration, which is whether the main effects (one or both of them) or the interaction are significant. If the interaction is significant then the multiple comparison should be done for the interaction means (i.e. the means in each treatment combination). If the interaction is not significant, then the significance of the differences between the main effect means (whichever are significant) should be evaluated. A.6 Question 2 – Associations between two variables? Assuming it is an association we’re after—not a difference between groups (see box below)—then the main decision we need to make is whether to use a correlation or regression analysis. A.6.1 [Question 2] Testing \\(y\\) as a function of \\(x\\), or an association between \\(x\\) and \\(y\\)? The choice between regression and correlation techniques, for analysing the relationship between two variables, depends on the nature of the data and the purpose of the analysis. If the purpose of the analysis is to find (and describe mathematically) the relationship that describes the dependence of one variable (\\(y\\)) on another (\\(x\\)), then this points to a regression being the most appropriate technique. If we just want to know whether there is an association between two variables, then this would suggest correlation. However, it is not just the goal of an analysis that matters, unfortunately! The two techniques also make different assumptions about the data. For example, regression makes the assumption that the \\(x\\)-variable is measured with little error relative to the \\(y\\)-variable, but doesn’t require the \\(x\\)-variable to be normally distributed. Pearson’s correlation assumes that both \\(x\\) and \\(y\\) are normally distributed. So the final decision about which method to use may depend on trying to match up both the question and the nature of the data. It sometimes happens that we want to ask a question that requires a regression approach (e.g. we need an equation that describes a relationship) but the data are not suitable. In this situation it can be worth proceeding with regression, bearing in mind that the answer we get may be less accurate than it should be (though careful use of transformations may improve things). Once we decide a correlation is appropriate, then the choice of parametric or non-parametric test should be based on the extent to which the data match the assumptions of the parametric test. A final point here that can cause difficulties is the issue of dependence of one variable on another. In biological terms we are often interested in the relationship between two variables, one of which we know is biologically dependent on the other. However, designating one variable the dependent variable (\\(y\\)) and the other the independent variable (\\(x\\)) does not imply that we think ‘y depends on x’, or that ‘x causes y’. For example, tooth wear in a mammal is dependent on age. However, imagine that we have collected a number of samples of mammalian teeth from individuals of a species which have died for various reasons, and for which we also have an estimate of age at death. We may want to find the an equation for the relationship between age (\\(y\\)) and tooth wear (\\(x\\)). Why? Well for example, in a population study you might often recover remains of individuals that have died, from which you can obtain the teeth (even if not much else remains). It could be useful to be able to use the measurement of tooth wear to estimate the age of the individuals, and to do this you want the equation that describes the ‘dependence’ of age on tooth wear. So here the direction of dependence in the analysis is not the same as the causal dependence of the two variables. The point is that the choice of analysis does not determine the direction of biological dependence—it is up to us to do the analysis in a way that makes sense for the purpose of the study. Qustion 1 or Question 2? Although it seems straightforward to choose between Question 1 and Question 2, it does sometimes cause a problem in the situation there are two groups, the data are paired, and the same variable has been measured in each ‘group’. Because two sets of measurements on the same objects (say individual organisms) fit the structure a paired-sample t-test or a regression (or correlation), it is very important to identify clearly the effect we want to test for. A concrete example will help make this clearer… The situation where confusion most easily arises is when the same variable has been measured in both groups. Imagine we’ve collected data on bone strength from males and females in twenty families, where the males and females are siblings—a brother and sister from each family. The pairing clearly makes sense because the siblings are genetically related and likely to have grown up in similar environments. Consider the following two situations… If our goal is to test whether males and females tend to have different bone strengths, then a paired-sample t-test makes sense: it compares males with females controlling for differences due to relatedness and environment. If our goal is to test whether bone strength runs in families then the t-test is no use. In this case it makes sense to evaluate whether there is a correlation in the bone strength of sibling pairs (i.e. if one sibling has high bone strength then does the other as well?). So while the data can be analysed in either way, it is the question we’re asking that is the critical thing to consider. Just because data can be analysed in a particular way, doesn’t mean that analysis will tell us what we want to know. One way to do tackle this sort of situation is to imagine what the result of each test using those data might look like and think about how one might interpret that result. Does that interpretation answer the right question? A.7 Question 3 -– Frequencies of categorical data This kind of question relates to categorical or qualitative data, (e.g. the number of male versus female offspring; the number of black versus red ladybirds, the number of plants of different species). The data are frequencies (counts, not means) of the number of objects or events belonging to each category. The principle of testing such data is that the observed frequencies in each category are compared with expected (predicted) frequencies in each category. Deciding between goodness of fit tests and contingency tables is generally fairly straightforward once we’ve determined whether counts are classified by a single categorical variable, or more than one categorical variable. If there is more than one categorical variable, then it should be possible to classify each observation (organism, event, habitat, location, etc.) into one category of each categorical variable, and that allocation should be unique. Each observation should fit in only one combination of categories. There is a further difference which may also affect our choice. In the case of a single factor, the question we ask is whether the numbers of objects in each category differ from some expected value. The expected values might be that there are equal numbers in each category, but could be something more complicated—it is entirely dependent on the question we’re asking. In the case of the two-variable classification, the test addresses one specific question: is there an association between the two factors? The expected numbers are generated automatically based on what would be expected if the frequencies of objects in the different categories of one factor were unaffected by their classification by the other factor. A.8 Variables or categories? One final issue that sometimes causes questions is when a variable is treated as categorical when it is really a continuous measure or when a continuous variable is made into categories. There is indeed some blurring of the boundaries here. Two situations are discussed below. A.8.1 ANOVA vs. regression There are many situations in which data may be suitable for analysis by regression or one-way ANOVA, even though they are different kinds of models. For example, if a farmer wishes to determine the optimal amount of fertiliser to add to fields to achieve maximum crop yield, he might set up a trial with 5 control plots and 5 replicate plots for each of 4 levels of fertiliser treatment: 10, 20, 40 and 80 kg ha NPK (nitrogen, phosphorus and potassium fertiliser) and measure the crop yield in each replicate plot at the end of the growing season (kg ha year). If we are simply interested in determining whether there is a significant difference in yields from different fertiliser treatments, and if so which dose from the levels we have used is best, then ANOVA (and multiple comparisons) is probably the best technique. On the other hand we might be interested in working out the general relationship between fertiliser dose and yield, perhaps in order to be able to make predictions about the yield at other doses than those we have tested. If the relationship between fertiliser and yield was linear, or could be made so by transformation, then we could use a regression to determine whether there was a significant relationship between the two, and describe it mathematically39. A.8.2 Making categories out of continuous measures Sometimes you will have data which are, at least in principle, continuous measurements (e.g. the abundance of an organism at different sites), but have been grouped into categories (e.g. abundance categories such as 1-100, 101-200, 201-300, etc. ). One question is whether these count as categories – and hence whether for example you could look at the association between abundance categories and habitat by taking the frequencies of samples in each abundance category across two different habitats and examining the association using a contingency table test. The answer is that this would indeed be a perfectly legitimate procedure (though it may not be the most powerful). It is a good idea not to group numeric data into categories if it can be avoided because this throws away information. However, it isn’t always possible to make a proper measurement, though we can at least assign observations to ordinal categories. For example, when observing flocks of birds we might find that it’s impossible to count them properly, but we can reliably place the numbers of birds in a flock into abundance categories (1-100, 101-200, etc. ). Many variables that we treat as ordinal categories could in principle be measured in a more continuous form: ‘unpigmented’, ‘lightly pigmented’, ‘heavily pigmented’; ‘yellow’, ‘orange’, ‘red’; ‘large’, ‘small’… These are all convenient categories, but in one sense they are fairly arbitrary. This doesn’t mean that we can’t construct an analysis using these categories. However, one thing to bear in mind is that when we divide a continuous variable into categories, decisions where to draw the boundaries will affect the pattern of the results. And, of course, there are many ‘true’ categorical data: on the whole things like male/female, species, dead/alive, can be regarded as fairly unambiguous categories. Alternatively, an appropriate technique exists, but we don’t know about it!↩ As the old joke goes: What does a statistician call it when ten mice have their heads cut off and one survives? Not significant.↩ In a paired design the two groups aren’t really separate, independent entities, in the sense that pairs of measurements have been taken from the same physical ‘thing’ (site, animal, tree, etc).↩ One additional potential advantage of regression in this kind of situation is that it might result in more a powerful statistical test of fertiliser effects than ANOVA. This is because a regression model only ‘uses up’ two degrees of freedom—one for each of the intercept and slope—while ANOVA uses four (n-1). A regression makes stronger assumptions about the data though, because it assumes a linear relationship between crop yield and fertiliser.↩ "],
["writing-a-scientific-report.html", "B Writing a scientific report B.1 Introduction B.2 The structure of a scientific report B.3 Presenting species names B.4 Approaches to writing B.5 Further reading", " B Writing a scientific report “The question is,” said Alice, “whether you can make words mean so many different things.” “The question is,” said Humpty Dumpty, “which is to be master – that’s all.” Lewis Carroll, Through the Looking Glass (1871) B.1 Introduction Scientific information is communicated in a variety of ways, through talks and seminars, through posters at meetings, but mainly through scientific papers. Papers, published in books or journals provide the main route by which the substance of scientific findings are made available to others, for evaluation and subsequent use. Over time the scientific paper has developed into a fairly formal method of communication, with certain structures, styles and conventions. These mean that information is presented in a standardised way, meaning particular bits of information can be extracted more easily by a knowledgeable reader. In this chapter, we will examine the structure and conventions of a biological paper, using an example of a field study of the territorial behaviour of a damselfly. Our aim is to illustrate the typical form and content of a scientific paper. Of course papers vary in their exact requirements, and no one example can cover all the possibilities. Read recent papers in a relevant subject area and analyse what styles and structures they use, and which work best. The structures and conventions discussed below are not rules and should be flexibly interpreted, under the guiding principle that the aim is to present information as clearly, concisely and unambiguously as possible. Although taking the scientific paper as a model, the principles here apply equally to other, less formal project write-ups and reports. B.2 The structure of a scientific report The normal scientific report has a standard structure (parts in parentheses are optional): Title Abstract / Summary Introduction Methods Results Discussion (Acknowledgements) Literature cited (Appendix) B.2.1 Title Although not really a section of the paper, it is worth giving the title some thought. Aim for something that gives a fairly specific description of the topic of the paper, and possibly the essential result, but without being too long: Diurnal changes in the depth distribution of copepods in lakes with and without planktivorous fish: evidence of a predator avoidance mechanism? An experimental study of the effect of food supply on laying date in the coot. The distribution and altitudinal limits of bracken (Pteridium aquilinum) in the North York Moors National Park. Reverse transcription-PCR detection of LaCrosse virus in mosquitoes and comparison with enzyme immunoassay and virus isolation. The important thing to note is that the titles contain a good deal of specific information—we have a pretty good idea what the paper is about before we read it. Avoid vague titles such as… A study of damselfly behaviour …when in fact we have looked at is the mating and oviposition behaviour of damselflies of a particular species in relation to the current speed in different areas of the river and what we want to say is: The influence of river flow rate on mating behaviour and oviposition in the damselfly Calopteryx splendens Don’t put irrelevant specific information in the title. It might be irrelevant to say that we carried out a study in a particular river—that detail is probably not important for the question we’re asking . The reference to the North York Moors above, however, is relevant because the study is of an area-specific problem (the study is primarily of use to people who want to know about bracken in that area). B.2.2 Abstract or Summary The purpose of an abstract is to present a factual summary of the main purpose, results and conclusions of the report which is short and makes sense on its own. Often it is best (and some journals require it) to do this as 3-6 numbered points comprising some, or all, the following: The scope and purpose of the study Methods (not always necessary) Result 1 Result 2… Conclusion For example: The territorial behaviour, mating frequency and oviposition of Calopteryx splendens (Charpentier) (Odonata: Calopterygidae) were studied in relation to the water flow rate in the territories (weed patches) of individual males. Weed patches with faster flow rates appeared to be preferentially selected by males, and more vigorously defended. Weed patches in slow or still water were often unoccupied. Experimental reduction of flow rate in individual patches caused males to desert previously defended territories. Males had greater mating success on territories with higher flow rates and more ovipositions were observed in these patches. It is not known why weed patches with faster flows seem to be better quality sites for Calopteryx oviposition, but possible reasons include higher oxygen levels for developing eggs and better protection from egg parasitoids. B.2.3 Introduction The introduction should: set the background to the question, using the literature (Why is it interesting? Why is it important?). state the question, hypotheses and predictions. (What is it that we’re actually investigating?) briefly state what the study does (What is in this paper?) Start with brief general statements to put the study into its broader context: Oviposition site selection by female insects can be a critical factor in offspring survival, and hence fitness (Smith 1981). In some insects, notably many of the Odonata, males occupy or defend oviposition sites and mate with arriving females before allowing them to oviposit at that site. Males in such systems benefit in two ways from defending high quality sites: mating with all females ovipositing at the site ensures their offspring will have higher survival, and by occupying high quality sites, they will have access to more females (Jones 1976). Then move on to more specific detail about the type of system: In calopterygid damselflies females oviposit in the submerged stems of aquatic plants in streams and small rivers (Hines 1956, Norman 1968). Males defend patches of weed… Then develop the question: It has been repeatedly observed that many weed patches are always occupied and are the subject of much territorial dispute amongst males, whilst others remain unoccupied or uncontested (Gateman and Nunn 1978, Speake 1982, Mollison 1987). This suggests substantial differences in patch quality, but the basis of this difference is not known. Since the larvae may disperse after hatching, the underwater environment of a weed patch seems most likely to be important for survival and development of the eggs. One important physical factor which could influence the environment in a weed patch, and which may vary considerably in different parts of the river channel, is flow rate. We therefore hypothesised that flow rates could be an important determinant of patch quality. Say what the study actually does: In this study we investigated the physico-chemical differences between ‘good’ and ‘poor’ quality patches of weed as defined by the behaviour of the damselfly Calopteryx virgo Linnaeus. We also tested the assumption that males on more vigorously defended patches have greater mating success. Don’t separate out the question, hypothesis and predictions as special statements in bold or whatever, or put them under separate headings. Although they provide vitally important context to a study they should simply appear where necessary as part of the normal text. B.2.4 Methods The Methods section should provide enough information about how the study was carried out to enable the reader to evaluate the validity of the results. What was done? Where (usually necessary for field work) ? When (may be necessary for seasonally dependent studies) ? Why (may be necessary to justify the use of a particular approach) ? It is often said that we should write the methods so that someone could repeat what we’ve have done exactly. This is OK in principle, but often takes an excessive amount of space and shouldn’t be the overriding principle. The emphasis should be on giving the reader sufficient information to evaluate the results of a study. Focus on the important detail: i.e., it doesn’t matter that we sorted our sample into Petri dishes, or which make of microscope we used, but it does matter that we worked at 20x magnification, because that may determine how likely it is that we missed very small items. The main exception to this is if we’re reporting a novel technique that other people are likely to want to use. In that case more detail than normal will be required. Be concise. There is no need to explain the details of standard procedures. If we’re using a procedure described by someone else then summarise the essential features and just cite the reference for the method. In the Methods it is not necessary to state which statistical tests were used unless they are non-standard or require particular discussion (e.g. it is useful to state that the data were transformed before analysis). Similarly, we don’t need to state what statistics package were used for standard statistical procedures. Avoid ‘padding’ sentences that just waste space, such as: The data were analysed statistically and by plotting graphs to see what the results were. The standard style in scientific reports is to write in the third person (“Experimental plots were marked out” rather than “We marked out experimental plots”). This an area where the accepted conventions vary between different areas of biology. In some the use of the first person, where it enhances readability of the text, is permitted and even encouraged. Judicious use of ‘I’ or ‘we’ can improve the clarity and readability of text and should be used where appropriate. Just keep in mind that the use of the first person is not accepted in some disciplines. If in doubt it is safest to stick with the third person approach. Also try to use the active voice, i.e., avoid this style… It was found that males always defended single weed patches. … and and use this style instead: Males always defended single weed patches. A final point is that when a study is made up of several experiments, or sets of observations, it is a good idea to use appropriate subheadings to make it easier for the reader to follow, both within a particular section (such as the Methods) but then also using the equivalent subheadings to organise the Results and possibly the Discussion. e.g., MATERIALS AND METHODS Study site Territory occupancy by males Oviposition behaviour Experimental manipulations of flow rate B.2.5 Results The central goal of a results section is to provide a clear account of the material factual findings of the investigation, using a combination of text, summarised data, and figures. If different parts of the study are covered under different subheadings in the Methods, then use the same subheadings (where relevant) to organize the Results. The Results section should focus on explaining clearly what the results are, but should not contain discussion of the biological implications of the results. Results are presented in a variety of different ways: Text. The text part is important. This must include clear statements of the results. No result should just be presented just as a figure or a table with no corresponding statement in the text. It is important to lead the reader through the information, bringing out the important features. This does not mean that we should duplicate information in text and figures, or tables, but if a figure is used then there should be a reference to that figure in the text, which summarizes the result. Data. Numerical data are normally presented in tables (see below), but sometimes ‘stand-alone’ simple numerical results can be given in the text. In either case the data is normally be presented in summarised form only (e.g. means and standard deviations). Presenting raw data may be appropriate if there are very few data, or there is a legitimate need to discuss the values of specific data points, but this is rarely the case. Don’t include big tables of raw data. If we want include the raw data (usually only the case if the data set may be of use to others as a basis for further analyses), they should go in as an appendix. Alternatively, if our goal is to share our data with the rest of the world we might consider using an online data repository like Figshare. Statistical summaries. The results should be where most or all of the statistical results appear. There are three places to include summaries of statistics: In the text… In figure legends… In tables. If there are large numbers of tests to present which would clutter the text, e.g. the analysis involves 10 regressions of the same kinds of variables, for a number of different taxa, then it may be convenient to summarise the slopes, intercepts and significance of the relationships in a table rather than trying to put all ten in the text (although a figure would be even better). Be sure to report statistical results in full: include the test statistic, degrees of freedom (both of them, when reporting an F-test), and the p-value. Figures and Tables. Any type of graphical presentation is a figure. A table has just text and numbers. All figures and tables must be referred to in the text of the Results (or elsewhere). Tables… contain just text and numbers have the legend at the top use just horizontal lines as separators are labelled: Table 1, Table 2, etc. Table 2. The flow rates of the manipulated patches, and mean simultaneous number and turnover (number of different males per day) of territorial males on experimental patches. (Values in parentheses are standard errors for each mean). Experimental treatment Mean flow rate (m s-1) Mean number of males per patch Mean turnover of males per patch Increased flow 0.45 (0.11) 1.20 (0.22) 1.2 (0.4) Increased flow 0.18 (0.09) 0.18 (0.09) 3.1 (0.6) Increased flow 0.02 (0.01) 0.10 (0.03) 5.9 (0.9) Figures… have a legend at the bottom should be labelled: Figure 1, Figure 2, etc. can comprise a single graph or diagram, but a single figure can also comprise several graphs – in which case each should be labelled: a, b, c, etc. Preparing a manuscript for publication Word processing software makes it easy to incorporate tables and figures directly into a document. When writing a report which is not going to be typeset, then this allows the production of very professional looking documents. However, if the material is for publication, then the printers will usually require the text and figures (and sometimes tables) on separate pages. If sending a manuscript for publication, then the figures and tables should be put at the end of the document (after all the text and references), but in a normal document if a table or figure occurs on a separate page then the page should immediately follow the first point at which the figure is referred to in the text. B.2.6 Discussion The function of the discussion is to consider the meaning of the results and the light they throw on the original question; to assess the results in the context of other studies; and, if appropriate, to consider the limitations of the work and future directions for study. The exact structure and content of the Discussion will vary somewhat depending on the particular study and what the results show, but usually the following components should be included. It is usually helpful to start the discussion with a short paragraph, or so, summarizing the key results. e.g., Calopteryx females exhibit a distinct preference for weed beds in faster flowing water as oviposition sites. Males recognise such good quality sites and occupy and defend them against other males, ignoring weed beds in slower water. This results in more copulations for males which occupy the fast flowing sites. The assessment and response of males to flow rate changes can occur within a few hours Next, we usually consider the whether the results support the hypothesis or suggest it requires modification or rejection. e.g., The male damselflies’ preferential occupancy and vigorous defence of weed patches with faster flow rates, combined with a clear positive relationship between flow rate and oviposition frequency, provides strong support for the view that the underwater environment is an important determinant of oviposition site quality. It may be important to discuss the limitations of the study and the appropriate direction for further work, but these are not always required. Don’t pad out the discussion with endless text considering every possible wrinkle in the study. When appropriate, a discussion of the limitations should be brief and to the point. Although the results do implicate flow rate as a determinant of oviposition site, it is not clear whether females are responding directly to flow rate, or whether males are assessing flow rate and females are selecting the higher quality males (presumably those that occupy the best patches) assessed in some other way. This would require a separate experiment where females were allowed to select oviposition sites in the absence of males. Don’t just grumble and don’t make stock criticisms without good reason (e.g., don’t automatically say it would have been better to have a larger sample size—this may be true, but it may not—large sample sizes don’t solve everything). There may be unresolved, or unsolvable, problems. Be honest about these, but also be positive: if the author don’t seem to be sure that a study is worth reporting, how will anyone else be convinced? A report does not require a section headed ‘Experimental Error’. Similarly don’t attribute any problems that can’t be explained to experimental error; everyone knows measurements aren’t perfect so it doesn’t explain anything. Finally, bring out the wider implications (but be realistic about the significance of the work) and future directions, e.g. These results indicate selection of oviposition sites, by females, on the basis of flow rate, but the reasons for such selectivity are not known. Flow rate has been implicated in other studies of aquatic insects as being of importance for preventing low oxygen conditions developing (a stress to which developing eggs may be particularly sensitive) (Armherst 1989). High flow may also reduce the ability of egg parasitoids to search the plants (Girton and Jenner 1976). A critical part of assessing the basis of site choice, and evaluating the role of the underwater environment will be measurement of egg and larval survival in weed beds of different flow rates. It seems likely that the patterns observed in Calopteryx in a single section of the river may also be important in determining choice of habitat between different river sections or even different rivers with high or flow rates. This also raises the unwelcome possibility that quite subtle changes in flow caused by water abstraction and river regulation (a problem on a neighbouring stream to the study site) could cause marked interference with Calopteryx breeding and even loss of the species from a river system. The Discussion should not contain new results (except occasionally for small additional analyses of the data that have arisen as a direct consequence of interpretation of the main results - and that shed light upon the questions in the paper). Also avoid over-extending the implications of what was found. A slight trend in the results from one particular experiment may not be an entirely sound basis from which to challenge the fundamental tenets of evolutionary biology. (On the other hand it just could be; the skill is in spotting the few occasions when it is!). Overall, keep the focus of the discussion firmly on the results, don’t wander off into ten pages of philosophical discourse on the state of the field in general. And keep the volume and depth of the discussion in proportion to the rest of the paper, and to the significance (biological rather than statistical) of the results. B.2.7 Acknowledgements This is the place to acknowledge persons or organizations who have made significant contributions to the execution of the work. For example: funding bodies, people who have contributed ideas or assisted with some of the actual work, landowners giving access to sites, specialists who have made identifications and people who have read and commented on the manuscript. Don’t get carried away—there’s no need to thank every friend, relation and loved one for general help through life’s little crises. B.2.8 Literature cited / References This section should provide a complete listing of all, and only, references cited in the text of the report. There are three things to consider here: What to cite How to cite it in the text How to construct a reference list B.2.8.1 What to cite We should cite appropriate references wherever a point of substance (fact, or opinion) is made that is not our own or may not be regarded as common knowledge. e.g. Facts: Several species in the genus Calopteryx perform a complex `wing floating’ display as part of the courtship behaviour (Malmquist 1956) Opinion of others: This behaviour is generally considered to be a display of male quality (Fredenholm 1978, Summers 1991). Absence of citation is taken to indicate either the author’s own view or a result generated by the present study: The function of this behaviour may be to signal the flow rate, and hence quality of a patch, to a female. …or something sufficiently well known to be regarded as common knowledge: Damselflies are predatory both in the larval and adult stages. B.2.8.2 Styles of citation If writing a manuscript for publication in a scientific journal, obviously use the style of the journal in question (exactly—including punctuation). When writing any other type of report, we can choose our own style, but if in doubt the easiest approach is probably to follow the style of a major journal in the appropriate subject area. There are two main styles in widespread use: The most common (and most straightforward) cites references in the text using names and dates, and lists all references alphabetically in the reference list. In the text, e.g. Wide fluctuations in temperature reduce egg viability (Smith 1987). OR Smith (1987) found that wide variations in temperature reduced egg viability. And in the reference list: Smith, A. J. (1987) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47: 231-243. Note that the necessary information about the journal is the: journal title, the volume number (47) and the pages of the article (231-243). Journals often also have a part number, e.g., volume 47(2). There is no to include this in the citation; the page numbers should be sufficient. The list should be in alphabetical order by first author. If there is more than one reference by the same author then order them by date. If there are papers with the same first author but different second/third authors then these come after the single author papers by the first author, and in alphabetical order by second, third, etc. authors, e.g., Smith A J (1987)… Smith A J (1989)… Smith A J, Girton S and Mackay R H (1984)… Smith A J and Wallis K C (1983)… Smith A J and Wallis K C (1985)… If several citations by the same author in the same year are in the list, then denote them with letters e.g. In the text: Smith (1987a), Smith (1987b) In the list: Smith, A. J. (1987a) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47: 231-243 Smith, A. J. (1987b) The oviposition behaviour of Calopteryx virgo (Odonata: Zygoptera). Anim. Behav. 27: 197-209 The other main style is to use numerical superscripts (or equivalent) in the text, numbering the references in the order in which they are mentioned in the text, and ordering the final reference list in the same way, e.g. In the text: Wide fluctuations in temperature reduce egg viability23. Smith23 found that wide variations in temperature reduced egg viability. In the reference list: Wilcove H, Papapangiotou L A and Lu, X (1978) Mating strategies in a calopterygid damselfly. Anim. Behav. 16:21-30 Smith A J (1987) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47:231-243 Morris L L (1991) A model of territory switching behaviour. Am. Nat. 230:390-395 In many journals using this system, the titles of the references in the list are also omitted e.g. Smith A J (1987) 47: 231-243 This is done purely to save space, so unless we are specifically asked to do this it is best to include the complete reference. Although such numerical systems usually require the reference list to be ordered by number, it is possible (and much more convenient) to use an alphabetical listing even if numbers are used in the text (alphabetically ordered references are numbered in order and then the numbers used in the text instead of names). The advantage to a numbering system is that is saves space in the text, the disadvantage is that the numbers don’t tell a reader which paper is being referred to as they read—they have to keep looking them up in the list. Some final points to bear in mind about references and their citation: 1) every reference cited in the text must appear in the reference list, and every reference in the list must appear in the text; 2) we should never cite something we’ve not seen. If we need to cite something we have seen discussed or cited somewhere else, but haven’t seen our self (and cannot get hold of) we should make it clear that we’re citing someone else’s interpretation of the original reference, e.g., …Jones (1928—cited in Smith 1987) In the list we should then give the full citation for Smith (1987), not Jones (1928). There are standard abbreviations for journal names. These are often given in the journal itself, and are available on a list in the Library, or can be found by looking up the journal on Biological Abstracts. If we don’t know what the standard abbreviation is, and it is not obvious, then it’s best to use the full name rather than making up a new abbreviation. Use a reference manager! Managing citations and generating reference lists can be a painful process, especially when everything is done ‘manually’ (cut and paste, cut and paste, cut and paste…). These days a number of reasonably good reference managers (software packages) are available to help with the process of managing references, inserting citations, formatting, and generating reference lists. Endnote, Mendeley and Papers are probably the most popular, but there are many different options. Choose one, and learn how to use it. This will save a huge amount of effort in the long run. B.2.9 Appendices Use appendices for large amounts of raw data, long species lists, detailed mathematical or laboratory working, of a non-standard method, or (short) program listings, but only where the inclusion of such information markedly enhances the usefulness of the paper. Normally such appendices are not required. Avoid using them just to show how much work went into a study! B.3 Presenting species names A final thing worth noting, is the correct way to present species names (there is an example of it in the passage above). This causes a great deal of confusion, largely because it is not always appreciated that specific meaning attaches to the conventions used for presenting species names. The name above, Calopteryx virgo Linnaeus, has several distinct elements in its presentation (italics, upper case initial letter(s) etc.). These elements matter. The full meanings of each of the various elements one might find in a scientific name are too extensive to cover here, but the following guidelines should cover most situations. Presentation of common names is less fixed by convention than of scientific binomials but, in general, common names can be written with lower case initial letters unless the name itself contains a proper name (e.g. Norway spruce). Common names are written in the same typeface as the normal text. Common names can be used in reports, but the scientific binomial is a unique identifier that provides a standard, internationally recognized, label for a species. A report should always include the scientific name of the species we’re dealing with. Obviously it is important ensure the scientific name is spelled correctly. Fortunately systematists’ scientific latin is fairly simple phonetically, but nonetheless it is best to check the name from a reliable source when writing it for the first time (try searching for ‘calopterix’ on the web!). So now the spelling is right let’s look at the parts of the name and how to present them. In typeset documents we usually see: Calopteryx virgo The first name (the genus) should begin with an upper case letter, and the second name (the species) should begin with a lower case letter. This is not a style choice, it is a rule! Both names are usually written in italics, but sometimes they’re written in normal type and underlined. Underlining is a less-used alternative that derives from the fact that single underlining was the printers’ instruction to a typesetter to set the text in italic. This convention was widely used in the days before word-processors, as italic text was tricky to produce on a typewriter. Sometimes there will be more than just the genus and species names: Calopteryx virgo (Odonata: Calopterygidae) The names on the right (though they could equally well be on the left) are the higher taxonomic classification (order and family in this case) and are sometimes presented to enable a reader to easily place the organism. Just having a species name is not always very informative unless the reader already knows what sort of organisms are being discussed. These are written in normal text, but with an upper case initial letter. Just to confuse things though, when we write the informal derivative version of such names such as ‘odonate’ or ‘calopterygid’, e.g. …calopterygids, unlike other odonates… …then they have a lower case initial letter. If, as is occasionally the case, we’re describing the subspecies of an organism (e.g. Calopteryx splendens xanthostoma) then the sub-species name (xanthostoma) is formatted the same way as the species name. In the passage above the name of the damselfly is followed by a name: Linnaeus. This is the authority, the name of the taxonomist responsible for naming the species (in this case the famous Swedish systematist Carolus Linnaeus). Sometimes a year is also shown after the authority. Taxonomy changes as groups are revised and new classifications developed, and so species names are often not the same now as the ones they were originally given. This results in a complicated system of having more than one authority, dates, and authorities appearing in different sorts of brackets and parentheses, sometimes abbreviated, sometimes not, e.g., Calopteryx virgo Linnaeus 1758 Calopteryx splendens xanthostoma (Charpentier) Althea rosea (L.) Cavanille To present things correctly in a report there is no need to know exactly what all these different arrangements mean. The important thing to remember is that things like the arrangements of parentheses mean something specific – don’t just stick them in to make it look tidy. And when authorities are abbreviated (e.g. Linnaeus to L.) these abbreviations are fixed. We can’t just decide to abbreviate an authority to something that looks sensible. If these esoteric details are really needed then copy them carefully from a reliable source. When should we include the authority? In a scientific paper it is conventional to include the authority when the species is first mentioned (in the main text, not the abstract), and leave it out thereafter. However, for most other purposes there is no need to include the authority. Finally, abbreviation of names. Once we’ve given the full name of a species it is often convenient to refer to it in an abbreviated form later in the report: Females of C. virgo were regularly observed… There is only one correct way of abbreviating the name: shorten the genus to its initial letter (plus full stop) and keep the full specific name; never do the reverse (Calopteryx v.). If there is a subspecies name then it’s fine to abbreviate both generic and specific names, e.g., C. s. xanthostoma. B.4 Approaches to writing Everyone will actually tackle the task of writing a report in different ways, dictated by a combination of personal preference and practical constraints. However, it is worth making some general suggestions. Separate the process of writing, editing and revising—these are not the same thing. Begin with a clear written plan of what to say before starting, but expect to go through several cycles of writing, editing and revising a report before it is satisfactory (it is impossible to get everything right first time): When writing, focus on writing! Follow the plan and get the key ideas down, even if the text ends up a little ugly in places, and keep going. Avoid the urge to double back and polish up text. Editing is essentially a sentence-level process. Editing is about the minutiae. It addresses problems with spelling, grammar or word choice. This step doesn’t involve large, structural changes to a document. Revising concerns higher level criticism. Are the questions clearly elaborated? Is there a logical flow of ideas? Are the Results and Discussion connected. This step often involves moving (or removing) paragraphs, extending or narrowing text, or rewriting confusing text. Word processors greatly ease the task of editing and revising manuscripts but it is better to get a draft down and then work on it on paper, rather than agonising too long in front of the screen. It is much easier to get an overview of the structure, and to spot errors, from printed copy rather than trying to do it entirely on the computer, where we’re always peering at a fragment of the manuscript though a little window. It is generally suggested that starting with the Results and/or Methods sections is the easiest approach—these require straightforward reporting of factual information, and the pattern and presentation of results will be clearly established before it’s time to move onto the Discussion and Introduction. If writing the Discussion and Introduction is still a struggle, try writing the Abstract first—this will crystallize the key messages. Keep the writing simple, clear and concise. Science writing is about clear communication, not verbal acrobatics. Explain things to an appropriate level for the intended audience. This is harder than it sounds. By the time we reach the writing stage we have usually spent a good deal of time thinking about the problem we’re discussing. Things that seem ‘obvious’ may not be so obvious to a naive reader. Find a critical friend to read a draft when it’s ready (and be prepared to accept their honest comments!). It is of value to have the manuscript read by people who know the field and those who don’t; they will pick up different things. If possible, it is worth putting the report away somewhere for a couple of weeks and doing something else, then going back and rereading it. A report’s faults are much easier to spot after a break from working on it. B.4.1 A last piece of advice… No discussion can cover all the subtly different sorts of report that a scientist may be required to write at one time or another, but the ideas above provide a guide to one of the commonest. It is only a guide, and some circumstances will require a different approach, or structure. Writing isn’t easy. Like any difficult skill, it takes a great deal of time and effort to develop. People learn to write in different ways, and what works for one person may not work for another. Nonetheless, two pieces of advice from experienced writers crop up time and time again: Read actively. A good way to develop your scientific writing is to think critically about the papers we read, not just in terms of the science they present, but also the effectiveness of that presentation. Decide what works well and what doesn’t, then adopt the good ideas. Of course, this only works if we spend time reading the scientific literature in the first place! It’s very difficult (impossible perhaps) to become a competent writer without first becoming a regular and active reader. Write often. Many good writers will say that although they don’t always write a lot, they write often (every day!). Spread writing tasks out and set aside time to get them right. Rather than waiting until just before a deadline is looming to begin a project report or essay, start early and spend a little time every day working on it. Write regularly, but also write with effort—be prepared to critically evaluate, edit and revise written work. B.5 Further reading Barnard C, Gilbert F and McGregor P (1993) Asking questions in biology. Longman. Booth V (1985) Communicating in science: writing and speaking. Cambridge University Press. Lindsay D (1990) Scientific writing. Longman "],
["one-tailed-vs-two-tailed-tests.html", "C One-tailed vs. two-tailed tests C.1 Introduction C.2 An example of a one-tailed hypothesis C.3 So how do we perform a one-tailed t-test?", " C One-tailed vs. two-tailed tests C.1 Introduction The tests we have considered so far have been appropriate for questions of the type: ‘is the population mean, or the difference between population means, different from some value’ (often this value is 0, but it doesn’t have to be). Usually, we simply want to know whether there is any difference. We’ve not been too concerned with the direction of the difference, i.e. if our test reveals a difference between the populations, we would be interested in it whichever way round it was. A test of this sort is called a two-tailed test. However, there are occasions when we may wish to test a more specific hypothesis. For example if we have measured a variable in two groups, A and B, we might only be interested in whether the population mean of A is bigger than that of B. That is, we set out to evaluate the specific hypothesis that the mean of A is bigger than the mean of B. A test this sort of directional hypothesis is called a one-tailed test. We’ll examine it one- vs. two-tailed test distinction in the context of a t-test. Just keep in mind that the distinction can arise up in many different situations. C.2 An example of a one-tailed hypothesis A farmer has been persuaded to try a new pesticide called Toxic Death on his broad bean crop. He sprays 20 fields of beans with Toxic Death and leaves 20 fields untreated. To test the effectiveness of Toxic Death he is only interested in detecting a positive response in his crop. It makes no difference to the farmer if the pesticide has no effect or proves to reduce the crop yield: in either case the product is a waste of money to the farmer and he will not use it again. The farmers’ test hypothesis is quite specific, in terms of the direction of the effect that is being tested for: ‘Toxic Death increases the mean yield of broad beans’. This is what is meant by a one-tailed test. In a one-tailed test we are only interested in a positive response, or a negative response. One rapidly descends into a philosophical quagmire when considering what ‘only interested in’ actually means, but let’s work with that definition for now. Where can we use a one-tailed test? In what follows we’re only going to consider one- vs. two-tailed tests in the context of t-tests. However, keep in mind that although the one- vs. two-tail distinction mostly pops up when discussing t-tests, it is relevant to some other kinds of statistical tests. C.3 So how do we perform a one-tailed t-test? We can use of any of the t-tests we’ve seen (one-sample, two-sample, paired-sample) to examine an hypothesis about means where the direction of the effect is pre-specified. We are interested in testing the hypothesis ‘Toxic Death increases the mean yield of broad beans’ since this is the hypothesis of greatest relevance to the farmer. If the yield of beans in the two treatments was as illustrated below… Figure C.1: Toxic death data …we would not even need to perform a test. The mean yield in the Toxic Death treatment is actually lower than in the control — we can automatically reject the hypothesis that treated fields have higher yield. The Toxic Death salesman might be in for some of his own medicine! However, if the results indicated that the mean yield was greater where Toxic Death was used we then need to to perform the test to determine how confident we can be that this is a real increase rather than a consequence of sampling variation. The aim is to compare two independent means, so we would use a two-sample t-test. The one-tailed version of the test is no different from the two-tailed version, but for one small tweak: when we come to find the p-value value to judge the significance of the test, the correct probability for a one-tailed test is half that found for the two-tailed test. So, suppose we had performed a two-tailed test (i.e. a test of the hypothesis ‘Toxic Death changes the yield of broad beans’––no direction specified) and found a positive effect of Toxic Death, but with a probability p = 0.08. Performing a one-tailed test of the hypothesis ‘Toxic Death increases the yield of broad beans’ would give a probability of exactly half this (p = 0.04). In this case using the two-tailed test we would have concluded that there was no significant effect of Toxic Death on the yield of broad beans (p = 0.08), whereas with the one-tailed test we would conclude that Toxic Death significantly increased the yield of broad beans (p = 0.04). This is a rather striking change of conclusion, which may seem like a fiddle40. It is not a fiddle (at least not if used properly). But because using a one- rather than two-tailed test can alter the conclusion we draw, such tests should be used with caution, and the rules about how and when to use them strictly adhered to. These rules are discussed below. First, we will see how to actually do a one-tailed test in R. C.3.1 Carrying out one-tailed t-tests in R You should work through the example in this section. We said one-tailed tests were not a different sort of t-test to those we’ve seen so far—we can do one-tailed tests with any of these t-tests. We’ll show you how to do it using one example, the paired-sample t-test, applied to drug data. Let’s go back to the data on glycolipid concentrations in eight patients being treated with Drugs A and B. Imagine now that rather than A and B being two new drugs, Drug A is the existing treatment for the disease, while Drug B is a new type of drug being tested for effectiveness. In this case the drug company is obviously only interested in whether the new drug causes a greater reduction in the glycolipid levels of individual patients than the old one. If it has the same effect, or if it is less effective than the existing treatment it will not be worth spending time and money developing to the production stage. So the company’s test hypothesis is: ‘The new drug (B) is more effective than the existing treatment (A) at reducing glycolipid levels’. Let’s test this. Download the GLYCOLIPID.CSV file from MOLE again if you don’t have it and place it in your working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name glycolipid, then remind yourself what the data looks like, e.g. glimpse(glycolipid) ## Observations: 16 ## Variables: 4 ## $ Patient &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8 ## $ Sex &lt;fctr&gt; Male, Female, Male, Female, Female, Male, Female, ... ## $ Drug &lt;fctr&gt; A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B ## $ Glycolipid &lt;dbl&gt; 142.9, 140.6, 144.7, 144.0, 142.4, 146.0, 149.1, 15... We’re going to use a paired-sample t-test again in this example. We just want to rerun the previous analysis using a one-tailed version of the test. In order to do this we have to set one more argument in the t.test function, called alternative. This can take one of three values: &quot;two.sided&quot;, &quot;less&quot;, or &quot;greater&quot;. The first option is the default used by the t.test function. We pick a one-side test with an associated direction of effect by choosing “less” or “greater”. Here is how it works: t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.01718 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.7863436 Inf ## sample estimates: ## mean of the differences ## 2.8375 Most of that output should seem quite familiar by now. However, why did we set the alternative to “greater”? We wanted to assess whether drug B really leads to lower glycolipid concentrations. Let’s make sure we understand what is going on… Look carefully at the mean of the differences in the output. This is a positive number: it looks like R has assumed we want to examine the ‘Drug A - Drug B’ differences between pairs. It does this because the letter ‘A’ comes before the letter ‘B’ in the alphabet. That’s all! It doesn’t actually matter which way round we calculate the differences—the t-statistic will be the same. However, we do have to be very careful to ensure that the direction of the alternative hypothesis we choose is correct. It is very easy to get this wrong if we are not paying attention, which is why R always prints the mean.41 Compare this with the output from the two-tailed paired-sample t-test on the glycolipid data (we just drop the alternative = &quot;greater&quot; argument): t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.277451 5.397549 ## sample estimates: ## mean of the differences ## 2.8375 The output differs in two respects: The alternative hypothesis in the one-tailed test (the alternative hypothesis: line) is: true difference in means is greater than 0. This tells us that R has tested whether the mean of our sample of differences is greater than zero42. The p-value in the one-tailed test is exactly half the value it is in two-tailed test, i.e. the result is ‘more significant’ when done as a one-tailed test. This is the primary advantage of the one-tailed test: it has more statistical power. Also note that in this case, the drug effect was significant in both one- and two-tailed tests, but it is not always so. Our conclusion from this test would be: Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A (one-tailed test: t=2.62, d.f.=7, p=0.017). Note two things about the conclusion. First, we should specify that a one-tailed test was used. If no information is given it is conventional to assume a two-tailed test has was used. Second, it is sensible to put the actual probability level, rather than just giving the category for p. This is because if anyone does then want to see what the significance of the two-tailed test would have been, they can easily double the probability, which cannot be done if we just say p &lt; 0.05. C.3.2 When to use, and not to use, one-tailed t-tests Whether we use a one- or two-tailed tests can sometimes appear to change our conclusions rather dramatically… There is an obvious temptation here! It would be easy to collect data, determine which mean value is larger, and then test for differences in that direction using a one-tailed test.This would increase the apparent significance of the results, but it is a fundamentally flawed analysis. Why? Well, if we do this we are implicitly doing a two-tailed test—we are going to test the effect whichever direction it goes in—while using the extra power of a one-tailed test by pretending only one direction was ever going to be considered. It is very important to get clear in our mind about what one-tailed tests do and when (if ever) we might legitimately use them. The key principle is that the direction of the predicted effect must be specified before the data are collected. We are then effectively forfeiting the right to test for differences in the opposite direction to that predicted—we are putting all our statistical eggs in one basket. We are saying we are not interested in testing the result if it is in the opposite direction to that we predicted. That’s a very strong stance to adopt. What this means is that one-tailed tests are never appropriate in investigative research. Just because we have an idea about which direction we think an experimental result might go in is not a good reason to just test for that and effect in that direction. Instead we need to ask whether or not we would genuinely be prepared to ignore a result in the other direction. Usually the answer is no. For example, if we dose the soil in which experimental plants are growing with a low concentration of a particular compound we suppose will be toxic to them, we might expect that their growth will be reduced. However, if in fact they show higher growth with the compound than without it, we would almost certainly want to test to see if this was a real effect43. We would, therefore, be better off using a two-tailed test. Similarly in the case of testing a new drug against an existing treatment, on the face of it we may primarily be interested in whether the new treatment is better than the old one, and might consequently think of the analysis as one-tailed. However, we may also be interested in whether the new treatment is actually worse that the old one, rather than simply the same—i.e. an effect in the opposite direction to that we predict. Why? Well perhaps the new treatment has fewer side effects, so even if it is only of the same efficacy as the old one, it may still be preferable. However, we would most definitely want to ensure that it was no worse. So a two-tailed test is probably still be the most appropriate analysis. If there are so many problems why use them? There are situations where we are only interested in the direction of the effects, rather than understanding mechanisms. Here one-tailed tests are a useful tool. Testing medical or veterinary interventions for efficacy might be one (as discussed above). Another is in situations such as industrial processes: if we are in charge of managing the production of blood test kits, and we are considering a change to the production process we might want to sample the production line under the old and new systems and test whether the new system has a lower failure rate. We are only interested in an improvement—if the change has a higher failure rate, or simply makes no difference, we are not going to convert the entire production process to the new system. Here the extra power to detect an effect in a specified direction would certainly be worth considering44. One-tailed tests have their uses (and sometimes appear in statistics books focussed on biological data) but they should be used with caution. The default procedure should be to use a two-tailed test unless there are very good reasons for doing otherwise. For one, the p &lt; 0.05 threshold is just a convention↩ This is also another reason for why it is important to really understand your data before you start analysing it. You have been warned!↩ Remember, the differences are ‘the wrong way around’ relative to our scientific hypothesis, so this is the right test.↩ Perhaps the compound also contains important trace nutrients, or affects the microbial community in the soil.↩ In this situation we might also need to consider whether a p &lt; 0.05 threshold is really appropriate. If it is going to cost millions of pounds to alter the industrial processes, do we really want to make such a change when our test has a 1 in 20 false positive rate?↩ "]
]
